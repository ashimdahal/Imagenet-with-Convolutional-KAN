{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":19988,"databundleVersionId":1651354,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/IvanDrokin/torch-conv-kan.git","metadata":{"execution":{"iopub.status.busy":"2024-11-19T21:52:58.167693Z","iopub.execute_input":"2024-11-19T21:52:58.168066Z","iopub.status.idle":"2024-11-19T21:52:59.181953Z","shell.execute_reply.started":"2024-11-19T21:52:58.168033Z","shell.execute_reply":"2024-11-19T21:52:59.180836Z"},"trusted":true},"outputs":[{"name":"stdout","text":"fatal: destination path 'torch-conv-kan' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!mv torch-conv-kan/* ./","metadata":{"execution":{"iopub.status.busy":"2024-11-19T21:52:59.184457Z","iopub.execute_input":"2024-11-19T21:52:59.184863Z","iopub.status.idle":"2024-11-19T21:53:00.232385Z","shell.execute_reply.started":"2024-11-19T21:52:59.184820Z","shell.execute_reply":"2024-11-19T21:53:00.231219Z"},"trusted":true},"outputs":[{"name":"stdout","text":"mv: cannot stat 'torch-conv-kan/*': No such file or directory\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-11-19T21:53:00.234088Z","iopub.execute_input":"2024-11-19T21:53:00.234495Z","iopub.status.idle":"2024-11-19T21:53:01.234285Z","shell.execute_reply.started":"2024-11-19T21:53:00.234452Z","shell.execute_reply":"2024-11-19T21:53:01.233109Z"},"trusted":true},"outputs":[{"name":"stdout","text":"FOLD_mod11_0_0_.pth   FOLD_mod11_1_0_.pth  FOLD_mod11_8_0_.pth\nFOLD_mod11_0_1_.pth   FOLD_mod11_1_1_.pth  FOLD_mod11_8_1_.pth\nFOLD_mod11_0_2_.pth   FOLD_mod11_1_2_.pth  FOLD_mod11_8_2_.pth\nFOLD_mod11_0_3_.pth   FOLD_mod11_1_3_.pth  FOLD_mod11_8_3_.pth\nFOLD_mod11_0_4_.pth   FOLD_mod11_1_4_.pth  FOLD_mod11_8_4_.pth\nFOLD_mod11_10_0_.pth  FOLD_mod11_2_0_.pth  FOLD_mod11_9_0_.pth\nFOLD_mod11_10_1_.pth  FOLD_mod11_2_1_.pth  FOLD_mod11_9_1_.pth\nFOLD_mod11_10_2_.pth  FOLD_mod11_2_2_.pth  FOLD_mod11_9_2_.pth\nFOLD_mod11_10_3_.pth  FOLD_mod11_2_3_.pth  FOLD_mod11_9_3_.pth\nFOLD_mod11_10_4_.pth  FOLD_mod11_2_4_.pth  FOLD_mod11_9_4_.pth\nFOLD_mod11_11_0_.pth  FOLD_mod11_3_0_.pth  LICENSE\nFOLD_mod11_11_1_.pth  FOLD_mod11_3_1_.pth  README.md\nFOLD_mod11_11_2_.pth  FOLD_mod11_3_2_.pth  assets\nFOLD_mod11_11_3_.pth  FOLD_mod11_3_3_.pth  cifar.py\nFOLD_mod11_11_4_.pth  FOLD_mod11_3_4_.pth  configs\nFOLD_mod11_12_0_.pth  FOLD_mod11_4_0_.pth  gram_cifar_ray_tune.py\nFOLD_mod11_12_1_.pth  FOLD_mod11_4_1_.pth  gram_dropout_placement.py\nFOLD_mod11_12_2_.pth  FOLD_mod11_4_2_.pth  imagenet_1k.py\nFOLD_mod11_12_3_.pth  FOLD_mod11_4_3_.pth  kan_convs\nFOLD_mod11_12_4_.pth  FOLD_mod11_4_4_.pth  kan_peft\nFOLD_mod11_13_0_.pth  FOLD_mod11_5_0_.pth  kans\nFOLD_mod11_13_1_.pth  FOLD_mod11_5_1_.pth  medsegmentation_ukagnet.py\nFOLD_mod11_13_2_.pth  FOLD_mod11_5_2_.pth  mnist_conv.py\nFOLD_mod11_13_3_.pth  FOLD_mod11_5_3_.pth  models\nFOLD_mod11_13_4_.pth  FOLD_mod11_5_4_.pth  reports\nFOLD_mod11_14_0_.pth  FOLD_mod11_6_0_.pth  requirements.txt\nFOLD_mod11_14_1_.pth  FOLD_mod11_6_1_.pth  skin_cancer_finetune.py\nFOLD_mod11_14_2_.pth  FOLD_mod11_6_2_.pth  state.db\nFOLD_mod11_14_3_.pth  FOLD_mod11_6_3_.pth  submission.csv\nFOLD_mod11_14_4_.pth  FOLD_mod11_6_4_.pth  tests\nFOLD_mod11_15_0_.pth  FOLD_mod11_7_0_.pth  tiny_imagenet.py\nFOLD_mod11_15_1_.pth  FOLD_mod11_7_1_.pth  torch-conv-kan\nFOLD_mod11_15_2_.pth  FOLD_mod11_7_2_.pth  train\nFOLD_mod11_15_3_.pth  FOLD_mod11_7_3_.pth  train_pred.csv\nFOLD_mod11_15_4_.pth  FOLD_mod11_7_4_.pth  utils\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-11-19T21:53:01.236116Z","iopub.execute_input":"2024-11-19T21:53:01.236501Z","iopub.status.idle":"2024-11-19T21:53:35.073766Z","shell.execute_reply.started":"2024-11-19T21:53:01.236458Z","shell.execute_reply":"2024-11-19T21:53:35.072840Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting optuna@ git+https://github.com/optuna/optuna (from -r requirements.txt (line 19))\n  Cloning https://github.com/optuna/optuna to /tmp/pip-install-jv11cqgb/optuna_150e809972044c1d9627318e740a581d\n  Running command git clone --filter=blob:none --quiet https://github.com/optuna/optuna /tmp/pip-install-jv11cqgb/optuna_150e809972044c1d9627318e740a581d\n  Resolved https://github.com/optuna/optuna to commit 77e5b8d2a9d9990fa394cbe8cc905017514e7e13\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.66.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.34.2)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.18.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.2.2)\nCollecting omegaconf (from -r requirements.txt (line 7))\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting hydra-core (from -r requirements.txt (line 8))\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (3.0.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (10.3.0)\nRequirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.8.0)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.4.2)\nCollecting einops (from -r requirements.txt (line 13))\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (3.7.5)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.25.1)\nCollecting lion-pytorch (from -r requirements.txt (line 16))\n  Downloading lion_pytorch-0.2.2-py3-none-any.whl.metadata (618 bytes)\nRequirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.12.3)\nRequirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (1.4.17)\nCollecting medpy (from -r requirements.txt (line 21))\n  Downloading medpy-0.5.2.tar.gz (156 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (1.0.9)\nRequirement already satisfied: ray[tune] in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (2.24.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 4)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 4)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 4)) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 4)) (0.4.5)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (3.20.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (70.0.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.5.0)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 7))\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 9)) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 9)) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 9)) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 9)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 9)) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 9)) (3.9.5)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics->-r requirements.txt (line 12)) (0.11.7)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 14)) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 14)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 14)) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 14)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 14)) (2.9.0.post0)\nRequirement already satisfied: scikit-image>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 18)) (0.23.2)\nRequirement already satisfied: pydantic>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 18)) (2.9.2)\nRequirement already satisfied: albucore==0.0.17 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 18)) (0.0.17)\nRequirement already satisfied: eval-type-backport in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 18)) (0.2.0)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/conda/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 18)) (4.10.0.84)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna@ git+https://github.com/optuna/optuna->-r requirements.txt (line 19)) (1.13.3)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna@ git+https://github.com/optuna/optuna->-r requirements.txt (line 19)) (6.8.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from optuna@ git+https://github.com/optuna/optuna->-r requirements.txt (line 19)) (2.0.30)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray[tune]->-r requirements.txt (line 20)) (4.22.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray[tune]->-r requirements.txt (line 20)) (1.0.8)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray[tune]->-r requirements.txt (line 20)) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray[tune]->-r requirements.txt (line 20)) (1.4.1)\nRequirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.10/site-packages (from ray[tune]->-r requirements.txt (line 20)) (2.6.2.2)\nRequirement already satisfied: SimpleITK>=2.1 in /opt/conda/lib/python3.10/site-packages (from medpy->-r requirements.txt (line 21)) (2.4.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna@ git+https://github.com/optuna/optuna->-r requirements.txt (line 19)) (1.3.5)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 5)) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (4.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations->-r requirements.txt (line 18)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations->-r requirements.txt (line 18)) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (2024.8.30)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations->-r requirements.txt (line 18)) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations->-r requirements.txt (line 18)) (2024.5.22)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations->-r requirements.txt (line 18)) (0.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna@ git+https://github.com/optuna/optuna->-r requirements.txt (line 19)) (3.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray[tune]->-r requirements.txt (line 20)) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray[tune]->-r requirements.txt (line 20)) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray[tune]->-r requirements.txt (line 20)) (0.18.1)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 9)) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 9)) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.1)\nDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lion_pytorch-0.2.2-py3-none-any.whl (5.4 kB)\nBuilding wheels for collected packages: antlr4-python3-runtime, optuna, medpy\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=03d3da91fbe44083d6bd7934212dd6ab9015a167623c0cfb4d1a3cc710b1eb94\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for optuna (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for optuna: filename=optuna-4.2.0.dev0-py3-none-any.whl size=364339 sha256=d6ba0345e0b21352020fb092c07bef09e96a0a493c47198524c01a099c63cd6a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kvgmqn5r/wheels/9b/bb/cc/63e9a98b502f90f72012482dde94977ac662ced1a8843e685d\n  Building wheel for medpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for medpy: filename=MedPy-0.5.2-py3-none-any.whl size=224707 sha256=49c168b03b0551a05eadf71c83c81dab2fa3e80de0aa68e7053cf33d1706b5da\n  Stored in directory: /root/.cache/pip/wheels/a1/b8/63/bdf557940ec60d1b8822e73ff9fbe7727ac19f009d46b5d175\nSuccessfully built antlr4-python3-runtime optuna medpy\nInstalling collected packages: antlr4-python3-runtime, omegaconf, einops, medpy, hydra-core, optuna, lion-pytorch\n  Attempting uninstall: optuna\n    Found existing installation: optuna 4.0.0\n    Uninstalling optuna-4.0.0:\n      Successfully uninstalled optuna-4.0.0\nSuccessfully installed antlr4-python3-runtime-4.9.3 einops-0.8.0 hydra-core-1.3.2 lion-pytorch-0.2.2 medpy-0.5.2 omegaconf-2.3.0 optuna-4.2.0.dev0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install iterative-stratification","metadata":{"execution":{"iopub.status.busy":"2024-11-19T21:53:35.075841Z","iopub.execute_input":"2024-11-19T21:53:35.076132Z","iopub.status.idle":"2024-11-19T21:53:43.506398Z","shell.execute_reply.started":"2024-11-19T21:53:35.076101Z","shell.execute_reply":"2024-11-19T21:53:43.505278Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting iterative-stratification\n  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.14.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification) (3.5.0)\nDownloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.9\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-19T21:53:43.507810Z","iopub.execute_input":"2024-11-19T21:53:43.508106Z","iopub.status.idle":"2024-11-19T21:53:43.515993Z","shell.execute_reply.started":"2024-11-19T21:53:43.508078Z","shell.execute_reply":"2024-11-19T21:53:43.515106Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_drug.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nfrom copy import deepcopy as dp\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom kan_convs import FastKANConv1DLayer\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n    ss_1_dic = {'zsco':StandardScaler(),\n                'mima':MinMaxScaler(),\n                'maxb':MaxAbsScaler(), \n                'robu':RobustScaler(),\n                'norm':Normalizer(), \n                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n                'powe':PowerTransformer()}\n    ss_1 = ss_1_dic[sc_name]\n    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n    if saveM == False:\n        return(df_2)\n    else:\n        return(df_2,ss_1)\n\ndef norm_tra(df_1,ss_x):\n    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n    return(df_2)\n\ndef g_table(list1):\n    table_dic = {}\n    for i in list1:\n        if i not in table_dic.keys():\n            table_dic[i] = 1\n        else:\n            table_dic[i] += 1\n    return(table_dic)\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-11-19T21:53:43.517201Z","iopub.execute_input":"2024-11-19T21:53:43.517950Z","iopub.status.idle":"2024-11-19T21:53:49.865942Z","shell.execute_reply.started":"2024-11-19T21:53:43.517912Z","shell.execute_reply":"2024-11-19T21:53:49.865265Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"SEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\ninput_dir = '../input/lish-moa/'\n\nsc_dic = {}\nfeat_dic = {}\ntrain_features = pd.read_csv(input_dir+'train_features.csv')\ntrain_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\ntest_features = pd.read_csv(input_dir+'test_features.csv')\nsample_submission = pd.read_csv(input_dir+'sample_submission.csv')\ntrain_drug = pd.read_csv(input_dir+'train_drug.csv')\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\ntarget_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n\n######## non-score ########\nnonctr_id = train_features.loc[train_features['cp_type']!='ctl_vehicle','sig_id'].tolist()\ntmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\nmat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\nmat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\nmat_cor2.index = target_nonsc_cols\nmat_cor2.columns = target_cols\nmat_cor2 = mat_cor2.dropna()\nmat_cor2_max = mat_cor2.abs().max(axis = 1)\n\nq_n_cut = 0.9\ntarget_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\nprint(len(target_nonsc_cols2))\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfeat_dic['gene'] = GENES\nfeat_dic['cell'] = CELLS\n\n# sample norm \nq2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\nqmean = (q2+q7)/2\ntrain_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\nq2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\nqmean = (q2+q7)/2\ntest_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n\nq2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\nqmean = (q2+q7)/2\ntrain_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\nqmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\ntrain_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n\nq2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\nqmean = (q2+q7)/2\ntest_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\nqmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\ntest_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n\n# remove ctl\ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[['sig_id']+target_cols]\ntarget_ns = train[['sig_id']+target_nonsc_cols2]\n\ntrain0 = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n# drug ids\ntar_sig = target['sig_id'].tolist()\ntrain_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\ntarget = target.merge(train_drug, on='sig_id', how='left') \n\n# LOCATE DRUGS\nvc = train_drug.drug_id.value_counts()\nvc1 = vc.loc[vc <= 19].index\nvc2 = vc.loc[vc > 19].index\n\nfeature_cols = []\nfor key_i in feat_dic.keys():\n    value_i = feat_dic[key_i]\n    print(key_i,len(value_i))\n    feature_cols += value_i\nlen(feature_cols)\nfeature_cols0 = dp(feature_cols)\n    \noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\n# Averaging on multiple SEEDS\nfor seed in SEED:\n\n    seed_everything(seed=seed)\n    folds = train0.copy()\n    feature_cols = dp(feature_cols0)\n    \n    # kfold - leave drug out\n    target2 = target.copy()\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 19X\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    target2['kfold'] = target2.drug_id.map(dct1)\n    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n    target2.kfold = target2.kfold.astype(int)\n\n    folds['kfold'] = target2['kfold'].copy()\n\n    train = folds.copy()\n    test_ = test.copy()\n\n    # HyperParameters\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 25\n    BATCH_SIZE = 128\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 5\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = False\n\n    n_comp1 = 50\n    n_comp2 = 15\n\n    num_features=len(feature_cols) + n_comp1 + n_comp2\n    num_targets=len(target_cols)\n    num_targets_0=len(target_nonsc_cols2)\n    hidden_size=4096\n\n    tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n    tar_weight0_min = dp(np.min(tar_weight0))\n    tar_weight = tar_weight0_min/tar_weight0\n    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n    from torch.nn.modules.loss import _WeightedLoss\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n                                                      pos_weight = pos_weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class TrainDataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n            }\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n            return dct\n\n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n\n        return final_loss\n\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n\n        return preds\n\n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets, hidden_size):\n            super(Model, self).__init__()\n#             cha_1 = 256\n#             cha_2 = 512\n#             cha_3 = 512\n            \n            cha_1 = 64\n            cha_2 = 128\n            cha_3 = 128\n\n            cha_1_reshape = int(hidden_size/cha_1)\n            cha_po_1 = int(hidden_size/cha_1/2)\n            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n\n            self.cha_1 = cha_1\n            self.cha_2 = cha_2\n            self.cha_3 = cha_3\n            self.cha_1_reshape = cha_1_reshape\n            self.cha_po_1 = cha_po_1\n            self.cha_po_2 = cha_po_2\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dropout1 = nn.Dropout(0.1)\n            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n            self.dropout_c1 = nn.Dropout(0.1)\n            self.conv1 = FastKANConv1DLayer(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2)\n\n            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n\n            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2 = nn.Dropout(0.1)\n            self.conv2 = FastKANConv1DLayer(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1)\n\n            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_1 = nn.Dropout(0.3)\n            self.conv2_1 = FastKANConv1DLayer(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1)\n\n            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_2 = nn.Dropout(0.2)\n            self.conv2_2 = FastKANConv1DLayer(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2)\n\n            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n            self.flt = nn.Flatten()\n\n            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n            self.dropout3 = nn.Dropout(0.2)\n            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n\n        def forward(self, x):\n\n            x = self.batch_norm1(x)\n            x = self.dropout1(x)\n            x = F.celu(self.dense1(x), alpha=0.06)\n\n            x = x.reshape(x.shape[0],self.cha_1,\n                          self.cha_1_reshape)\n\n            x = self.batch_norm_c1(x)\n            x = self.dropout_c1(x)\n            x = F.relu(self.conv1(x))\n\n            x = self.ave_po_c1(x)\n\n            x = self.batch_norm_c2(x)\n            x = self.dropout_c2(x)\n            x = F.relu(self.conv2(x))\n            x_s = x\n\n            x = self.batch_norm_c2_1(x)\n            x = self.dropout_c2_1(x)\n            x = F.relu(self.conv2_1(x))\n\n            x = self.batch_norm_c2_2(x)\n            x = self.dropout_c2_2(x)\n            x = F.relu(self.conv2_2(x))\n            x =  x * x_s\n\n            x = self.max_po_c2(x)\n\n            x = self.flt(x)\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = self.dense3(x)\n\n            return x\n\n    def run_training(fold, seed):\n\n        seed_everything(seed)\n\n        trn_idx = train[train['kfold'] != fold].index\n        val_idx = train[train['kfold'] == fold].index\n\n        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n\n        x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\n        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n        x_test = test_[feature_cols]\n\n        #------------ norm --------------\n        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n        col_num.sort()\n        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n\n        #------------ pca --------------\n        def pca_pre(tr,va,te,\n                    n_comp,feat_raw,feat_new):\n            pca = PCA(n_components=n_comp, random_state=42)\n            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n            return(tr2,va2,te2)\n\n\n        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n        feat_dic['pca_g'] = pca_feat_g\n        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n\n        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n        feat_dic['pca_c'] = pca_feat_g\n        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n\n        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n\n        train_dataset = TrainDataset(x_train, y_train_ns)\n        valid_dataset = TrainDataset(x_valid, y_valid_ns)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets_0,\n            hidden_size=hidden_size,\n        )\n\n        model.to(DEVICE)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, \n                                                  max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n        loss_tr = nn.BCEWithLogitsLoss()   #SmoothBCEwLogits(smoothing = 0.001)\n        loss_va = nn.BCEWithLogitsLoss()    \n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        for epoch in range(1):\n            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n            print(f\"FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n\n        model.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\n        model.to(DEVICE)\n\n        train_dataset = TrainDataset(x_train, y_train)\n        valid_dataset = TrainDataset(x_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n        loss_va = nn.BCEWithLogitsLoss()    \n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        oof = np.zeros((len(train), len(target_cols)))\n        best_loss = np.inf\n\n        mod_name = f\"FOLD_mod11_{seed}_{fold}_.pth\"\n        \n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n            print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n\n            if valid_loss < best_loss:\n\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), mod_name)\n\n            elif(EARLY_STOP == True):\n\n                early_step += 1\n                if (early_step >= early_stopping_steps):\n                    break\n\n        #--------------------- PREDICTION---------------------\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n        )\n\n        model.load_state_dict(torch.load(mod_name))\n        model.to(DEVICE)\n\n        predictions = np.zeros((len(test_), len(target_cols)))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n            oof_, pred_ = run_training(fold, seed)\n\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n    \n    oof_tmp = dp(oof)\n    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n    \n\nfrom sklearn.metrics import log_loss\nprint(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n\ntrain0[target_cols] = oof\ntest[target_cols] = predictions\n\n### for blend test ###\ntrain0.to_csv('train_pred.csv', index=False)\n### for blend test ###\n\nsub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T21:53:49.867107Z","iopub.execute_input":"2024-11-19T21:53:49.867385Z","iopub.status.idle":"2024-11-20T00:41:32.433686Z","shell.execute_reply.started":"2024-11-19T21:53:49.867359Z","shell.execute_reply":"2024-11-20T00:41:32.432555Z"},"trusted":true},"outputs":[{"name":"stdout","text":"33\ngene 772\ncell 100\nFOLD: 0, EPOCH: 0,train_loss: 0.7333418066086976, valid_loss: 0.6978098733084542\nSEED: 0, FOLD: 0, EPOCH: 0,train_loss: 0.4691720162315861, valid_loss: 0.024828314408659936\nSEED: 0, FOLD: 0, EPOCH: 1,train_loss: 0.021686490935583908, valid_loss: 0.021153220374669348\nSEED: 0, FOLD: 0, EPOCH: 2,train_loss: 0.020258854507752087, valid_loss: 0.018883360709462848\nSEED: 0, FOLD: 0, EPOCH: 3,train_loss: 0.018956109543965347, valid_loss: 0.018219612405768464\nSEED: 0, FOLD: 0, EPOCH: 4,train_loss: 0.018420447892360928, valid_loss: 7.687085860116142\nSEED: 0, FOLD: 0, EPOCH: 5,train_loss: 0.017728823000916105, valid_loss: 0.018102741693811757\nSEED: 0, FOLD: 0, EPOCH: 6,train_loss: 0.01726946557291608, valid_loss: 0.018393209709652834\nSEED: 0, FOLD: 0, EPOCH: 7,train_loss: 0.016965936275496, valid_loss: 0.018016721068748406\nSEED: 0, FOLD: 0, EPOCH: 8,train_loss: 0.016656766287928473, valid_loss: 0.01830864708338465\nSEED: 0, FOLD: 0, EPOCH: 9,train_loss: 0.016537004629608946, valid_loss: 0.018210702602352413\nSEED: 0, FOLD: 0, EPOCH: 10,train_loss: 0.016184001830339, valid_loss: 0.018183661438524724\nSEED: 0, FOLD: 0, EPOCH: 11,train_loss: 0.016130904127182304, valid_loss: 0.025438798697931427\nSEED: 0, FOLD: 0, EPOCH: 12,train_loss: 0.015525981358697882, valid_loss: 0.019846734856920584\nSEED: 0, FOLD: 0, EPOCH: 13,train_loss: 0.015070946192017931, valid_loss: 0.018466632201203276\nSEED: 0, FOLD: 0, EPOCH: 14,train_loss: 0.014650702753198751, valid_loss: 0.01873619465955666\nSEED: 0, FOLD: 0, EPOCH: 15,train_loss: 0.013541980977237656, valid_loss: 0.019354425211037907\nSEED: 0, FOLD: 0, EPOCH: 16,train_loss: 0.012458801337018393, valid_loss: 0.01976603264255183\nSEED: 0, FOLD: 0, EPOCH: 17,train_loss: 0.011178229415816242, valid_loss: 0.02042157697890486\nSEED: 0, FOLD: 0, EPOCH: 18,train_loss: 0.00973562876680407, valid_loss: 0.021424718680126326\nSEED: 0, FOLD: 0, EPOCH: 19,train_loss: 0.008278017347354604, valid_loss: 0.02131173158330577\nSEED: 0, FOLD: 0, EPOCH: 20,train_loss: 0.00706255504105618, valid_loss: 0.021314780041575433\nSEED: 0, FOLD: 0, EPOCH: 21,train_loss: 0.0062595670975312805, valid_loss: 0.021559150410549982\nSEED: 0, FOLD: 0, EPOCH: 22,train_loss: 0.0058219784573800325, valid_loss: 0.021497274349842752\nSEED: 0, FOLD: 0, EPOCH: 23,train_loss: 0.005589121473494215, valid_loss: 0.02151253798178264\nSEED: 0, FOLD: 0, EPOCH: 24,train_loss: 0.005489002447575331, valid_loss: 0.021535770169326236\nFOLD: 1, EPOCH: 0,train_loss: 0.733479095636493, valid_loss: 0.6968707067625863\nSEED: 0, FOLD: 1, EPOCH: 0,train_loss: 0.4700301517426533, valid_loss: 0.024759012194616455\nSEED: 0, FOLD: 1, EPOCH: 1,train_loss: 0.02173280476653663, valid_loss: 0.02152007333934307\nSEED: 0, FOLD: 1, EPOCH: 2,train_loss: 0.019984895145914852, valid_loss: 0.019173225707241465\nSEED: 0, FOLD: 1, EPOCH: 3,train_loss: 0.01868028099220382, valid_loss: 0.018826371005603245\nSEED: 0, FOLD: 1, EPOCH: 4,train_loss: 0.017957215360535756, valid_loss: 0.018460196895258768\nSEED: 0, FOLD: 1, EPOCH: 5,train_loss: 0.017422093282433323, valid_loss: 0.018350583421332497\nSEED: 0, FOLD: 1, EPOCH: 6,train_loss: 0.017136440435628387, valid_loss: 0.018455733465296883\nSEED: 0, FOLD: 1, EPOCH: 7,train_loss: 0.01678701952426103, valid_loss: 0.018529427370854785\nSEED: 0, FOLD: 1, EPOCH: 8,train_loss: 0.016658725468509824, valid_loss: 0.01936226968786546\nSEED: 0, FOLD: 1, EPOCH: 9,train_loss: 0.016473989396689148, valid_loss: 0.018687049565570695\nSEED: 0, FOLD: 1, EPOCH: 10,train_loss: 0.01622610549394884, valid_loss: 0.019797203317284583\nSEED: 0, FOLD: 1, EPOCH: 11,train_loss: 0.016012564151935332, valid_loss: 0.019048398839575903\nSEED: 0, FOLD: 1, EPOCH: 12,train_loss: 0.015633888073591857, valid_loss: 0.0192043415669884\nSEED: 0, FOLD: 1, EPOCH: 13,train_loss: 0.015225065681729873, valid_loss: 0.019419032388499805\nSEED: 0, FOLD: 1, EPOCH: 14,train_loss: 0.014649622339456186, valid_loss: 0.020231292290346963\nSEED: 0, FOLD: 1, EPOCH: 15,train_loss: 0.01385532115606496, valid_loss: 0.020506225526332855\nSEED: 0, FOLD: 1, EPOCH: 16,train_loss: 0.012919276320531855, valid_loss: 0.02116582196738039\nSEED: 0, FOLD: 1, EPOCH: 17,train_loss: 0.011618952423225354, valid_loss: 0.021623144085918155\nSEED: 0, FOLD: 1, EPOCH: 18,train_loss: 0.010344890700857135, valid_loss: 0.02208589597472123\nSEED: 0, FOLD: 1, EPOCH: 19,train_loss: 0.008868902374439649, valid_loss: 0.02212848945387772\nSEED: 0, FOLD: 1, EPOCH: 20,train_loss: 0.007618546615062404, valid_loss: 0.02185268753341266\nSEED: 0, FOLD: 1, EPOCH: 21,train_loss: 0.006680210460874721, valid_loss: 0.02216511235705444\nSEED: 0, FOLD: 1, EPOCH: 22,train_loss: 0.00609898625662292, valid_loss: 0.022021124511957167\nSEED: 0, FOLD: 1, EPOCH: 23,train_loss: 0.0058307541554698545, valid_loss: 0.022178983209388596\nSEED: 0, FOLD: 1, EPOCH: 24,train_loss: 0.005712577784230022, valid_loss: 0.0221987320376294\nFOLD: 2, EPOCH: 0,train_loss: 0.733432276957277, valid_loss: 0.6979047358036041\nSEED: 0, FOLD: 2, EPOCH: 0,train_loss: 0.4708032043299813, valid_loss: 0.024234397062922224\nSEED: 0, FOLD: 2, EPOCH: 1,train_loss: 0.021838780843477318, valid_loss: 0.02117411107482279\nSEED: 0, FOLD: 2, EPOCH: 2,train_loss: 0.02048455787471671, valid_loss: 0.018904475221300825\nSEED: 0, FOLD: 2, EPOCH: 3,train_loss: 0.019191935159050037, valid_loss: 0.018365936518153724\nSEED: 0, FOLD: 2, EPOCH: 4,train_loss: 0.018329565804721653, valid_loss: 0.01806376627920305\nSEED: 0, FOLD: 2, EPOCH: 5,train_loss: 0.017841368083558653, valid_loss: 0.01843235964941628\nSEED: 0, FOLD: 2, EPOCH: 6,train_loss: 0.017519086896293404, valid_loss: 0.017946193022105622\nSEED: 0, FOLD: 2, EPOCH: 7,train_loss: 0.017311544198056927, valid_loss: 0.017942030777168626\nSEED: 0, FOLD: 2, EPOCH: 8,train_loss: 0.01710053724979145, valid_loss: 0.017850453828406686\nSEED: 0, FOLD: 2, EPOCH: 9,train_loss: 0.016952879159994747, valid_loss: 0.01810225222588462\nSEED: 0, FOLD: 2, EPOCH: 10,train_loss: 0.01687615916834793, valid_loss: 0.018260409124195576\nSEED: 0, FOLD: 2, EPOCH: 11,train_loss: 0.016730364115125892, valid_loss: 0.09595809307168512\nSEED: 0, FOLD: 2, EPOCH: 12,train_loss: 0.016501617671894855, valid_loss: 0.01833317198735826\nSEED: 0, FOLD: 2, EPOCH: 13,train_loss: 0.01618771935286729, valid_loss: 0.01819387379595462\nSEED: 0, FOLD: 2, EPOCH: 14,train_loss: 0.01588037900923603, valid_loss: 0.023994547041023478\nSEED: 0, FOLD: 2, EPOCH: 15,train_loss: 0.015212237794438133, valid_loss: 0.018495729193091393\nSEED: 0, FOLD: 2, EPOCH: 16,train_loss: 0.014392147738270569, valid_loss: 0.018707707743434346\nSEED: 0, FOLD: 2, EPOCH: 17,train_loss: 0.013369325595651415, valid_loss: 0.020358028164242998\nSEED: 0, FOLD: 2, EPOCH: 18,train_loss: 0.012015265237162079, valid_loss: 0.021537464933798593\nSEED: 0, FOLD: 2, EPOCH: 19,train_loss: 0.010502413691331943, valid_loss: 0.02029201908804038\nSEED: 0, FOLD: 2, EPOCH: 20,train_loss: 0.008953959124324761, valid_loss: 0.02061958676751922\nSEED: 0, FOLD: 2, EPOCH: 21,train_loss: 0.007691638787155566, valid_loss: 0.02074587230077561\nSEED: 0, FOLD: 2, EPOCH: 22,train_loss: 0.006877391615990496, valid_loss: 0.020803497775512582\nSEED: 0, FOLD: 2, EPOCH: 23,train_loss: 0.006489325868154781, valid_loss: 0.020787690985290444\nSEED: 0, FOLD: 2, EPOCH: 24,train_loss: 0.0063290200634873, valid_loss: 0.02087055749314673\nFOLD: 3, EPOCH: 0,train_loss: 0.7333921194076538, valid_loss: 0.6939650450434004\nSEED: 0, FOLD: 3, EPOCH: 0,train_loss: 0.4694407752080672, valid_loss: 0.023439589834639004\nSEED: 0, FOLD: 3, EPOCH: 1,train_loss: 0.02169295635236346, valid_loss: 42.68824479430914\nSEED: 0, FOLD: 3, EPOCH: 2,train_loss: 0.02038837483395701, valid_loss: 0.01894897994186197\nSEED: 0, FOLD: 3, EPOCH: 3,train_loss: 0.01917763660405425, valid_loss: 0.018303867110184262\nSEED: 0, FOLD: 3, EPOCH: 4,train_loss: 0.018414149054096662, valid_loss: 0.01820855340255158\nSEED: 0, FOLD: 3, EPOCH: 5,train_loss: 0.01789881384162151, valid_loss: 0.018052426166832447\nSEED: 0, FOLD: 3, EPOCH: 6,train_loss: 0.017536185472609774, valid_loss: 0.017815890855022838\nSEED: 0, FOLD: 3, EPOCH: 7,train_loss: 0.01723522853538178, valid_loss: 0.04827205544071538\nSEED: 0, FOLD: 3, EPOCH: 8,train_loss: 0.017097686214939407, valid_loss: 0.5048168049859149\nSEED: 0, FOLD: 3, EPOCH: 9,train_loss: 0.01694121730743327, valid_loss: 0.018163891934922762\nSEED: 0, FOLD: 3, EPOCH: 10,train_loss: 0.01685714423386515, valid_loss: 0.018068664920117175\nSEED: 0, FOLD: 3, EPOCH: 11,train_loss: 0.01669418683572524, valid_loss: 0.017969761522752897\nSEED: 0, FOLD: 3, EPOCH: 12,train_loss: 0.016445311312766178, valid_loss: 0.017924830663417067\nSEED: 0, FOLD: 3, EPOCH: 13,train_loss: 0.016235386919014265, valid_loss: 0.019482142797538213\nSEED: 0, FOLD: 3, EPOCH: 14,train_loss: 0.015857420719998037, valid_loss: 0.018274800532630513\nSEED: 0, FOLD: 3, EPOCH: 15,train_loss: 0.015323339536300173, valid_loss: 0.02158181459775993\nSEED: 0, FOLD: 3, EPOCH: 16,train_loss: 0.014565261688245379, valid_loss: 0.018907072395086287\nSEED: 0, FOLD: 3, EPOCH: 17,train_loss: 0.013380449987354054, valid_loss: 0.019212153447525842\nSEED: 0, FOLD: 3, EPOCH: 18,train_loss: 0.012063404005290806, valid_loss: 0.01955481804907322\nSEED: 0, FOLD: 3, EPOCH: 19,train_loss: 0.010479591010759274, valid_loss: 0.02031294389494828\nSEED: 0, FOLD: 3, EPOCH: 20,train_loss: 0.00896294729149752, valid_loss: 0.02045267889542239\nSEED: 0, FOLD: 3, EPOCH: 21,train_loss: 0.007740984249023208, valid_loss: 0.020467942314488546\nSEED: 0, FOLD: 3, EPOCH: 22,train_loss: 0.00695307361488433, valid_loss: 0.02055452588413443\nSEED: 0, FOLD: 3, EPOCH: 23,train_loss: 0.006568389759142546, valid_loss: 0.02074840659541743\nSEED: 0, FOLD: 3, EPOCH: 24,train_loss: 0.006423500494734532, valid_loss: 0.02075584733060428\nFOLD: 4, EPOCH: 0,train_loss: 0.7326504350572393, valid_loss: 0.6960134369986398\nSEED: 0, FOLD: 4, EPOCH: 0,train_loss: 0.46944465882320335, valid_loss: 0.02347536182829312\nSEED: 0, FOLD: 4, EPOCH: 1,train_loss: 0.021822439628126827, valid_loss: 0.02097381880240781\nSEED: 0, FOLD: 4, EPOCH: 2,train_loss: 0.020296507339546646, valid_loss: 0.01924306744975703\nSEED: 0, FOLD: 4, EPOCH: 3,train_loss: 0.019065063176811604, valid_loss: 0.018382870725222995\nSEED: 0, FOLD: 4, EPOCH: 4,train_loss: 0.01827236192057962, valid_loss: 0.018494795794997895\nSEED: 0, FOLD: 4, EPOCH: 5,train_loss: 0.01780806980568214, valid_loss: 0.01854261372770582\nSEED: 0, FOLD: 4, EPOCH: 6,train_loss: 0.017512856186300083, valid_loss: 0.01826569376779454\nSEED: 0, FOLD: 4, EPOCH: 7,train_loss: 0.017223416746634503, valid_loss: 0.018167866447142193\nSEED: 0, FOLD: 4, EPOCH: 8,train_loss: 0.017012702294395887, valid_loss: 0.019276757591537068\nSEED: 0, FOLD: 4, EPOCH: 9,train_loss: 0.016810579279410667, valid_loss: 0.01846677836562906\nSEED: 0, FOLD: 4, EPOCH: 10,train_loss: 0.016606425825992355, valid_loss: 0.01846144252589771\nSEED: 0, FOLD: 4, EPOCH: 11,train_loss: 0.016275070992338915, valid_loss: 0.01824687078062977\nSEED: 0, FOLD: 4, EPOCH: 12,train_loss: 0.016032239449196968, valid_loss: 0.01867635329919202\nSEED: 0, FOLD: 4, EPOCH: 13,train_loss: 0.0156997032193602, valid_loss: 0.018480598979762623\nSEED: 0, FOLD: 4, EPOCH: 14,train_loss: 0.015221350529379602, valid_loss: 0.019222415877240044\nSEED: 0, FOLD: 4, EPOCH: 15,train_loss: 0.014515281762873781, valid_loss: 0.019285854910101208\nSEED: 0, FOLD: 4, EPOCH: 16,train_loss: 0.01367191677454157, valid_loss: 0.019597696991903443\nSEED: 0, FOLD: 4, EPOCH: 17,train_loss: 0.012617429050252489, valid_loss: 0.019815724662372042\nSEED: 0, FOLD: 4, EPOCH: 18,train_loss: 0.011178239815584991, valid_loss: 0.02046067432633468\nSEED: 0, FOLD: 4, EPOCH: 19,train_loss: 0.009583619288236334, valid_loss: 0.020679650296057974\nSEED: 0, FOLD: 4, EPOCH: 20,train_loss: 0.008216242046108928, valid_loss: 0.0208637858075755\nSEED: 0, FOLD: 4, EPOCH: 21,train_loss: 0.007150712659231563, valid_loss: 0.021027878512229238\nSEED: 0, FOLD: 4, EPOCH: 22,train_loss: 0.0064872230733812285, valid_loss: 0.021178046773586954\nSEED: 0, FOLD: 4, EPOCH: 23,train_loss: 0.006164534287392229, valid_loss: 0.021207413556320328\nSEED: 0, FOLD: 4, EPOCH: 24,train_loss: 0.006045856303872837, valid_loss: 0.021227133965917997\nFOLD: 0, EPOCH: 0,train_loss: 0.7320367490899735, valid_loss: 0.7013229216848101\nSEED: 1, FOLD: 0, EPOCH: 0,train_loss: 0.46885296710483404, valid_loss: 0.024464149134499687\nSEED: 1, FOLD: 0, EPOCH: 1,train_loss: 0.02195081874674213, valid_loss: 0.020545612062726704\nSEED: 1, FOLD: 0, EPOCH: 2,train_loss: 0.02057718279083138, valid_loss: 0.018908540744866642\nSEED: 1, FOLD: 0, EPOCH: 3,train_loss: 0.01934946784614653, valid_loss: 0.018577133305370808\nSEED: 1, FOLD: 0, EPOCH: 4,train_loss: 0.018454754104216892, valid_loss: 0.017415542847343854\nSEED: 1, FOLD: 0, EPOCH: 5,train_loss: 0.01790026050951818, valid_loss: 0.01769387304250683\nSEED: 1, FOLD: 0, EPOCH: 6,train_loss: 0.017564016326830006, valid_loss: 0.017498543299734593\nSEED: 1, FOLD: 0, EPOCH: 7,train_loss: 0.017263318115062473, valid_loss: 0.027856714704207013\nSEED: 1, FOLD: 0, EPOCH: 8,train_loss: 0.016993725253943947, valid_loss: 0.01761309528457267\nSEED: 1, FOLD: 0, EPOCH: 9,train_loss: 0.016816016644293417, valid_loss: 0.017697860566633087\nSEED: 1, FOLD: 0, EPOCH: 10,train_loss: 0.016681434485413458, valid_loss: 0.020241586358419487\nSEED: 1, FOLD: 0, EPOCH: 11,train_loss: 0.01646237774495629, valid_loss: 0.017454637454024383\nSEED: 1, FOLD: 0, EPOCH: 12,train_loss: 0.016140751898342718, valid_loss: 0.017810571433178015\nSEED: 1, FOLD: 0, EPOCH: 13,train_loss: 0.01581258355113475, valid_loss: 0.01936542194868837\nSEED: 1, FOLD: 0, EPOCH: 14,train_loss: 0.015266148022551468, valid_loss: 0.01836299414613417\nSEED: 1, FOLD: 0, EPOCH: 15,train_loss: 0.014541687916262426, valid_loss: 0.019972916346575532\nSEED: 1, FOLD: 0, EPOCH: 16,train_loss: 0.013608653057852516, valid_loss: 0.01896612766597952\nSEED: 1, FOLD: 0, EPOCH: 17,train_loss: 0.01250133641820023, valid_loss: 0.01961976071553571\nSEED: 1, FOLD: 0, EPOCH: 18,train_loss: 0.011046110368941141, valid_loss: 0.019494925918323653\nSEED: 1, FOLD: 0, EPOCH: 19,train_loss: 0.00939627879879613, valid_loss: 0.019972618030650275\nSEED: 1, FOLD: 0, EPOCH: 20,train_loss: 0.007908885326722393, valid_loss: 0.020774911876235688\nSEED: 1, FOLD: 0, EPOCH: 21,train_loss: 0.00686692926303848, valid_loss: 0.02143194978790624\nSEED: 1, FOLD: 0, EPOCH: 22,train_loss: 0.0061994331814618645, valid_loss: 0.02033172313656126\nSEED: 1, FOLD: 0, EPOCH: 23,train_loss: 0.005899261774333275, valid_loss: 0.02029239455504077\nSEED: 1, FOLD: 0, EPOCH: 24,train_loss: 0.005784784609019972, valid_loss: 0.020446839343224252\nFOLD: 1, EPOCH: 0,train_loss: 0.7323248925870353, valid_loss: 0.7030059712273734\nSEED: 1, FOLD: 1, EPOCH: 0,train_loss: 0.4704640671393297, valid_loss: 0.023449871156896864\nSEED: 1, FOLD: 1, EPOCH: 1,train_loss: 0.021768100614095256, valid_loss: 4.576690958493522\nSEED: 1, FOLD: 1, EPOCH: 2,train_loss: 0.020405319594118718, valid_loss: 0.021235992759466173\nSEED: 1, FOLD: 1, EPOCH: 3,train_loss: 0.01902506662274364, valid_loss: 0.018676005090985978\nSEED: 1, FOLD: 1, EPOCH: 4,train_loss: 0.018389737410266904, valid_loss: 0.018173020439488548\nSEED: 1, FOLD: 1, EPOCH: 5,train_loss: 0.01785186461995553, valid_loss: 0.01803418951375144\nSEED: 1, FOLD: 1, EPOCH: 6,train_loss: 0.01745193170206825, valid_loss: 0.018149335948484283\nSEED: 1, FOLD: 1, EPOCH: 7,train_loss: 0.017196319113573888, valid_loss: 0.018151307106018068\nSEED: 1, FOLD: 1, EPOCH: 8,train_loss: 0.016991489893165384, valid_loss: 0.017778849149388928\nSEED: 1, FOLD: 1, EPOCH: 9,train_loss: 0.016823430729173396, valid_loss: 0.018573697124208723\nSEED: 1, FOLD: 1, EPOCH: 10,train_loss: 0.0167024461212602, valid_loss: 0.017807069207940784\nSEED: 1, FOLD: 1, EPOCH: 11,train_loss: 0.016527047278835392, valid_loss: 0.017836493360144753\nSEED: 1, FOLD: 1, EPOCH: 12,train_loss: 0.016232392805064246, valid_loss: 0.01825891046651772\nSEED: 1, FOLD: 1, EPOCH: 13,train_loss: 0.015941967711831533, valid_loss: 0.018245261082691807\nSEED: 1, FOLD: 1, EPOCH: 14,train_loss: 0.015554920362349409, valid_loss: 0.01834307930299214\nSEED: 1, FOLD: 1, EPOCH: 15,train_loss: 0.014804085383504412, valid_loss: 0.018483075233442444\nSEED: 1, FOLD: 1, EPOCH: 16,train_loss: 0.013972816173068798, valid_loss: 0.018988874767507824\nSEED: 1, FOLD: 1, EPOCH: 17,train_loss: 0.012819166016513414, valid_loss: 0.019390495653663364\nSEED: 1, FOLD: 1, EPOCH: 18,train_loss: 0.011275447857477805, valid_loss: 0.02033199141068118\nSEED: 1, FOLD: 1, EPOCH: 19,train_loss: 0.009638095628062305, valid_loss: 0.02049306801387242\nSEED: 1, FOLD: 1, EPOCH: 20,train_loss: 0.008087248667177275, valid_loss: 0.02074588324342455\nSEED: 1, FOLD: 1, EPOCH: 21,train_loss: 0.006980062492980357, valid_loss: 0.02079656246517386\nSEED: 1, FOLD: 1, EPOCH: 22,train_loss: 0.006321694419794056, valid_loss: 0.02095243866954531\nSEED: 1, FOLD: 1, EPOCH: 23,train_loss: 0.005978718525764063, valid_loss: 0.020975040537970408\nSEED: 1, FOLD: 1, EPOCH: 24,train_loss: 0.0058565902877191125, valid_loss: 0.02102488721055644\nFOLD: 2, EPOCH: 0,train_loss: 0.7318112016588018, valid_loss: 0.7043154069355556\nSEED: 1, FOLD: 2, EPOCH: 0,train_loss: 0.46820941764483415, valid_loss: 0.0243729688759361\nSEED: 1, FOLD: 2, EPOCH: 1,train_loss: 0.02177435557857372, valid_loss: 0.02142864428460598\nSEED: 1, FOLD: 2, EPOCH: 2,train_loss: 0.020646978520612785, valid_loss: 0.019539500613297733\nSEED: 1, FOLD: 2, EPOCH: 3,train_loss: 0.01905593351609465, valid_loss: 0.018436739673571928\nSEED: 1, FOLD: 2, EPOCH: 4,train_loss: 0.018279178007303373, valid_loss: 0.01823886965534517\nSEED: 1, FOLD: 2, EPOCH: 5,train_loss: 0.017862311301186033, valid_loss: 0.018198970971362933\nSEED: 1, FOLD: 2, EPOCH: 6,train_loss: 0.017444174662502348, valid_loss: 0.01795477329620293\nSEED: 1, FOLD: 2, EPOCH: 7,train_loss: 0.017233990315023973, valid_loss: 0.01805944735450404\nSEED: 1, FOLD: 2, EPOCH: 8,train_loss: 0.017036838679695906, valid_loss: 0.02258320525288582\nSEED: 1, FOLD: 2, EPOCH: 9,train_loss: 0.017039279250994972, valid_loss: 0.018003135813134057\nSEED: 1, FOLD: 2, EPOCH: 10,train_loss: 0.016889254960730887, valid_loss: 0.018119451749537673\nSEED: 1, FOLD: 2, EPOCH: 11,train_loss: 0.016772207642055077, valid_loss: 0.01900840981730393\nSEED: 1, FOLD: 2, EPOCH: 12,train_loss: 0.016544795676094036, valid_loss: 0.057917475407677034\nSEED: 1, FOLD: 2, EPOCH: 13,train_loss: 0.016316890264388876, valid_loss: 0.0182313559576869\nSEED: 1, FOLD: 2, EPOCH: 14,train_loss: 0.015907484997549782, valid_loss: 0.018466374544160707\nSEED: 1, FOLD: 2, EPOCH: 15,train_loss: 0.015354279273500046, valid_loss: 0.01880050322839192\nSEED: 1, FOLD: 2, EPOCH: 16,train_loss: 0.014733074253182049, valid_loss: 0.018689327713634286\nSEED: 1, FOLD: 2, EPOCH: 17,train_loss: 0.013576984655220007, valid_loss: 0.019464362944875444\nSEED: 1, FOLD: 2, EPOCH: 18,train_loss: 0.012245081047918917, valid_loss: 0.021170626633933612\nSEED: 1, FOLD: 2, EPOCH: 19,train_loss: 0.010655648689177157, valid_loss: 0.020560180076531\nSEED: 1, FOLD: 2, EPOCH: 20,train_loss: 0.008936173603127616, valid_loss: 0.02099793255329132\nSEED: 1, FOLD: 2, EPOCH: 21,train_loss: 0.007617187588408157, valid_loss: 0.020980658488614218\nSEED: 1, FOLD: 2, EPOCH: 22,train_loss: 0.006734625787298748, valid_loss: 0.02138004329587732\nSEED: 1, FOLD: 2, EPOCH: 23,train_loss: 0.006291414536805689, valid_loss: 0.0216636097324746\nSEED: 1, FOLD: 2, EPOCH: 24,train_loss: 0.006136754834754527, valid_loss: 0.021417683522616113\nFOLD: 3, EPOCH: 0,train_loss: 0.7320777834325597, valid_loss: 0.7014131648199898\nSEED: 1, FOLD: 3, EPOCH: 0,train_loss: 0.468350468378892, valid_loss: 0.024147321549909454\nSEED: 1, FOLD: 3, EPOCH: 1,train_loss: 0.021903979281584423, valid_loss: 0.02168341034225055\nSEED: 1, FOLD: 3, EPOCH: 2,train_loss: 0.020500184212257896, valid_loss: 0.020143871967281613\nSEED: 1, FOLD: 3, EPOCH: 3,train_loss: 0.019155092031249533, valid_loss: 0.019071066805294582\nSEED: 1, FOLD: 3, EPOCH: 4,train_loss: 0.018309231726047787, valid_loss: 0.018926711965884482\nSEED: 1, FOLD: 3, EPOCH: 5,train_loss: 0.017820329432362232, valid_loss: 0.110971838714821\nSEED: 1, FOLD: 3, EPOCH: 6,train_loss: 0.017375901408925438, valid_loss: 0.019036359659263067\nSEED: 1, FOLD: 3, EPOCH: 7,train_loss: 0.017039841830568468, valid_loss: 0.01883309528763805\nSEED: 1, FOLD: 3, EPOCH: 8,train_loss: 0.016849510304197884, valid_loss: 0.018690407888165544\nSEED: 1, FOLD: 3, EPOCH: 9,train_loss: 0.016717286551020283, valid_loss: 0.018685540051332543\nSEED: 1, FOLD: 3, EPOCH: 10,train_loss: 0.01663884356536943, valid_loss: 0.01863298879138061\nSEED: 1, FOLD: 3, EPOCH: 11,train_loss: 0.01639962861098457, valid_loss: 0.01896029444677489\nSEED: 1, FOLD: 3, EPOCH: 12,train_loss: 0.016204952734751976, valid_loss: 0.01904762743839196\nSEED: 1, FOLD: 3, EPOCH: 13,train_loss: 0.015864516399206892, valid_loss: 0.01883059356893812\nSEED: 1, FOLD: 3, EPOCH: 14,train_loss: 0.015431147804348797, valid_loss: 0.01907460168004036\nSEED: 1, FOLD: 3, EPOCH: 15,train_loss: 0.014648456551620493, valid_loss: 0.020413831514971597\nSEED: 1, FOLD: 3, EPOCH: 16,train_loss: 0.013775588033477898, valid_loss: 0.01977121186043535\nSEED: 1, FOLD: 3, EPOCH: 17,train_loss: 0.012499635681455982, valid_loss: 0.020736518129706383\nSEED: 1, FOLD: 3, EPOCH: 18,train_loss: 0.011015313157838756, valid_loss: 0.021117625491959707\nSEED: 1, FOLD: 3, EPOCH: 19,train_loss: 0.00932791138715718, valid_loss: 0.02159321089940412\nSEED: 1, FOLD: 3, EPOCH: 20,train_loss: 0.00784361371230604, valid_loss: 0.021775165359888757\nSEED: 1, FOLD: 3, EPOCH: 21,train_loss: 0.006781358997324022, valid_loss: 0.021942167835576194\nSEED: 1, FOLD: 3, EPOCH: 22,train_loss: 0.0061725872312335004, valid_loss: 0.021874113540564264\nSEED: 1, FOLD: 3, EPOCH: 23,train_loss: 0.0058730649795599174, valid_loss: 0.021851858124136925\nSEED: 1, FOLD: 3, EPOCH: 24,train_loss: 0.005775377620011568, valid_loss: 0.021869099725569996\nFOLD: 4, EPOCH: 0,train_loss: 0.7326514294547756, valid_loss: 0.7044573613575527\nSEED: 1, FOLD: 4, EPOCH: 0,train_loss: 0.46983665333938424, valid_loss: 0.024401525301592692\nSEED: 1, FOLD: 4, EPOCH: 1,train_loss: 0.021671846237060796, valid_loss: 0.02728052373443331\nSEED: 1, FOLD: 4, EPOCH: 2,train_loss: 0.020236331588812988, valid_loss: 0.019282412555600915\nSEED: 1, FOLD: 4, EPOCH: 3,train_loss: 0.018750566214214275, valid_loss: 0.01848030649125576\nSEED: 1, FOLD: 4, EPOCH: 4,train_loss: 0.0179739446739537, valid_loss: 0.018420505709946154\nSEED: 1, FOLD: 4, EPOCH: 5,train_loss: 0.01748853118369614, valid_loss: 0.018461003793137414\nSEED: 1, FOLD: 4, EPOCH: 6,train_loss: 0.017110853816253425, valid_loss: 0.904168009438685\nSEED: 1, FOLD: 4, EPOCH: 7,train_loss: 0.01682569463159481, valid_loss: 0.018775495860193458\nSEED: 1, FOLD: 4, EPOCH: 8,train_loss: 0.016675915460299402, valid_loss: 0.018543672348771776\nSEED: 1, FOLD: 4, EPOCH: 9,train_loss: 0.016693106138684452, valid_loss: 0.01867725383490324\nSEED: 1, FOLD: 4, EPOCH: 10,train_loss: 0.016581122246808813, valid_loss: 0.01868075763008424\nSEED: 1, FOLD: 4, EPOCH: 11,train_loss: 0.016238903667587432, valid_loss: 0.018626152617590767\nSEED: 1, FOLD: 4, EPOCH: 12,train_loss: 0.016007121903889806, valid_loss: 0.019292080056454455\nSEED: 1, FOLD: 4, EPOCH: 13,train_loss: 0.015559116480396177, valid_loss: 0.019068785517343454\nSEED: 1, FOLD: 4, EPOCH: 14,train_loss: 0.01492233271880524, valid_loss: 0.019558498103703772\nSEED: 1, FOLD: 4, EPOCH: 15,train_loss: 0.01429994402276556, valid_loss: 0.02191679695887225\nSEED: 1, FOLD: 4, EPOCH: 16,train_loss: 0.013322121334554505, valid_loss: 0.02080930753477982\nSEED: 1, FOLD: 4, EPOCH: 17,train_loss: 0.012001419929366042, valid_loss: 0.02099574665938105\nSEED: 1, FOLD: 4, EPOCH: 18,train_loss: 0.01058577175122978, valid_loss: 0.021836322173476218\nSEED: 1, FOLD: 4, EPOCH: 19,train_loss: 0.009100987589544189, valid_loss: 0.02189605012536049\nSEED: 1, FOLD: 4, EPOCH: 20,train_loss: 0.007693428726336599, valid_loss: 0.022141766494938306\nSEED: 1, FOLD: 4, EPOCH: 21,train_loss: 0.0067344275915002735, valid_loss: 0.02209813216967242\nSEED: 1, FOLD: 4, EPOCH: 22,train_loss: 0.006115931076045237, valid_loss: 0.02221883444913796\nSEED: 1, FOLD: 4, EPOCH: 23,train_loss: 0.0058156204219554026, valid_loss: 0.022176870544041907\nSEED: 1, FOLD: 4, EPOCH: 24,train_loss: 0.005713239539904098, valid_loss: 0.022179819430623735\nFOLD: 0, EPOCH: 0,train_loss: 0.7316791264043339, valid_loss: 0.6929982304573059\nSEED: 2, FOLD: 0, EPOCH: 0,train_loss: 0.4702505792450646, valid_loss: 0.022944854040231024\nSEED: 2, FOLD: 0, EPOCH: 1,train_loss: 0.021650737326970135, valid_loss: 812.5395028604993\nSEED: 2, FOLD: 0, EPOCH: 2,train_loss: 0.020684365884981293, valid_loss: 0.02109117747417518\nSEED: 2, FOLD: 0, EPOCH: 3,train_loss: 0.019234242606098236, valid_loss: 0.017974716424942017\nSEED: 2, FOLD: 0, EPOCH: 4,train_loss: 0.018349704467623993, valid_loss: 0.017673621220248088\nSEED: 2, FOLD: 0, EPOCH: 5,train_loss: 0.017864197425112343, valid_loss: 0.01795240185622658\nSEED: 2, FOLD: 0, EPOCH: 6,train_loss: 0.01760062160294341, valid_loss: 0.017601817367332323\nSEED: 2, FOLD: 0, EPOCH: 7,train_loss: 0.017361299789416185, valid_loss: 0.019029036563422\nSEED: 2, FOLD: 0, EPOCH: 8,train_loss: 0.017134288643095373, valid_loss: 0.017627423256635667\nSEED: 2, FOLD: 0, EPOCH: 9,train_loss: 0.017086644238535908, valid_loss: 0.01744800212660006\nSEED: 2, FOLD: 0, EPOCH: 10,train_loss: 0.016867639233722635, valid_loss: 0.017706324266535897\nSEED: 2, FOLD: 0, EPOCH: 11,train_loss: 0.016750941599678736, valid_loss: 0.017733962115432534\nSEED: 2, FOLD: 0, EPOCH: 12,train_loss: 0.016452840131208086, valid_loss: 0.018126140800969942\nSEED: 2, FOLD: 0, EPOCH: 13,train_loss: 0.0162385827726752, valid_loss: 0.017986532779676573\nSEED: 2, FOLD: 0, EPOCH: 14,train_loss: 0.015786175123429384, valid_loss: 0.017944102681108885\nSEED: 2, FOLD: 0, EPOCH: 15,train_loss: 0.015195709443988575, valid_loss: 0.018193613258855684\nSEED: 2, FOLD: 0, EPOCH: 16,train_loss: 0.014378103980983513, valid_loss: 0.019203413543956622\nSEED: 2, FOLD: 0, EPOCH: 17,train_loss: 0.013261755604458891, valid_loss: 0.01925226894340345\nSEED: 2, FOLD: 0, EPOCH: 18,train_loss: 0.012061517159252064, valid_loss: 0.019802186425243104\nSEED: 2, FOLD: 0, EPOCH: 19,train_loss: 0.010449861776947544, valid_loss: 0.020221912621387412\nSEED: 2, FOLD: 0, EPOCH: 20,train_loss: 0.00884717149808463, valid_loss: 0.020539353575025288\nSEED: 2, FOLD: 0, EPOCH: 21,train_loss: 0.007550154195126632, valid_loss: 0.020317613945475647\nSEED: 2, FOLD: 0, EPOCH: 22,train_loss: 0.006758822890781406, valid_loss: 0.020559797968183247\nSEED: 2, FOLD: 0, EPOCH: 23,train_loss: 0.0063428165850000105, valid_loss: 0.020663139490144592\nSEED: 2, FOLD: 0, EPOCH: 24,train_loss: 0.006210137877370353, valid_loss: 0.0206795244078551\nFOLD: 1, EPOCH: 0,train_loss: 0.7309916386569756, valid_loss: 0.6889405710356576\nSEED: 2, FOLD: 1, EPOCH: 0,train_loss: 0.4692039661433386, valid_loss: 0.02568268621606486\nSEED: 2, FOLD: 1, EPOCH: 1,train_loss: 0.021864001855146194, valid_loss: 0.020634548578943524\nSEED: 2, FOLD: 1, EPOCH: 2,train_loss: 0.0200724381737519, valid_loss: 0.018431987560221127\nSEED: 2, FOLD: 1, EPOCH: 3,train_loss: 0.0188337921610345, valid_loss: 0.018065238185226918\nSEED: 2, FOLD: 1, EPOCH: 4,train_loss: 0.018204557581170313, valid_loss: 0.017820380069315432\nSEED: 2, FOLD: 1, EPOCH: 5,train_loss: 0.017799428475183853, valid_loss: 0.01776593987430845\nSEED: 2, FOLD: 1, EPOCH: 6,train_loss: 0.017502823635341898, valid_loss: 0.4035938018134662\nSEED: 2, FOLD: 1, EPOCH: 7,train_loss: 0.017310110379712307, valid_loss: 0.017550987750291826\nSEED: 2, FOLD: 1, EPOCH: 8,train_loss: 0.017217535093642662, valid_loss: 0.06823610062045711\nSEED: 2, FOLD: 1, EPOCH: 9,train_loss: 0.01713396191758954, valid_loss: 0.12352479211986064\nSEED: 2, FOLD: 1, EPOCH: 10,train_loss: 0.016934598708336336, valid_loss: 0.01956980430654117\nSEED: 2, FOLD: 1, EPOCH: 11,train_loss: 0.01675662580553604, valid_loss: 0.01796835746083941\nSEED: 2, FOLD: 1, EPOCH: 12,train_loss: 0.016549252891454144, valid_loss: 0.017865866741963794\nSEED: 2, FOLD: 1, EPOCH: 13,train_loss: 0.016200444154927263, valid_loss: 0.018278977328113146\nSEED: 2, FOLD: 1, EPOCH: 14,train_loss: 0.01593015449580507, valid_loss: 0.018111096961157664\nSEED: 2, FOLD: 1, EPOCH: 15,train_loss: 0.015211036226347736, valid_loss: 0.018328542209097316\nSEED: 2, FOLD: 1, EPOCH: 16,train_loss: 0.0143656678551781, valid_loss: 0.01865814072745187\nSEED: 2, FOLD: 1, EPOCH: 17,train_loss: 0.013502017628617477, valid_loss: 0.01907531949026244\nSEED: 2, FOLD: 1, EPOCH: 18,train_loss: 0.012127605199381926, valid_loss: 0.01943591263677393\nSEED: 2, FOLD: 1, EPOCH: 19,train_loss: 0.010477681097615025, valid_loss: 0.019705411046743393\nSEED: 2, FOLD: 1, EPOCH: 20,train_loss: 0.00891214531198468, valid_loss: 0.01999519568468843\nSEED: 2, FOLD: 1, EPOCH: 21,train_loss: 0.0076775387354681025, valid_loss: 0.020226607684578214\nSEED: 2, FOLD: 1, EPOCH: 22,train_loss: 0.0068639675965127735, valid_loss: 0.02039012648165226\nSEED: 2, FOLD: 1, EPOCH: 23,train_loss: 0.006450664633345129, valid_loss: 0.020495653578213282\nSEED: 2, FOLD: 1, EPOCH: 24,train_loss: 0.006291793649206343, valid_loss: 0.020449853794915335\nFOLD: 2, EPOCH: 0,train_loss: 0.7312282958756322, valid_loss: 0.6901773078101022\nSEED: 2, FOLD: 2, EPOCH: 0,train_loss: 0.4691007705810277, valid_loss: 0.024494806144918713\nSEED: 2, FOLD: 2, EPOCH: 1,train_loss: 0.02170397735376289, valid_loss: 0.021721470994608742\nSEED: 2, FOLD: 2, EPOCH: 2,train_loss: 0.020328699071230232, valid_loss: 13.55055556478245\nSEED: 2, FOLD: 2, EPOCH: 3,train_loss: 0.01894392903921181, valid_loss: 0.018323259108832906\nSEED: 2, FOLD: 2, EPOCH: 4,train_loss: 0.018302950009271717, valid_loss: 0.017974250204861163\nSEED: 2, FOLD: 2, EPOCH: 5,train_loss: 0.01775716893725853, valid_loss: 0.018640122402991566\nSEED: 2, FOLD: 2, EPOCH: 6,train_loss: 0.017446926720710337, valid_loss: 0.01800791405673538\nSEED: 2, FOLD: 2, EPOCH: 7,train_loss: 0.017318081198449152, valid_loss: 0.018054324654596193\nSEED: 2, FOLD: 2, EPOCH: 8,train_loss: 0.01716763657801177, valid_loss: 13.087736659497022\nSEED: 2, FOLD: 2, EPOCH: 9,train_loss: 0.016991837962490063, valid_loss: 33.75297372192144\nSEED: 2, FOLD: 2, EPOCH: 10,train_loss: 0.016837358413993017, valid_loss: 13.184428609428661\nSEED: 2, FOLD: 2, EPOCH: 11,train_loss: 0.016743646921131058, valid_loss: 0.24776016814368113\nSEED: 2, FOLD: 2, EPOCH: 12,train_loss: 0.016530479945620333, valid_loss: 0.019959548488259315\nSEED: 2, FOLD: 2, EPOCH: 13,train_loss: 0.016274306936648445, valid_loss: 0.026693337543734482\nSEED: 2, FOLD: 2, EPOCH: 14,train_loss: 0.015844431219865448, valid_loss: 0.01869548721505063\nSEED: 2, FOLD: 2, EPOCH: 15,train_loss: 0.015413139663312746, valid_loss: 0.019128812849521636\nSEED: 2, FOLD: 2, EPOCH: 16,train_loss: 0.01467228298196974, valid_loss: 0.019821348626698765\nSEED: 2, FOLD: 2, EPOCH: 17,train_loss: 0.013551932172444851, valid_loss: 0.01971787762429033\nSEED: 2, FOLD: 2, EPOCH: 18,train_loss: 0.012346625928699539, valid_loss: 0.020618144742080142\nSEED: 2, FOLD: 2, EPOCH: 19,train_loss: 0.010721177875023821, valid_loss: 0.023515691608190538\nSEED: 2, FOLD: 2, EPOCH: 20,train_loss: 0.00917347667036929, valid_loss: 0.020819783210754395\nSEED: 2, FOLD: 2, EPOCH: 21,train_loss: 0.008096783978702582, valid_loss: 0.02115422540477344\nSEED: 2, FOLD: 2, EPOCH: 22,train_loss: 0.007289291254879124, valid_loss: 0.02117188796401024\nSEED: 2, FOLD: 2, EPOCH: 23,train_loss: 0.006745452091664723, valid_loss: 0.021353799051472118\nSEED: 2, FOLD: 2, EPOCH: 24,train_loss: 0.006563556174734148, valid_loss: 0.021300816110202243\nFOLD: 3, EPOCH: 0,train_loss: 0.7316494101155413, valid_loss: 0.6929674199649266\nSEED: 2, FOLD: 3, EPOCH: 0,train_loss: 0.4710747242028261, valid_loss: 0.023895507412297384\nSEED: 2, FOLD: 3, EPOCH: 1,train_loss: 0.021754568021227844, valid_loss: 0.021802722875561033\nSEED: 2, FOLD: 3, EPOCH: 2,train_loss: 0.020665913697903174, valid_loss: 0.019600150840623037\nSEED: 2, FOLD: 3, EPOCH: 3,train_loss: 0.019293719114069522, valid_loss: 0.018654156316603932\nSEED: 2, FOLD: 3, EPOCH: 4,train_loss: 0.018440812894136367, valid_loss: 0.01848495075745242\nSEED: 2, FOLD: 3, EPOCH: 5,train_loss: 0.017938225482502124, valid_loss: 0.018259448637919767\nSEED: 2, FOLD: 3, EPOCH: 6,train_loss: 0.0175360493620273, valid_loss: 0.018150258303752966\nSEED: 2, FOLD: 3, EPOCH: 7,train_loss: 0.01723498590698425, valid_loss: 0.018175795461450303\nSEED: 2, FOLD: 3, EPOCH: 8,train_loss: 0.017004132889428713, valid_loss: 0.01811381086174931\nSEED: 2, FOLD: 3, EPOCH: 9,train_loss: 0.01684446095142269, valid_loss: 0.018557738033788546\nSEED: 2, FOLD: 3, EPOCH: 10,train_loss: 0.016690149318671573, valid_loss: 0.018244882433542182\nSEED: 2, FOLD: 3, EPOCH: 11,train_loss: 0.016342620474089235, valid_loss: 0.018600227683782576\nSEED: 2, FOLD: 3, EPOCH: 12,train_loss: 0.016134688025680338, valid_loss: 0.018736987241676877\nSEED: 2, FOLD: 3, EPOCH: 13,train_loss: 0.01570229934542066, valid_loss: 0.01867948624172381\nSEED: 2, FOLD: 3, EPOCH: 14,train_loss: 0.015185272123963728, valid_loss: 0.019058898996029583\nSEED: 2, FOLD: 3, EPOCH: 15,train_loss: 0.01436199714857949, valid_loss: 0.01936540390763964\nSEED: 2, FOLD: 3, EPOCH: 16,train_loss: 0.013470234410551778, valid_loss: 0.019953920585768562\nSEED: 2, FOLD: 3, EPOCH: 17,train_loss: 0.01224675476822975, valid_loss: 0.020390136912465096\nSEED: 2, FOLD: 3, EPOCH: 18,train_loss: 0.010891139255768626, valid_loss: 0.02105881491942065\nSEED: 2, FOLD: 3, EPOCH: 19,train_loss: 0.009482895860271732, valid_loss: 0.021481753566435405\nSEED: 2, FOLD: 3, EPOCH: 20,train_loss: 0.008096093536239036, valid_loss: 0.021804718566792353\nSEED: 2, FOLD: 3, EPOCH: 21,train_loss: 0.00701536178371332, valid_loss: 0.02186424380966595\nSEED: 2, FOLD: 3, EPOCH: 22,train_loss: 0.006395256279349109, valid_loss: 0.021892964999590602\nSEED: 2, FOLD: 3, EPOCH: 23,train_loss: 0.006076377649947892, valid_loss: 0.022016707488468717\nSEED: 2, FOLD: 3, EPOCH: 24,train_loss: 0.0059614233896952045, valid_loss: 0.02200257017144135\nFOLD: 4, EPOCH: 0,train_loss: 0.7316205721834431, valid_loss: 0.691992437839508\nSEED: 2, FOLD: 4, EPOCH: 0,train_loss: 0.4694807737984735, valid_loss: 0.025273887813091277\nSEED: 2, FOLD: 4, EPOCH: 1,train_loss: 0.021482403903011826, valid_loss: 0.02133480188037668\nSEED: 2, FOLD: 4, EPOCH: 2,train_loss: 0.01981034762887419, valid_loss: 0.019490951778633255\nSEED: 2, FOLD: 4, EPOCH: 3,train_loss: 0.018529423107595547, valid_loss: 0.018748574224965914\nSEED: 2, FOLD: 4, EPOCH: 4,train_loss: 0.01787156803344471, valid_loss: 0.019533000460692816\nSEED: 2, FOLD: 4, EPOCH: 5,train_loss: 0.01747298281153907, valid_loss: 0.018698565768344062\nSEED: 2, FOLD: 4, EPOCH: 6,train_loss: 0.017138189804888723, valid_loss: 0.019229951394455772\nSEED: 2, FOLD: 4, EPOCH: 7,train_loss: 0.016944404178555462, valid_loss: 0.018674829761896813\nSEED: 2, FOLD: 4, EPOCH: 8,train_loss: 0.016857446421045755, valid_loss: 0.019227343425154687\nSEED: 2, FOLD: 4, EPOCH: 9,train_loss: 0.01677483736631879, valid_loss: 0.019270127000553267\nSEED: 2, FOLD: 4, EPOCH: 10,train_loss: 0.016620459298238806, valid_loss: 0.018722920838211265\nSEED: 2, FOLD: 4, EPOCH: 11,train_loss: 0.016529333379551554, valid_loss: 0.01899599879980087\nSEED: 2, FOLD: 4, EPOCH: 12,train_loss: 0.016455388513218233, valid_loss: 1.0654038453740733\nSEED: 2, FOLD: 4, EPOCH: 13,train_loss: 0.0160503546923291, valid_loss: 0.019376955713544572\nSEED: 2, FOLD: 4, EPOCH: 14,train_loss: 0.0157098191169401, valid_loss: 0.01984121131577662\nSEED: 2, FOLD: 4, EPOCH: 15,train_loss: 0.015189291092742613, valid_loss: 0.019369472989014216\nSEED: 2, FOLD: 4, EPOCH: 16,train_loss: 0.014457312938959702, valid_loss: 0.03868400811084679\nSEED: 2, FOLD: 4, EPOCH: 17,train_loss: 0.013545041917350845, valid_loss: 0.020571409431951387\nSEED: 2, FOLD: 4, EPOCH: 18,train_loss: 0.012230499006429876, valid_loss: 0.021388931572437285\nSEED: 2, FOLD: 4, EPOCH: 19,train_loss: 0.01066753133267596, valid_loss: 0.021278324191059385\nSEED: 2, FOLD: 4, EPOCH: 20,train_loss: 0.009034035605904848, valid_loss: 0.02161712098334517\nSEED: 2, FOLD: 4, EPOCH: 21,train_loss: 0.007676951350995164, valid_loss: 0.021955830497401102\nSEED: 2, FOLD: 4, EPOCH: 22,train_loss: 0.0067884217529301195, valid_loss: 0.021979934456092972\nSEED: 2, FOLD: 4, EPOCH: 23,train_loss: 0.0063482758055940485, valid_loss: 0.022017424181103706\nSEED: 2, FOLD: 4, EPOCH: 24,train_loss: 0.006182354294519493, valid_loss: 0.02205586140709264\nFOLD: 0, EPOCH: 0,train_loss: 0.7335456877514936, valid_loss: 0.7021780933652605\nSEED: 3, FOLD: 0, EPOCH: 0,train_loss: 0.4702220311172415, valid_loss: 0.02530229613184929\nSEED: 3, FOLD: 0, EPOCH: 1,train_loss: 0.02178251307349706, valid_loss: 0.02075144462287426\nSEED: 3, FOLD: 0, EPOCH: 2,train_loss: 0.020395713979783264, valid_loss: 0.01873214560161744\nSEED: 3, FOLD: 0, EPOCH: 3,train_loss: 0.019046004105737244, valid_loss: 0.021864416703049627\nSEED: 3, FOLD: 0, EPOCH: 4,train_loss: 0.01841784821337332, valid_loss: 0.01885338802156704\nSEED: 3, FOLD: 0, EPOCH: 5,train_loss: 0.01797841948227606, valid_loss: 30.269020215075994\nSEED: 3, FOLD: 0, EPOCH: 6,train_loss: 0.01766051929039152, valid_loss: 0.01804505094353642\nSEED: 3, FOLD: 0, EPOCH: 7,train_loss: 0.017634153015155724, valid_loss: 0.01836439786212785\nSEED: 3, FOLD: 0, EPOCH: 8,train_loss: 0.017436121199010075, valid_loss: 0.01791467406520886\nSEED: 3, FOLD: 0, EPOCH: 9,train_loss: 0.017257633551523304, valid_loss: 0.024336114026872174\nSEED: 3, FOLD: 0, EPOCH: 10,train_loss: 0.01711877727665115, valid_loss: 0.01798466295003891\nSEED: 3, FOLD: 0, EPOCH: 11,train_loss: 0.016945920074763504, valid_loss: 0.017865330167114734\nSEED: 3, FOLD: 0, EPOCH: 12,train_loss: 0.016680942281432774, valid_loss: 0.017857368104159832\nSEED: 3, FOLD: 0, EPOCH: 13,train_loss: 0.01645492926320952, valid_loss: 0.017914637437622463\nSEED: 3, FOLD: 0, EPOCH: 14,train_loss: 0.016110028477682583, valid_loss: 0.01802574247121811\nSEED: 3, FOLD: 0, EPOCH: 15,train_loss: 0.015653813865197742, valid_loss: 0.018403504130297472\nSEED: 3, FOLD: 0, EPOCH: 16,train_loss: 0.015027623076963684, valid_loss: 0.018429761273520334\nSEED: 3, FOLD: 0, EPOCH: 17,train_loss: 0.014208414929284565, valid_loss: 0.019010180354650533\nSEED: 3, FOLD: 0, EPOCH: 18,train_loss: 0.012942332153518995, valid_loss: 0.019243014553960946\nSEED: 3, FOLD: 0, EPOCH: 19,train_loss: 0.01151228488922335, valid_loss: 0.019665096104810282\nSEED: 3, FOLD: 0, EPOCH: 20,train_loss: 0.009953099809101095, valid_loss: 0.019995325437879987\nSEED: 3, FOLD: 0, EPOCH: 21,train_loss: 0.008468984464264435, valid_loss: 0.0202975720100637\nSEED: 3, FOLD: 0, EPOCH: 22,train_loss: 0.00743121665942928, valid_loss: 0.020372472277709417\nSEED: 3, FOLD: 0, EPOCH: 23,train_loss: 0.006916625037168463, valid_loss: 0.02053976607109819\nSEED: 3, FOLD: 0, EPOCH: 24,train_loss: 0.006695718723821683, valid_loss: 0.02050862944951015\nFOLD: 1, EPOCH: 0,train_loss: 0.7331535846426867, valid_loss: 0.7017844830240522\nSEED: 3, FOLD: 1, EPOCH: 0,train_loss: 0.4701928771573348, valid_loss: 0.02428007179072925\nSEED: 3, FOLD: 1, EPOCH: 1,train_loss: 0.02173010266615429, valid_loss: 0.02058611304632255\nSEED: 3, FOLD: 1, EPOCH: 2,train_loss: 0.020277987450253273, valid_loss: 0.018558450894696373\nSEED: 3, FOLD: 1, EPOCH: 3,train_loss: 0.01890620968534031, valid_loss: 37.80476018908833\nSEED: 3, FOLD: 1, EPOCH: 4,train_loss: 0.018119618634058945, valid_loss: 0.01803316990179675\nSEED: 3, FOLD: 1, EPOCH: 5,train_loss: 0.017643066595537937, valid_loss: 0.01805568624820028\nSEED: 3, FOLD: 1, EPOCH: 6,train_loss: 0.017365644409226767, valid_loss: 0.01790964063256979\nSEED: 3, FOLD: 1, EPOCH: 7,train_loss: 0.017134272466427174, valid_loss: 0.017920106649398804\nSEED: 3, FOLD: 1, EPOCH: 8,train_loss: 0.017067067743535492, valid_loss: 0.02352068658385958\nSEED: 3, FOLD: 1, EPOCH: 9,train_loss: 0.016949479090238827, valid_loss: 0.018061465184603418\nSEED: 3, FOLD: 1, EPOCH: 10,train_loss: 0.01694250247184781, valid_loss: 0.01805821329887424\nSEED: 3, FOLD: 1, EPOCH: 11,train_loss: 0.016799059089111244, valid_loss: 0.36709718927741053\nSEED: 3, FOLD: 1, EPOCH: 12,train_loss: 0.016486418507723272, valid_loss: 0.017952690991972175\nSEED: 3, FOLD: 1, EPOCH: 13,train_loss: 0.016270818026817364, valid_loss: 0.018069328341100897\nSEED: 3, FOLD: 1, EPOCH: 14,train_loss: 0.01589050925458255, valid_loss: 0.01789086844239916\nSEED: 3, FOLD: 1, EPOCH: 15,train_loss: 0.015417245911785227, valid_loss: 0.018202432777200427\nSEED: 3, FOLD: 1, EPOCH: 16,train_loss: 0.014669805846136549, valid_loss: 0.01942749960081918\nSEED: 3, FOLD: 1, EPOCH: 17,train_loss: 0.013677806107570295, valid_loss: 0.019021334924868176\nSEED: 3, FOLD: 1, EPOCH: 18,train_loss: 0.012357892063648804, valid_loss: 0.019724327751568387\nSEED: 3, FOLD: 1, EPOCH: 19,train_loss: 0.010653473828257858, valid_loss: 0.020085486343928746\nSEED: 3, FOLD: 1, EPOCH: 20,train_loss: 0.008992749873710714, valid_loss: 0.020448907411524227\nSEED: 3, FOLD: 1, EPOCH: 21,train_loss: 0.007678866005111216, valid_loss: 0.020695483152355466\nSEED: 3, FOLD: 1, EPOCH: 22,train_loss: 0.0067587175872176886, valid_loss: 0.020880409862313952\nSEED: 3, FOLD: 1, EPOCH: 23,train_loss: 0.006318317311208533, valid_loss: 0.020981426537036895\nSEED: 3, FOLD: 1, EPOCH: 24,train_loss: 0.006146137079601918, valid_loss: 0.020946198595421656\nFOLD: 2, EPOCH: 0,train_loss: 0.7332160290140305, valid_loss: 0.7014123831476484\nSEED: 3, FOLD: 2, EPOCH: 0,train_loss: 0.4706819709390402, valid_loss: 0.02472070466194834\nSEED: 3, FOLD: 2, EPOCH: 1,train_loss: 0.02199273575505201, valid_loss: 0.02173606934291976\nSEED: 3, FOLD: 2, EPOCH: 2,train_loss: 0.020463820533269513, valid_loss: 0.019565114166055407\nSEED: 3, FOLD: 2, EPOCH: 3,train_loss: 0.01903480447720002, valid_loss: 0.018991662827985627\nSEED: 3, FOLD: 2, EPOCH: 4,train_loss: 0.018328402069036978, valid_loss: 0.024732377167258943\nSEED: 3, FOLD: 2, EPOCH: 5,train_loss: 0.017931507900357246, valid_loss: 0.018967579305171966\nSEED: 3, FOLD: 2, EPOCH: 6,train_loss: 0.017763687392873484, valid_loss: 0.018641829783363\nSEED: 3, FOLD: 2, EPOCH: 7,train_loss: 0.017400767327877726, valid_loss: 0.019023722037672997\nSEED: 3, FOLD: 2, EPOCH: 8,train_loss: 0.017242363110239054, valid_loss: 0.018655288432325636\nSEED: 3, FOLD: 2, EPOCH: 9,train_loss: 0.01699770157001097, valid_loss: 0.01873142501073224\nSEED: 3, FOLD: 2, EPOCH: 10,train_loss: 0.016864770703887852, valid_loss: 0.018299564187015806\nSEED: 3, FOLD: 2, EPOCH: 11,train_loss: 0.01663053094198669, valid_loss: 0.018786227676485266\nSEED: 3, FOLD: 2, EPOCH: 12,train_loss: 0.01631210046908716, valid_loss: 0.018643517792224885\nSEED: 3, FOLD: 2, EPOCH: 13,train_loss: 0.016000463926389704, valid_loss: 0.018859208508261612\nSEED: 3, FOLD: 2, EPOCH: 14,train_loss: 0.01562922296325003, valid_loss: 0.01870725854699101\nSEED: 3, FOLD: 2, EPOCH: 15,train_loss: 0.015070322819434814, valid_loss: 0.019655133464506695\nSEED: 3, FOLD: 2, EPOCH: 16,train_loss: 0.014309987031521589, valid_loss: 0.020197951634015356\nSEED: 3, FOLD: 2, EPOCH: 17,train_loss: 0.013334003576233874, valid_loss: 0.019973576707499368\nSEED: 3, FOLD: 2, EPOCH: 18,train_loss: 0.012174671182721636, valid_loss: 0.02050068559391158\nSEED: 3, FOLD: 2, EPOCH: 19,train_loss: 0.01084047535529537, valid_loss: 0.020827750914863177\nSEED: 3, FOLD: 2, EPOCH: 20,train_loss: 0.00935199345550398, valid_loss: 0.021269593547497478\nSEED: 3, FOLD: 2, EPOCH: 21,train_loss: 0.008110658041317097, valid_loss: 0.021393723306911333\nSEED: 3, FOLD: 2, EPOCH: 22,train_loss: 0.007236230207511979, valid_loss: 0.02160737035529954\nSEED: 3, FOLD: 2, EPOCH: 23,train_loss: 0.006757741566257973, valid_loss: 0.021539070350783212\nSEED: 3, FOLD: 2, EPOCH: 24,train_loss: 0.006566665628177188, valid_loss: 0.021549325329916817\nFOLD: 3, EPOCH: 0,train_loss: 0.7330808902996174, valid_loss: 0.6999652368681771\nSEED: 3, FOLD: 3, EPOCH: 0,train_loss: 0.4696438139493483, valid_loss: 0.024151586049369404\nSEED: 3, FOLD: 3, EPOCH: 1,train_loss: 0.021709081568363785, valid_loss: 0.02052308762712138\nSEED: 3, FOLD: 3, EPOCH: 2,train_loss: 0.020268452731703503, valid_loss: 0.018686613706605775\nSEED: 3, FOLD: 3, EPOCH: 3,train_loss: 0.019084968874096008, valid_loss: 0.021750707738101484\nSEED: 3, FOLD: 3, EPOCH: 4,train_loss: 0.01846094582688765, valid_loss: 0.018292708482061115\nSEED: 3, FOLD: 3, EPOCH: 5,train_loss: 0.017942132947939463, valid_loss: 0.018062093295156956\nSEED: 3, FOLD: 3, EPOCH: 6,train_loss: 0.017659460552090753, valid_loss: 0.018460536535297122\nSEED: 3, FOLD: 3, EPOCH: 7,train_loss: 0.017360315455690674, valid_loss: 0.018214409026716435\nSEED: 3, FOLD: 3, EPOCH: 8,train_loss: 0.017132105076334614, valid_loss: 0.018124612207923618\nSEED: 3, FOLD: 3, EPOCH: 9,train_loss: 0.016964721898345844, valid_loss: 0.01840576332594667\nSEED: 3, FOLD: 3, EPOCH: 10,train_loss: 0.0168754318729043, valid_loss: 0.017983365298381872\nSEED: 3, FOLD: 3, EPOCH: 11,train_loss: 0.016676551183226748, valid_loss: 0.018355448544025422\nSEED: 3, FOLD: 3, EPOCH: 12,train_loss: 0.01635564075431962, valid_loss: 0.018355923970895156\nSEED: 3, FOLD: 3, EPOCH: 13,train_loss: 0.016071844731282064, valid_loss: 0.018399575405887197\nSEED: 3, FOLD: 3, EPOCH: 14,train_loss: 0.01566729465148587, valid_loss: 0.019336193108132907\nSEED: 3, FOLD: 3, EPOCH: 15,train_loss: 0.015239515017880045, valid_loss: 0.018911276864154\nSEED: 3, FOLD: 3, EPOCH: 16,train_loss: 0.01446459007759889, valid_loss: 0.019173689346228328\nSEED: 3, FOLD: 3, EPOCH: 17,train_loss: 0.013502652365444363, valid_loss: 0.019624742014067515\nSEED: 3, FOLD: 3, EPOCH: 18,train_loss: 0.012077561593142109, valid_loss: 0.0201087336987257\nSEED: 3, FOLD: 3, EPOCH: 19,train_loss: 0.010489738957983429, valid_loss: 0.020514032670429776\nSEED: 3, FOLD: 3, EPOCH: 20,train_loss: 0.009021942753452753, valid_loss: 0.02086180097290448\nSEED: 3, FOLD: 3, EPOCH: 21,train_loss: 0.007730317420150706, valid_loss: 0.021369538935167447\nSEED: 3, FOLD: 3, EPOCH: 22,train_loss: 0.00689086108468473, valid_loss: 0.02104791205908571\nSEED: 3, FOLD: 3, EPOCH: 23,train_loss: 0.006445082336448241, valid_loss: 0.021062730625271798\nSEED: 3, FOLD: 3, EPOCH: 24,train_loss: 0.006313405269621939, valid_loss: 0.02108997032046318\nFOLD: 4, EPOCH: 0,train_loss: 0.7331804267681428, valid_loss: 0.7040667278426034\nSEED: 3, FOLD: 4, EPOCH: 0,train_loss: 0.47120354842584933, valid_loss: 0.024491594039968083\nSEED: 3, FOLD: 4, EPOCH: 1,train_loss: 0.02189886024779212, valid_loss: 0.020592315707887922\nSEED: 3, FOLD: 4, EPOCH: 2,train_loss: 0.02062670368510876, valid_loss: 0.018780013333473886\nSEED: 3, FOLD: 4, EPOCH: 3,train_loss: 0.01926073310964734, valid_loss: 0.017887835444084236\nSEED: 3, FOLD: 4, EPOCH: 4,train_loss: 0.01837451236635229, valid_loss: 0.017744873304452215\nSEED: 3, FOLD: 4, EPOCH: 5,train_loss: 0.01795042945874216, valid_loss: 0.01777755195008857\nSEED: 3, FOLD: 4, EPOCH: 6,train_loss: 0.017611850719273524, valid_loss: 0.23380259773028747\nSEED: 3, FOLD: 4, EPOCH: 7,train_loss: 0.01731049705867785, valid_loss: 0.018200211519641536\nSEED: 3, FOLD: 4, EPOCH: 8,train_loss: 0.01717504241034715, valid_loss: 0.017809169207300458\nSEED: 3, FOLD: 4, EPOCH: 9,train_loss: 0.017051401289764546, valid_loss: 0.017732111950005805\nSEED: 3, FOLD: 4, EPOCH: 10,train_loss: 0.016846547909352902, valid_loss: 0.018203031670834337\nSEED: 3, FOLD: 4, EPOCH: 11,train_loss: 0.016724955141000503, valid_loss: 0.0177844785419958\nSEED: 3, FOLD: 4, EPOCH: 12,train_loss: 0.016518130912071598, valid_loss: 0.017909793662173407\nSEED: 3, FOLD: 4, EPOCH: 13,train_loss: 0.01619534627500459, valid_loss: 0.01776737373854433\nSEED: 3, FOLD: 4, EPOCH: 14,train_loss: 0.01580930737112343, valid_loss: 0.14904016639505113\nSEED: 3, FOLD: 4, EPOCH: 15,train_loss: 0.015611198798746524, valid_loss: 0.018085439396756036\nSEED: 3, FOLD: 4, EPOCH: 16,train_loss: 0.014676282032780404, valid_loss: 0.01885012664965221\nSEED: 3, FOLD: 4, EPOCH: 17,train_loss: 0.013677767260394392, valid_loss: 0.019142783699291094\nSEED: 3, FOLD: 4, EPOCH: 18,train_loss: 0.012302673204264937, valid_loss: 0.019874113744923046\nSEED: 3, FOLD: 4, EPOCH: 19,train_loss: 0.010707452342621167, valid_loss: 0.020222465747169086\nSEED: 3, FOLD: 4, EPOCH: 20,train_loss: 0.009143256133653386, valid_loss: 0.02041981481015682\nSEED: 3, FOLD: 4, EPOCH: 21,train_loss: 0.007794526171102358, valid_loss: 0.020514201585735595\nSEED: 3, FOLD: 4, EPOCH: 22,train_loss: 0.0069272175879917875, valid_loss: 0.020536835757749422\nSEED: 3, FOLD: 4, EPOCH: 23,train_loss: 0.006469702436486735, valid_loss: 0.020595888420939446\nSEED: 3, FOLD: 4, EPOCH: 24,train_loss: 0.0063382399907458, valid_loss: 0.020621133223176002\nFOLD: 0, EPOCH: 0,train_loss: 0.733754473319952, valid_loss: 0.695405924320221\nSEED: 4, FOLD: 0, EPOCH: 0,train_loss: 0.4680735827221171, valid_loss: 0.024467648885079792\nSEED: 4, FOLD: 0, EPOCH: 1,train_loss: 0.021612441072753376, valid_loss: 0.020710616292698044\nSEED: 4, FOLD: 0, EPOCH: 2,train_loss: 0.020396591191166553, valid_loss: 0.020262358923043525\nSEED: 4, FOLD: 0, EPOCH: 3,train_loss: 0.019228630051340744, valid_loss: 0.019260741184864726\nSEED: 4, FOLD: 0, EPOCH: 4,train_loss: 0.01861711985607078, valid_loss: 202.25520723760127\nSEED: 4, FOLD: 0, EPOCH: 5,train_loss: 0.01813989085401746, valid_loss: 4.949755712119597\nSEED: 4, FOLD: 0, EPOCH: 6,train_loss: 0.0178061470227397, valid_loss: 3.9903491270861458\nSEED: 4, FOLD: 0, EPOCH: 7,train_loss: 0.017514000147365143, valid_loss: 0.0179931621200272\nSEED: 4, FOLD: 0, EPOCH: 8,train_loss: 0.017318942503112812, valid_loss: 0.018701447173953056\nSEED: 4, FOLD: 0, EPOCH: 9,train_loss: 0.017095381758459236, valid_loss: 0.02234257427709443\nSEED: 4, FOLD: 0, EPOCH: 10,train_loss: 0.016804525534203953, valid_loss: 0.02191685044339725\nSEED: 4, FOLD: 0, EPOCH: 11,train_loss: 0.016609457739885303, valid_loss: 0.018238461523183754\nSEED: 4, FOLD: 0, EPOCH: 12,train_loss: 0.016276733072447605, valid_loss: 0.019486169304166523\nSEED: 4, FOLD: 0, EPOCH: 13,train_loss: 0.015888178025952715, valid_loss: 2.1996303093220506\nSEED: 4, FOLD: 0, EPOCH: 14,train_loss: 0.015378566647785297, valid_loss: 0.17839543633162974\nSEED: 4, FOLD: 0, EPOCH: 15,train_loss: 0.014705216302873863, valid_loss: 0.02000562468809741\nSEED: 4, FOLD: 0, EPOCH: 16,train_loss: 0.013790404322840597, valid_loss: 0.020529263306941305\nSEED: 4, FOLD: 0, EPOCH: 17,train_loss: 0.01269859276657951, valid_loss: 0.020006000037704197\nSEED: 4, FOLD: 0, EPOCH: 18,train_loss: 0.011285563795894816, valid_loss: 0.02135121471115521\nSEED: 4, FOLD: 0, EPOCH: 19,train_loss: 0.009806889386010775, valid_loss: 0.021402662566729954\nSEED: 4, FOLD: 0, EPOCH: 20,train_loss: 0.008438005284203784, valid_loss: 0.022258819001061577\nSEED: 4, FOLD: 0, EPOCH: 21,train_loss: 0.007333086354765987, valid_loss: 0.02305325312273843\nSEED: 4, FOLD: 0, EPOCH: 22,train_loss: 0.006662996060660352, valid_loss: 0.02310356205063207\nSEED: 4, FOLD: 0, EPOCH: 23,train_loss: 0.006305647801364894, valid_loss: 0.021729598992637225\nSEED: 4, FOLD: 0, EPOCH: 24,train_loss: 0.006155828850856726, valid_loss: 0.027569601738027163\nFOLD: 1, EPOCH: 0,train_loss: 0.7336443809495456, valid_loss: 0.6956124831648434\nSEED: 4, FOLD: 1, EPOCH: 0,train_loss: 0.46978994720763917, valid_loss: 0.023851695470511913\nSEED: 4, FOLD: 1, EPOCH: 1,train_loss: 0.021885919182196907, valid_loss: 13.69665890334941\nSEED: 4, FOLD: 1, EPOCH: 2,train_loss: 0.020064574895777565, valid_loss: 0.01897101816447342\nSEED: 4, FOLD: 1, EPOCH: 3,train_loss: 0.01880185922904723, valid_loss: 0.018688070752164897\nSEED: 4, FOLD: 1, EPOCH: 4,train_loss: 0.018127263267187103, valid_loss: 0.018493763116352698\nSEED: 4, FOLD: 1, EPOCH: 5,train_loss: 0.01766907168633264, valid_loss: 0.01920572087606963\nSEED: 4, FOLD: 1, EPOCH: 6,train_loss: 0.017332941536670147, valid_loss: 0.018412295427611646\nSEED: 4, FOLD: 1, EPOCH: 7,train_loss: 0.016942500831910234, valid_loss: 0.018592494485132834\nSEED: 4, FOLD: 1, EPOCH: 8,train_loss: 0.01668366437534923, valid_loss: 0.018656891661093515\nSEED: 4, FOLD: 1, EPOCH: 9,train_loss: 0.01642207116347508, valid_loss: 0.018677466905073208\nSEED: 4, FOLD: 1, EPOCH: 10,train_loss: 0.016147033102216497, valid_loss: 0.019270963254658616\nSEED: 4, FOLD: 1, EPOCH: 11,train_loss: 0.0158224877010545, valid_loss: 0.019027579164899448\nSEED: 4, FOLD: 1, EPOCH: 12,train_loss: 0.0154881054279057, valid_loss: 0.01926616151981494\nSEED: 4, FOLD: 1, EPOCH: 13,train_loss: 0.014958414323358, valid_loss: 0.019692119320525843\nSEED: 4, FOLD: 1, EPOCH: 14,train_loss: 0.014343449130546356, valid_loss: 0.019845286439008573\nSEED: 4, FOLD: 1, EPOCH: 15,train_loss: 0.013490603602342848, valid_loss: 0.020199728088782114\nSEED: 4, FOLD: 1, EPOCH: 16,train_loss: 0.01240804073387298, valid_loss: 0.021464827415697715\nSEED: 4, FOLD: 1, EPOCH: 17,train_loss: 0.01123676503725026, valid_loss: 0.021609374212429804\nSEED: 4, FOLD: 1, EPOCH: 18,train_loss: 0.009748862597389498, valid_loss: 0.021984967074411756\nSEED: 4, FOLD: 1, EPOCH: 19,train_loss: 0.008340514781277465, valid_loss: 0.02184570038362461\nSEED: 4, FOLD: 1, EPOCH: 20,train_loss: 0.007172782247400154, valid_loss: 0.021903065690661177\nSEED: 4, FOLD: 1, EPOCH: 21,train_loss: 0.0063712209112186365, valid_loss: 0.0218671155436074\nSEED: 4, FOLD: 1, EPOCH: 22,train_loss: 0.005879534158747697, valid_loss: 0.021826739696895376\nSEED: 4, FOLD: 1, EPOCH: 23,train_loss: 0.005625045638990359, valid_loss: 0.021774475079248932\nSEED: 4, FOLD: 1, EPOCH: 24,train_loss: 0.005542537983695882, valid_loss: 0.021837157745133427\nFOLD: 2, EPOCH: 0,train_loss: 0.7337059313836305, valid_loss: 0.6945890733173915\nSEED: 4, FOLD: 2, EPOCH: 0,train_loss: 0.4686448779632, valid_loss: 0.023157321475446226\nSEED: 4, FOLD: 2, EPOCH: 1,train_loss: 0.02152257713664701, valid_loss: 0.020197990483471324\nSEED: 4, FOLD: 2, EPOCH: 2,train_loss: 0.02039450622987056, valid_loss: 0.01850315590522119\nSEED: 4, FOLD: 2, EPOCH: 3,train_loss: 0.01915604567182237, valid_loss: 0.017687950669122595\nSEED: 4, FOLD: 2, EPOCH: 4,train_loss: 0.018324681611704655, valid_loss: 0.017490759744708026\nSEED: 4, FOLD: 2, EPOCH: 5,train_loss: 0.017827271942751133, valid_loss: 0.01813538134364145\nSEED: 4, FOLD: 2, EPOCH: 6,train_loss: 0.017469687049911507, valid_loss: 0.01765400247116174\nSEED: 4, FOLD: 2, EPOCH: 7,train_loss: 0.017249430232829807, valid_loss: 0.02168181368282863\nSEED: 4, FOLD: 2, EPOCH: 8,train_loss: 0.01707354310117122, valid_loss: 0.017564395708697184\nSEED: 4, FOLD: 2, EPOCH: 9,train_loss: 0.01701508413163432, valid_loss: 0.018769067765346596\nSEED: 4, FOLD: 2, EPOCH: 10,train_loss: 0.01686639249648737, valid_loss: 0.05844368724418538\nSEED: 4, FOLD: 2, EPOCH: 11,train_loss: 0.016723547306289707, valid_loss: 0.018202125281095504\nSEED: 4, FOLD: 2, EPOCH: 12,train_loss: 0.016492175599695114, valid_loss: 0.017479107781712497\nSEED: 4, FOLD: 2, EPOCH: 13,train_loss: 0.01621641381306277, valid_loss: 0.018766316665070396\nSEED: 4, FOLD: 2, EPOCH: 14,train_loss: 0.015863992962176384, valid_loss: 0.024819316808134317\nSEED: 4, FOLD: 2, EPOCH: 15,train_loss: 0.015214694414179827, valid_loss: 0.01791902274957725\nSEED: 4, FOLD: 2, EPOCH: 16,train_loss: 0.014419088248109472, valid_loss: 0.01828284077346325\nSEED: 4, FOLD: 2, EPOCH: 17,train_loss: 0.013431822766374418, valid_loss: 0.01880215975855078\nSEED: 4, FOLD: 2, EPOCH: 18,train_loss: 0.011955459696659143, valid_loss: 0.018996573545570883\nSEED: 4, FOLD: 2, EPOCH: 19,train_loss: 0.01037167929166901, valid_loss: 0.02003993909539921\nSEED: 4, FOLD: 2, EPOCH: 20,train_loss: 0.008773338896613837, valid_loss: 0.01987619806480195\nSEED: 4, FOLD: 2, EPOCH: 21,train_loss: 0.0075467425580743866, valid_loss: 0.020113877772486636\nSEED: 4, FOLD: 2, EPOCH: 22,train_loss: 0.006734019438938602, valid_loss: 0.02029821528121829\nSEED: 4, FOLD: 2, EPOCH: 23,train_loss: 0.0063589169724803905, valid_loss: 0.02029684883808451\nSEED: 4, FOLD: 2, EPOCH: 24,train_loss: 0.006204991108751384, valid_loss: 0.020311281011839\nFOLD: 3, EPOCH: 0,train_loss: 0.7338532598122306, valid_loss: 0.6943812131881714\nSEED: 4, FOLD: 3, EPOCH: 0,train_loss: 0.4696593356548228, valid_loss: 0.02560509970145566\nSEED: 4, FOLD: 3, EPOCH: 1,train_loss: 0.02197136820388445, valid_loss: 0.020685620605945587\nSEED: 4, FOLD: 3, EPOCH: 2,train_loss: 0.020924976298018642, valid_loss: 53.48994697422854\nSEED: 4, FOLD: 3, EPOCH: 3,train_loss: 0.020107796753122322, valid_loss: 0.021378163993358613\nSEED: 4, FOLD: 3, EPOCH: 4,train_loss: 0.019370071826151747, valid_loss: 27.537232274402466\nSEED: 4, FOLD: 3, EPOCH: 5,train_loss: 0.0188019458083031, valid_loss: 0.018462159378187997\nSEED: 4, FOLD: 3, EPOCH: 6,train_loss: 0.01835376911027276, valid_loss: 0.02357362514095647\nSEED: 4, FOLD: 3, EPOCH: 7,train_loss: 0.01889764725406101, valid_loss: 0.6740098289613213\nSEED: 4, FOLD: 3, EPOCH: 8,train_loss: 0.018134985874960388, valid_loss: 0.08156628776341676\nSEED: 4, FOLD: 3, EPOCH: 9,train_loss: 0.01777867059074882, valid_loss: 0.026149164406316622\nSEED: 4, FOLD: 3, EPOCH: 10,train_loss: 0.01754619436932431, valid_loss: 0.2861300931711282\nSEED: 4, FOLD: 3, EPOCH: 11,train_loss: 0.01710904990930272, valid_loss: 0.023563895906720842\nSEED: 4, FOLD: 3, EPOCH: 12,train_loss: 0.019191795710366274, valid_loss: 0.018868411598461015\nSEED: 4, FOLD: 3, EPOCH: 13,train_loss: 0.018405256178769945, valid_loss: 0.9658348800880568\nSEED: 4, FOLD: 3, EPOCH: 14,train_loss: 0.019171614557558645, valid_loss: 1.9784555755289537\nSEED: 4, FOLD: 3, EPOCH: 15,train_loss: 0.018319065480128578, valid_loss: 0.8197303865637098\nSEED: 4, FOLD: 3, EPOCH: 16,train_loss: 0.017801358317281458, valid_loss: 0.07857784352132252\nSEED: 4, FOLD: 3, EPOCH: 17,train_loss: 0.017258399267397497, valid_loss: 0.01777147759816476\nSEED: 4, FOLD: 3, EPOCH: 18,train_loss: 0.016951049138130485, valid_loss: 0.09434411073369639\nSEED: 4, FOLD: 3, EPOCH: 19,train_loss: 0.016495650580179863, valid_loss: 0.02852999307215214\nSEED: 4, FOLD: 3, EPOCH: 20,train_loss: 0.017112026564722906, valid_loss: 0.02075331330831562\nSEED: 4, FOLD: 3, EPOCH: 21,train_loss: 0.0167336269059097, valid_loss: 0.018071507156959602\nSEED: 4, FOLD: 3, EPOCH: 22,train_loss: 0.01635296624558775, valid_loss: 0.017936012627823014\nSEED: 4, FOLD: 3, EPOCH: 23,train_loss: 0.016164110258113647, valid_loss: 0.026608397918088094\nSEED: 4, FOLD: 3, EPOCH: 24,train_loss: 0.016020840843734535, valid_loss: 0.018047850472586495\nFOLD: 4, EPOCH: 0,train_loss: 0.7334336699360479, valid_loss: 0.6918671250343322\nSEED: 4, FOLD: 4, EPOCH: 0,train_loss: 0.4699339180994425, valid_loss: 0.02447191732270377\nSEED: 4, FOLD: 4, EPOCH: 1,train_loss: 0.021717628464102745, valid_loss: 0.020848906306283815\nSEED: 4, FOLD: 4, EPOCH: 2,train_loss: 0.020275458584736734, valid_loss: 0.019129659182259014\nSEED: 4, FOLD: 4, EPOCH: 3,train_loss: 0.018924233170538922, valid_loss: 0.018589315935969353\nSEED: 4, FOLD: 4, EPOCH: 4,train_loss: 0.018246406444559132, valid_loss: 0.022755437876496996\nSEED: 4, FOLD: 4, EPOCH: 5,train_loss: 0.01773999216728402, valid_loss: 0.018668680797730174\nSEED: 4, FOLD: 4, EPOCH: 6,train_loss: 0.017431862659099764, valid_loss: 0.03395993845271213\nSEED: 4, FOLD: 4, EPOCH: 7,train_loss: 0.017160142316435374, valid_loss: 0.01844380867800542\nSEED: 4, FOLD: 4, EPOCH: 8,train_loss: 0.016934667727536094, valid_loss: 0.018282419443130492\nSEED: 4, FOLD: 4, EPOCH: 9,train_loss: 0.016768297051371884, valid_loss: 0.018327564187347888\nSEED: 4, FOLD: 4, EPOCH: 10,train_loss: 0.01668070454531125, valid_loss: 0.018405531799154624\nSEED: 4, FOLD: 4, EPOCH: 11,train_loss: 0.01651353539259982, valid_loss: 0.022402514410870416\nSEED: 4, FOLD: 4, EPOCH: 12,train_loss: 0.016195826266423193, valid_loss: 0.01847878218229328\nSEED: 4, FOLD: 4, EPOCH: 13,train_loss: 0.015836090746804747, valid_loss: 0.01869150722133262\nSEED: 4, FOLD: 4, EPOCH: 14,train_loss: 0.015389910090143663, valid_loss: 0.018849384625043188\nSEED: 4, FOLD: 4, EPOCH: 15,train_loss: 0.014695233780972278, valid_loss: 0.019236226699181964\nSEED: 4, FOLD: 4, EPOCH: 16,train_loss: 0.013848483338136307, valid_loss: 0.019573695957660675\nSEED: 4, FOLD: 4, EPOCH: 17,train_loss: 0.012748392781473859, valid_loss: 0.02042516613645213\nSEED: 4, FOLD: 4, EPOCH: 18,train_loss: 0.01130281619890763, valid_loss: 0.02038730861885207\nSEED: 4, FOLD: 4, EPOCH: 19,train_loss: 0.009812253765272397, valid_loss: 0.020868736558726855\nSEED: 4, FOLD: 4, EPOCH: 20,train_loss: 0.008429898489538554, valid_loss: 0.021195793311510766\nSEED: 4, FOLD: 4, EPOCH: 21,train_loss: 0.007330688729501554, valid_loss: 0.021313741430640222\nSEED: 4, FOLD: 4, EPOCH: 22,train_loss: 0.006618960604180385, valid_loss: 0.021355949235813958\nSEED: 4, FOLD: 4, EPOCH: 23,train_loss: 0.006265888320295698, valid_loss: 0.021403300921831812\nSEED: 4, FOLD: 4, EPOCH: 24,train_loss: 0.0061567928947943405, valid_loss: 0.02144788716520582\nFOLD: 0, EPOCH: 0,train_loss: 0.7326458709827368, valid_loss: 0.6857628038951329\nSEED: 5, FOLD: 0, EPOCH: 0,train_loss: 0.4677647775767938, valid_loss: 0.024590991916401045\nSEED: 5, FOLD: 0, EPOCH: 1,train_loss: 0.021654802334049473, valid_loss: 0.020457225186484202\nSEED: 5, FOLD: 0, EPOCH: 2,train_loss: 0.02002049338720415, valid_loss: 0.019921786284872463\nSEED: 5, FOLD: 0, EPOCH: 3,train_loss: 0.018826007930750864, valid_loss: 0.018864152234579837\nSEED: 5, FOLD: 0, EPOCH: 4,train_loss: 0.018091784108538126, valid_loss: 0.018247292217399392\nSEED: 5, FOLD: 0, EPOCH: 5,train_loss: 0.017453559053440888, valid_loss: 0.018863217293151786\nSEED: 5, FOLD: 0, EPOCH: 6,train_loss: 0.017031876822474642, valid_loss: 0.018341791204043795\nSEED: 5, FOLD: 0, EPOCH: 7,train_loss: 0.016664992316045624, valid_loss: 0.01841987608266728\nSEED: 5, FOLD: 0, EPOCH: 8,train_loss: 0.016387115718553894, valid_loss: 0.018406220179583346\nSEED: 5, FOLD: 0, EPOCH: 9,train_loss: 0.016256482677831165, valid_loss: 0.01854188766862665\nSEED: 5, FOLD: 0, EPOCH: 10,train_loss: 0.01593317100238325, valid_loss: 0.01869270918624742\nSEED: 5, FOLD: 0, EPOCH: 11,train_loss: 0.015652306155180155, valid_loss: 0.10013087147048541\nSEED: 5, FOLD: 0, EPOCH: 12,train_loss: 0.015239595658267322, valid_loss: 0.019137379873011794\nSEED: 5, FOLD: 0, EPOCH: 13,train_loss: 0.014650772649632849, valid_loss: 0.14578302444091865\nSEED: 5, FOLD: 0, EPOCH: 14,train_loss: 0.014086478828466024, valid_loss: 0.019540710534368243\nSEED: 5, FOLD: 0, EPOCH: 15,train_loss: 0.013042814273765121, valid_loss: 0.020457671050514492\nSEED: 5, FOLD: 0, EPOCH: 16,train_loss: 0.012010929201716099, valid_loss: 0.020683740877679418\nSEED: 5, FOLD: 0, EPOCH: 17,train_loss: 0.010720326997123766, valid_loss: 0.02197173668869904\nSEED: 5, FOLD: 0, EPOCH: 18,train_loss: 0.009191169157165332, valid_loss: 0.0221400826637234\nSEED: 5, FOLD: 0, EPOCH: 19,train_loss: 0.0077720424092874146, valid_loss: 0.02213177207325186\nSEED: 5, FOLD: 0, EPOCH: 20,train_loss: 0.006719239052736025, valid_loss: 0.021926691489560262\nSEED: 5, FOLD: 0, EPOCH: 21,train_loss: 0.005931679378736062, valid_loss: 0.022251971172434944\nSEED: 5, FOLD: 0, EPOCH: 22,train_loss: 0.005504073274821259, valid_loss: 0.022083973086306028\nSEED: 5, FOLD: 0, EPOCH: 23,train_loss: 0.005325470220945452, valid_loss: 0.02221832424402237\nSEED: 5, FOLD: 0, EPOCH: 24,train_loss: 0.005239280672959875, valid_loss: 0.022225303575396538\nFOLD: 1, EPOCH: 0,train_loss: 0.7325594895077447, valid_loss: 0.6884279234068734\nSEED: 5, FOLD: 1, EPOCH: 0,train_loss: 0.4706919244932432, valid_loss: 0.023798341516937528\nSEED: 5, FOLD: 1, EPOCH: 1,train_loss: 0.02194155865505229, valid_loss: 0.020423680809991702\nSEED: 5, FOLD: 1, EPOCH: 2,train_loss: 0.020520010582395713, valid_loss: 0.01907386103911059\nSEED: 5, FOLD: 1, EPOCH: 3,train_loss: 0.018981050577585715, valid_loss: 0.01805611095790352\nSEED: 5, FOLD: 1, EPOCH: 4,train_loss: 0.018191339903558694, valid_loss: 0.017807253184063093\nSEED: 5, FOLD: 1, EPOCH: 5,train_loss: 0.017684874362754125, valid_loss: 0.036119206036840164\nSEED: 5, FOLD: 1, EPOCH: 6,train_loss: 0.017387097667440447, valid_loss: 0.017969172155218465\nSEED: 5, FOLD: 1, EPOCH: 7,train_loss: 0.01715092838191203, valid_loss: 0.0181664999840515\nSEED: 5, FOLD: 1, EPOCH: 8,train_loss: 0.017115246429767486, valid_loss: 0.019348415679165294\nSEED: 5, FOLD: 1, EPOCH: 9,train_loss: 0.01700736727076073, valid_loss: 0.01821524713720594\nSEED: 5, FOLD: 1, EPOCH: 10,train_loss: 0.016861671649844107, valid_loss: 0.0178972632757255\nSEED: 5, FOLD: 1, EPOCH: 11,train_loss: 0.0168343218193002, valid_loss: 0.017672063569937432\nSEED: 5, FOLD: 1, EPOCH: 12,train_loss: 0.01645075508740044, valid_loss: 0.0315214563693319\nSEED: 5, FOLD: 1, EPOCH: 13,train_loss: 0.016257432031098508, valid_loss: 0.018336903303861618\nSEED: 5, FOLD: 1, EPOCH: 14,train_loss: 0.015835154590869906, valid_loss: 0.01793857842151608\nSEED: 5, FOLD: 1, EPOCH: 15,train_loss: 0.015339025146715398, valid_loss: 0.018407658327903066\nSEED: 5, FOLD: 1, EPOCH: 16,train_loss: 0.014574175020747811, valid_loss: 0.01900536412639277\nSEED: 5, FOLD: 1, EPOCH: 17,train_loss: 0.013607088184106523, valid_loss: 0.019408108560102326\nSEED: 5, FOLD: 1, EPOCH: 18,train_loss: 0.01218458993129269, valid_loss: 0.019705966008560998\nSEED: 5, FOLD: 1, EPOCH: 19,train_loss: 0.010421928997239926, valid_loss: 0.019915179429309707\nSEED: 5, FOLD: 1, EPOCH: 20,train_loss: 0.008703214527672008, valid_loss: 0.020219731224434715\nSEED: 5, FOLD: 1, EPOCH: 21,train_loss: 0.007379566927705586, valid_loss: 0.020426338698182788\nSEED: 5, FOLD: 1, EPOCH: 22,train_loss: 0.006558788073568666, valid_loss: 0.02051856123975345\nSEED: 5, FOLD: 1, EPOCH: 23,train_loss: 0.006147283615449267, valid_loss: 0.020605955964752606\nSEED: 5, FOLD: 1, EPOCH: 24,train_loss: 0.006011193958077117, valid_loss: 0.02060947333063398\nFOLD: 2, EPOCH: 0,train_loss: 0.7325884267903756, valid_loss: 0.6881399886948721\nSEED: 5, FOLD: 2, EPOCH: 0,train_loss: 0.46770075445427844, valid_loss: 0.02288665489426681\nSEED: 5, FOLD: 2, EPOCH: 1,train_loss: 0.021639601759396603, valid_loss: 0.0200103972107172\nSEED: 5, FOLD: 2, EPOCH: 2,train_loss: 0.020235375757666603, valid_loss: 0.8401420498000723\nSEED: 5, FOLD: 2, EPOCH: 3,train_loss: 0.018949177116155624, valid_loss: 0.017917420395783017\nSEED: 5, FOLD: 2, EPOCH: 4,train_loss: 0.01828005468553823, valid_loss: 0.02966240474155971\nSEED: 5, FOLD: 2, EPOCH: 5,train_loss: 0.01773742046477138, valid_loss: 0.017869976003255163\nSEED: 5, FOLD: 2, EPOCH: 6,train_loss: 0.017344352087356907, valid_loss: 0.017901926806994848\nSEED: 5, FOLD: 2, EPOCH: 7,train_loss: 0.017070412014921505, valid_loss: 0.01779116168618202\nSEED: 5, FOLD: 2, EPOCH: 8,train_loss: 0.016788584639088833, valid_loss: 0.01777188421360084\nSEED: 5, FOLD: 2, EPOCH: 9,train_loss: 0.01652262177642273, valid_loss: 0.017952266175832066\nSEED: 5, FOLD: 2, EPOCH: 10,train_loss: 0.016294536234783955, valid_loss: 0.018090803708348955\nSEED: 5, FOLD: 2, EPOCH: 11,train_loss: 0.015943858374342108, valid_loss: 0.01811191500829799\nSEED: 5, FOLD: 2, EPOCH: 12,train_loss: 0.015473436888145365, valid_loss: 0.018461205384560992\nSEED: 5, FOLD: 2, EPOCH: 13,train_loss: 0.015039352760852678, valid_loss: 0.018868259127650944\nSEED: 5, FOLD: 2, EPOCH: 14,train_loss: 0.014301015711996866, valid_loss: 0.019346305834395543\nSEED: 5, FOLD: 2, EPOCH: 15,train_loss: 0.01347648247104624, valid_loss: 0.01951384193130902\nSEED: 5, FOLD: 2, EPOCH: 16,train_loss: 0.012317293114366306, valid_loss: 0.019770467707089016\nSEED: 5, FOLD: 2, EPOCH: 17,train_loss: 0.010991181564125894, valid_loss: 0.020341992537890163\nSEED: 5, FOLD: 2, EPOCH: 18,train_loss: 0.00958538072966579, valid_loss: 0.020422825110810144\nSEED: 5, FOLD: 2, EPOCH: 19,train_loss: 0.008228225123974076, valid_loss: 0.020864079360451017\nSEED: 5, FOLD: 2, EPOCH: 20,train_loss: 0.007103106961923017, valid_loss: 0.020890373630183083\nSEED: 5, FOLD: 2, EPOCH: 21,train_loss: 0.0062760127931023426, valid_loss: 0.020849069420780453\nSEED: 5, FOLD: 2, EPOCH: 22,train_loss: 0.005789716387221562, valid_loss: 0.020841805104698453\nSEED: 5, FOLD: 2, EPOCH: 23,train_loss: 0.005584152361405068, valid_loss: 0.020922998019627163\nSEED: 5, FOLD: 2, EPOCH: 24,train_loss: 0.005480747541709655, valid_loss: 0.020903938902275904\nFOLD: 3, EPOCH: 0,train_loss: 0.7321574879388739, valid_loss: 0.689297992842538\nSEED: 5, FOLD: 3, EPOCH: 0,train_loss: 0.4704050928979677, valid_loss: 0.02429312366460051\nSEED: 5, FOLD: 3, EPOCH: 1,train_loss: 0.021566130139314344, valid_loss: 0.020699339147124973\nSEED: 5, FOLD: 3, EPOCH: 2,train_loss: 0.019817606056530546, valid_loss: 0.018989613971539906\nSEED: 5, FOLD: 3, EPOCH: 3,train_loss: 0.018549721184982, valid_loss: 0.018800395354628564\nSEED: 5, FOLD: 3, EPOCH: 4,train_loss: 0.017768489611573027, valid_loss: 0.018265184494001524\nSEED: 5, FOLD: 3, EPOCH: 5,train_loss: 0.017285573799299063, valid_loss: 0.01871312702340739\nSEED: 5, FOLD: 3, EPOCH: 6,train_loss: 0.01684729671989479, valid_loss: 0.018422783964446614\nSEED: 5, FOLD: 3, EPOCH: 7,train_loss: 0.016537802089957424, valid_loss: 0.018267871466066156\nSEED: 5, FOLD: 3, EPOCH: 8,train_loss: 0.016368319185273927, valid_loss: 0.01860925094889743\nSEED: 5, FOLD: 3, EPOCH: 9,train_loss: 0.016112076050608697, valid_loss: 0.01958734369171517\nSEED: 5, FOLD: 3, EPOCH: 10,train_loss: 0.01586991592045248, valid_loss: 0.01904608719050884\nSEED: 5, FOLD: 3, EPOCH: 11,train_loss: 0.015594088336466437, valid_loss: 0.019031950618539538\nSEED: 5, FOLD: 3, EPOCH: 12,train_loss: 0.015103798142097292, valid_loss: 0.01924272286040442\nSEED: 5, FOLD: 3, EPOCH: 13,train_loss: 0.014566386376854278, valid_loss: 0.019241166646991457\nSEED: 5, FOLD: 3, EPOCH: 14,train_loss: 0.01382021516915003, valid_loss: 0.019648057115929467\nSEED: 5, FOLD: 3, EPOCH: 15,train_loss: 0.012963447461489343, valid_loss: 0.020021160745194982\nSEED: 5, FOLD: 3, EPOCH: 16,train_loss: 0.01161035054438088, valid_loss: 0.021051283766116414\nSEED: 5, FOLD: 3, EPOCH: 17,train_loss: 0.010276899081620857, valid_loss: 0.02124640526516097\nSEED: 5, FOLD: 3, EPOCH: 18,train_loss: 0.008858850269993074, valid_loss: 0.02155360435800893\nSEED: 5, FOLD: 3, EPOCH: 19,train_loss: 0.007516690291953783, valid_loss: 0.02144278394324439\nSEED: 5, FOLD: 3, EPOCH: 20,train_loss: 0.006495702301362788, valid_loss: 0.021697419456073216\nSEED: 5, FOLD: 3, EPOCH: 21,train_loss: 0.005831622977462345, valid_loss: 0.021556378794567926\nSEED: 5, FOLD: 3, EPOCH: 22,train_loss: 0.0054451438843061886, valid_loss: 0.021593903803399633\nSEED: 5, FOLD: 3, EPOCH: 23,train_loss: 0.005271271391207501, valid_loss: 0.021579084119626453\nSEED: 5, FOLD: 3, EPOCH: 24,train_loss: 0.005191723608758545, valid_loss: 0.021545968683702604\nFOLD: 4, EPOCH: 0,train_loss: 0.7325960108335468, valid_loss: 0.6875758434043211\nSEED: 5, FOLD: 4, EPOCH: 0,train_loss: 0.46925672378552996, valid_loss: 0.023882145445574734\nSEED: 5, FOLD: 4, EPOCH: 1,train_loss: 0.021535981014586876, valid_loss: 0.02104585902655826\nSEED: 5, FOLD: 4, EPOCH: 2,train_loss: 0.019964473119572453, valid_loss: 0.019155839896377397\nSEED: 5, FOLD: 4, EPOCH: 3,train_loss: 0.018656156390257504, valid_loss: 0.01842827651211444\nSEED: 5, FOLD: 4, EPOCH: 4,train_loss: 0.017873383039419634, valid_loss: 0.01855372396462104\nSEED: 5, FOLD: 4, EPOCH: 5,train_loss: 0.01736287014099999, valid_loss: 0.8754287800368141\nSEED: 5, FOLD: 4, EPOCH: 6,train_loss: 0.016950306914530804, valid_loss: 0.018839323674054706\nSEED: 5, FOLD: 4, EPOCH: 7,train_loss: 0.016646277814077726, valid_loss: 0.018687335445600396\nSEED: 5, FOLD: 4, EPOCH: 8,train_loss: 0.01639834402695946, valid_loss: 0.01861680348348968\nSEED: 5, FOLD: 4, EPOCH: 9,train_loss: 0.016125583481313526, valid_loss: 0.018972608699079824\nSEED: 5, FOLD: 4, EPOCH: 10,train_loss: 0.015829613128596026, valid_loss: 0.019269456265165526\nSEED: 5, FOLD: 4, EPOCH: 11,train_loss: 0.015617626550458912, valid_loss: 0.018968808902975393\nSEED: 5, FOLD: 4, EPOCH: 12,train_loss: 0.015182625596829947, valid_loss: 0.01920278008808108\nSEED: 5, FOLD: 4, EPOCH: 13,train_loss: 0.014657043055563734, valid_loss: 0.01969540787532049\nSEED: 5, FOLD: 4, EPOCH: 14,train_loss: 0.013839760505040918, valid_loss: 0.01990778333343127\nSEED: 5, FOLD: 4, EPOCH: 15,train_loss: 0.01307814471774559, valid_loss: 0.020546928595970657\nSEED: 5, FOLD: 4, EPOCH: 16,train_loss: 0.0118953264048458, valid_loss: 0.021080303005874157\nSEED: 5, FOLD: 4, EPOCH: 17,train_loss: 0.01049894999469752, valid_loss: 0.021340382285416126\nSEED: 5, FOLD: 4, EPOCH: 18,train_loss: 0.009063998088780521, valid_loss: 0.02158872810575892\nSEED: 5, FOLD: 4, EPOCH: 19,train_loss: 0.007740278616952507, valid_loss: 0.021924940650077426\nSEED: 5, FOLD: 4, EPOCH: 20,train_loss: 0.00663218946547072, valid_loss: 0.022007510339950815\nSEED: 5, FOLD: 4, EPOCH: 21,train_loss: 0.005914261777196889, valid_loss: 0.021867723423330224\nSEED: 5, FOLD: 4, EPOCH: 22,train_loss: 0.005527546733914726, valid_loss: 0.021918879493194467\nSEED: 5, FOLD: 4, EPOCH: 23,train_loss: 0.005341980990994236, valid_loss: 0.021919345976236987\nSEED: 5, FOLD: 4, EPOCH: 24,train_loss: 0.00526451062667521, valid_loss: 0.02189555364277433\nFOLD: 0, EPOCH: 0,train_loss: 0.7315557333674744, valid_loss: 0.7042827929769243\nSEED: 6, FOLD: 0, EPOCH: 0,train_loss: 0.4710091977876468, valid_loss: 0.024834492589746203\nSEED: 6, FOLD: 0, EPOCH: 1,train_loss: 0.021876868825868097, valid_loss: 0.021125390008091927\nSEED: 6, FOLD: 0, EPOCH: 2,train_loss: 0.020332247535460188, valid_loss: 0.019464553679738726\nSEED: 6, FOLD: 0, EPOCH: 3,train_loss: 0.018990176709463995, valid_loss: 0.01874915600887367\nSEED: 6, FOLD: 0, EPOCH: 4,train_loss: 0.01830859753527563, valid_loss: 0.018494266218372753\nSEED: 6, FOLD: 0, EPOCH: 5,train_loss: 0.017940244379106664, valid_loss: 0.039975912044090886\nSEED: 6, FOLD: 0, EPOCH: 6,train_loss: 0.01742331014714972, valid_loss: 0.0180257931883846\nSEED: 6, FOLD: 0, EPOCH: 7,train_loss: 0.01723056407577365, valid_loss: 0.018071230367890427\nSEED: 6, FOLD: 0, EPOCH: 8,train_loss: 0.017089258403564893, valid_loss: 0.018762938997575213\nSEED: 6, FOLD: 0, EPOCH: 9,train_loss: 0.016898950840598042, valid_loss: 0.018083334554518972\nSEED: 6, FOLD: 0, EPOCH: 10,train_loss: 0.016768043908378938, valid_loss: 0.017986009057079044\nSEED: 6, FOLD: 0, EPOCH: 11,train_loss: 0.016579950343899048, valid_loss: 0.01828139972473894\nSEED: 6, FOLD: 0, EPOCH: 12,train_loss: 0.01640565653270396, valid_loss: 0.018370881729892322\nSEED: 6, FOLD: 0, EPOCH: 13,train_loss: 0.016086464571039173, valid_loss: 0.01884527099984033\nSEED: 6, FOLD: 0, EPOCH: 14,train_loss: 0.015552572124250178, valid_loss: 0.02202163413167\nSEED: 6, FOLD: 0, EPOCH: 15,train_loss: 0.015040758432969994, valid_loss: 0.018988331300871714\nSEED: 6, FOLD: 0, EPOCH: 16,train_loss: 0.014232594281924468, valid_loss: 0.01976718668426786\nSEED: 6, FOLD: 0, EPOCH: 17,train_loss: 0.013105151059962537, valid_loss: 0.020376804417797496\nSEED: 6, FOLD: 0, EPOCH: 18,train_loss: 0.011640813833877554, valid_loss: 0.021158350738031524\nSEED: 6, FOLD: 0, EPOCH: 19,train_loss: 0.010033179462010409, valid_loss: 0.0215080981275865\nSEED: 6, FOLD: 0, EPOCH: 20,train_loss: 0.00851816428063886, valid_loss: 0.021704411400215968\nSEED: 6, FOLD: 0, EPOCH: 21,train_loss: 0.00731153429628615, valid_loss: 0.021815449904118267\nSEED: 6, FOLD: 0, EPOCH: 22,train_loss: 0.006588484728233005, valid_loss: 0.021767941383378845\nSEED: 6, FOLD: 0, EPOCH: 23,train_loss: 0.006176891169055318, valid_loss: 0.02182846399290221\nSEED: 6, FOLD: 0, EPOCH: 24,train_loss: 0.006062603079761467, valid_loss: 0.02188256952379431\nFOLD: 1, EPOCH: 0,train_loss: 0.7320311764876047, valid_loss: 0.7081807340894427\nSEED: 6, FOLD: 1, EPOCH: 0,train_loss: 0.4694387978212773, valid_loss: 0.024221210022057807\nSEED: 6, FOLD: 1, EPOCH: 1,train_loss: 0.021801672713912052, valid_loss: 0.023926659034831183\nSEED: 6, FOLD: 1, EPOCH: 2,train_loss: 0.020487619030788756, valid_loss: 0.02127781409238066\nSEED: 6, FOLD: 1, EPOCH: 3,train_loss: 0.019244752675834774, valid_loss: 0.018580781561987742\nSEED: 6, FOLD: 1, EPOCH: 4,train_loss: 0.01849229303121135, valid_loss: 1.0241195250834738\nSEED: 6, FOLD: 1, EPOCH: 5,train_loss: 0.018105946487976587, valid_loss: 0.23571164325944016\nSEED: 6, FOLD: 1, EPOCH: 6,train_loss: 0.017579845793923174, valid_loss: 12.162906138173172\nSEED: 6, FOLD: 1, EPOCH: 7,train_loss: 0.017237359347442787, valid_loss: 0.018403582594224383\nSEED: 6, FOLD: 1, EPOCH: 8,train_loss: 0.017126242974368128, valid_loss: 0.874943863547274\nSEED: 6, FOLD: 1, EPOCH: 9,train_loss: 0.017058673915385767, valid_loss: 0.018740183700408253\nSEED: 6, FOLD: 1, EPOCH: 10,train_loss: 0.016851143729265616, valid_loss: 0.15471131993191584\nSEED: 6, FOLD: 1, EPOCH: 11,train_loss: 0.016625172755532505, valid_loss: 0.02365420726793153\nSEED: 6, FOLD: 1, EPOCH: 12,train_loss: 0.01669483978058333, valid_loss: 0.01861824542284012\nSEED: 6, FOLD: 1, EPOCH: 13,train_loss: 0.01655303570104466, valid_loss: 0.018666908411043032\nSEED: 6, FOLD: 1, EPOCH: 14,train_loss: 0.015720804464881836, valid_loss: 0.01923951953649521\nSEED: 6, FOLD: 1, EPOCH: 15,train_loss: 0.015433406275089668, valid_loss: 0.01973278932273388\nSEED: 6, FOLD: 1, EPOCH: 16,train_loss: 0.01487509203750802, valid_loss: 0.019320929263319287\nSEED: 6, FOLD: 1, EPOCH: 17,train_loss: 0.014114581817842049, valid_loss: 0.019681077397295407\nSEED: 6, FOLD: 1, EPOCH: 18,train_loss: 0.012609811357991852, valid_loss: 0.02060822466654437\nSEED: 6, FOLD: 1, EPOCH: 19,train_loss: 0.012267715824039085, valid_loss: 0.0202689102185624\nSEED: 6, FOLD: 1, EPOCH: 20,train_loss: 0.010132679385065601, valid_loss: 0.02072616038577897\nSEED: 6, FOLD: 1, EPOCH: 21,train_loss: 0.008967424051134267, valid_loss: 0.02097489966877869\nSEED: 6, FOLD: 1, EPOCH: 22,train_loss: 0.008024016334472792, valid_loss: 0.021069271489977837\nSEED: 6, FOLD: 1, EPOCH: 23,train_loss: 0.00740342072305688, valid_loss: 0.021150560943143708\nSEED: 6, FOLD: 1, EPOCH: 24,train_loss: 0.007236801745855938, valid_loss: 0.021195033458726746\nFOLD: 2, EPOCH: 0,train_loss: 0.7317757610857052, valid_loss: 0.7065558671951294\nSEED: 6, FOLD: 2, EPOCH: 0,train_loss: 0.4716815996072153, valid_loss: 0.02412707395851612\nSEED: 6, FOLD: 2, EPOCH: 1,train_loss: 0.021890706593429086, valid_loss: 0.020840312806623323\nSEED: 6, FOLD: 2, EPOCH: 2,train_loss: 0.020637547610885036, valid_loss: 0.35560268525566374\nSEED: 6, FOLD: 2, EPOCH: 3,train_loss: 0.019349981235326642, valid_loss: 0.019009579398802347\nSEED: 6, FOLD: 2, EPOCH: 4,train_loss: 0.01845414285976304, valid_loss: 0.018108950315841605\nSEED: 6, FOLD: 2, EPOCH: 5,train_loss: 0.01786297414261494, valid_loss: 0.018196545142148222\nSEED: 6, FOLD: 2, EPOCH: 6,train_loss: 0.017429383084123586, valid_loss: 0.017903451275612627\nSEED: 6, FOLD: 2, EPOCH: 7,train_loss: 0.017140623209250236, valid_loss: 0.018152950038867337\nSEED: 6, FOLD: 2, EPOCH: 8,train_loss: 0.01685092804178487, valid_loss: 0.018026403045015675\nSEED: 6, FOLD: 2, EPOCH: 9,train_loss: 0.016691527064264255, valid_loss: 0.018068059374179157\nSEED: 6, FOLD: 2, EPOCH: 10,train_loss: 0.01643146219642928, valid_loss: 0.0182949102882828\nSEED: 6, FOLD: 2, EPOCH: 11,train_loss: 0.016194825563715758, valid_loss: 0.018392679361360412\nSEED: 6, FOLD: 2, EPOCH: 12,train_loss: 0.015788574573876214, valid_loss: 0.018457747623324396\nSEED: 6, FOLD: 2, EPOCH: 13,train_loss: 0.015431221676514532, valid_loss: 0.018745766686541692\nSEED: 6, FOLD: 2, EPOCH: 14,train_loss: 0.014795306288249736, valid_loss: 0.019057512496198928\nSEED: 6, FOLD: 2, EPOCH: 15,train_loss: 0.014032766379307221, valid_loss: 0.019446638173290662\nSEED: 6, FOLD: 2, EPOCH: 16,train_loss: 0.013019254199997351, valid_loss: 0.10234798549541406\nSEED: 6, FOLD: 2, EPOCH: 17,train_loss: 0.01162921123369767, valid_loss: 0.020561579960797514\nSEED: 6, FOLD: 2, EPOCH: 18,train_loss: 0.010255248156668496, valid_loss: 0.02107894378049033\nSEED: 6, FOLD: 2, EPOCH: 19,train_loss: 0.008723254913776896, valid_loss: 0.021367146873048373\nSEED: 6, FOLD: 2, EPOCH: 20,train_loss: 0.007394014820320545, valid_loss: 0.021440757092620645\nSEED: 6, FOLD: 2, EPOCH: 21,train_loss: 0.006495076068507059, valid_loss: 0.0214698479909982\nSEED: 6, FOLD: 2, EPOCH: 22,train_loss: 0.005982328182507823, valid_loss: 0.02154012649719204\nSEED: 6, FOLD: 2, EPOCH: 23,train_loss: 0.005740598126239803, valid_loss: 0.02161011876804488\nSEED: 6, FOLD: 2, EPOCH: 24,train_loss: 0.005637631486475903, valid_loss: 0.021639447659254074\nFOLD: 3, EPOCH: 0,train_loss: 0.7317939884420754, valid_loss: 0.7051552755492074\nSEED: 6, FOLD: 3, EPOCH: 0,train_loss: 0.4683080800147592, valid_loss: 0.023894146510532923\nSEED: 6, FOLD: 3, EPOCH: 1,train_loss: 0.02175093573126672, valid_loss: 0.02122299186885357\nSEED: 6, FOLD: 3, EPOCH: 2,train_loss: 0.020688317729619102, valid_loss: 0.0202971734638725\nSEED: 6, FOLD: 3, EPOCH: 3,train_loss: 0.019423609343019947, valid_loss: 0.019103874133101533\nSEED: 6, FOLD: 3, EPOCH: 4,train_loss: 0.01844771931865725, valid_loss: 0.018527974667293685\nSEED: 6, FOLD: 3, EPOCH: 5,train_loss: 0.018026025344928105, valid_loss: 0.018432959036103317\nSEED: 6, FOLD: 3, EPOCH: 6,train_loss: 0.01763022465604371, valid_loss: 0.018133588560989924\nSEED: 6, FOLD: 3, EPOCH: 7,train_loss: 0.01739094824111764, valid_loss: 0.018381671740540437\nSEED: 6, FOLD: 3, EPOCH: 8,train_loss: 0.0170512189278784, valid_loss: 0.01875117870845965\nSEED: 6, FOLD: 3, EPOCH: 9,train_loss: 0.016974980888915234, valid_loss: 0.017999247993741717\nSEED: 6, FOLD: 3, EPOCH: 10,train_loss: 0.01679032322937164, valid_loss: 0.01807797034936292\nSEED: 6, FOLD: 3, EPOCH: 11,train_loss: 0.01657845510466807, valid_loss: 0.020828742454094547\nSEED: 6, FOLD: 3, EPOCH: 12,train_loss: 0.016273600245029596, valid_loss: 0.018529968416052204\nSEED: 6, FOLD: 3, EPOCH: 13,train_loss: 0.015866277320985344, valid_loss: 0.018536707998386453\nSEED: 6, FOLD: 3, EPOCH: 14,train_loss: 0.015407370632865292, valid_loss: 0.018491137214004992\nSEED: 6, FOLD: 3, EPOCH: 15,train_loss: 0.014785992136845985, valid_loss: 0.018824621263359275\nSEED: 6, FOLD: 3, EPOCH: 16,train_loss: 0.013873206930693941, valid_loss: 0.019277567602694036\nSEED: 6, FOLD: 3, EPOCH: 17,train_loss: 0.012700134058199499, valid_loss: 0.01949738793607269\nSEED: 6, FOLD: 3, EPOCH: 18,train_loss: 0.011224153120934532, valid_loss: 0.01986491046845913\nSEED: 6, FOLD: 3, EPOCH: 19,train_loss: 0.00962137179179252, valid_loss: 0.02061265329165118\nSEED: 6, FOLD: 3, EPOCH: 20,train_loss: 0.008147844045922377, valid_loss: 0.020893670591924873\nSEED: 6, FOLD: 3, EPOCH: 21,train_loss: 0.007117373389664335, valid_loss: 0.02097864600696734\nSEED: 6, FOLD: 3, EPOCH: 22,train_loss: 0.006432695631477712, valid_loss: 0.021179111514772687\nSEED: 6, FOLD: 3, EPOCH: 23,train_loss: 0.0060906674361963205, valid_loss: 0.021288999782076903\nSEED: 6, FOLD: 3, EPOCH: 24,train_loss: 0.005962854585326884, valid_loss: 0.021289083042315075\nFOLD: 4, EPOCH: 0,train_loss: 0.7319533570089202, valid_loss: 0.7072260747937595\nSEED: 6, FOLD: 4, EPOCH: 0,train_loss: 0.4693480843705112, valid_loss: 0.02431828123243416\nSEED: 6, FOLD: 4, EPOCH: 1,train_loss: 0.021602715379086094, valid_loss: 0.020333225126652157\nSEED: 6, FOLD: 4, EPOCH: 2,train_loss: 0.020032855209665024, valid_loss: 0.018665130633641693\nSEED: 6, FOLD: 4, EPOCH: 3,train_loss: 0.018787189845697605, valid_loss: 0.01822611182818518\nSEED: 6, FOLD: 4, EPOCH: 4,train_loss: 0.018046164662455736, valid_loss: 0.017935442765626836\nSEED: 6, FOLD: 4, EPOCH: 5,train_loss: 0.017571004134589348, valid_loss: 0.06520132693078588\nSEED: 6, FOLD: 4, EPOCH: 6,train_loss: 0.01721115831447684, valid_loss: 0.0178772789590499\nSEED: 6, FOLD: 4, EPOCH: 7,train_loss: 0.01687620964873096, valid_loss: 0.04427093692014322\nSEED: 6, FOLD: 4, EPOCH: 8,train_loss: 0.016738018554135942, valid_loss: 0.017938490190050182\nSEED: 6, FOLD: 4, EPOCH: 9,train_loss: 0.016605857361539984, valid_loss: 0.01798585443483556\nSEED: 6, FOLD: 4, EPOCH: 10,train_loss: 0.01644258832127072, valid_loss: 0.13846968774519422\nSEED: 6, FOLD: 4, EPOCH: 11,train_loss: 0.016227800129116444, valid_loss: 0.06524656461003948\nSEED: 6, FOLD: 4, EPOCH: 12,train_loss: 0.015939186846810408, valid_loss: 0.01849186494398643\nSEED: 6, FOLD: 4, EPOCH: 13,train_loss: 0.01552374841834324, valid_loss: 0.018357151352307376\nSEED: 6, FOLD: 4, EPOCH: 14,train_loss: 0.014915956737662571, valid_loss: 0.018808028414188063\nSEED: 6, FOLD: 4, EPOCH: 15,train_loss: 0.014176162465920916, valid_loss: 0.018892564783420634\nSEED: 6, FOLD: 4, EPOCH: 16,train_loss: 0.01319336143416771, valid_loss: 0.0194040513049592\nSEED: 6, FOLD: 4, EPOCH: 17,train_loss: 0.011912278358595095, valid_loss: 0.026426634312990832\nSEED: 6, FOLD: 4, EPOCH: 18,train_loss: 0.010411129508545433, valid_loss: 0.020435787737369537\nSEED: 6, FOLD: 4, EPOCH: 19,train_loss: 0.008820853320935714, valid_loss: 0.02067380557384561\nSEED: 6, FOLD: 4, EPOCH: 20,train_loss: 0.007398846842672514, valid_loss: 0.020681155297686073\nSEED: 6, FOLD: 4, EPOCH: 21,train_loss: 0.006440681186032252, valid_loss: 0.02219378241502187\nSEED: 6, FOLD: 4, EPOCH: 22,train_loss: 0.005888562841826807, valid_loss: 0.020928922766710028\nSEED: 6, FOLD: 4, EPOCH: 23,train_loss: 0.005611925146313033, valid_loss: 0.021178069688818035\nSEED: 6, FOLD: 4, EPOCH: 24,train_loss: 0.005527574798443179, valid_loss: 0.021090771783800685\nFOLD: 0, EPOCH: 0,train_loss: 0.7337006758088651, valid_loss: 0.7011374286242894\nSEED: 7, FOLD: 0, EPOCH: 0,train_loss: 0.4707180483239716, valid_loss: 0.02339033005493028\nSEED: 7, FOLD: 0, EPOCH: 1,train_loss: 0.021635148147849934, valid_loss: 0.02129003740847111\nSEED: 7, FOLD: 0, EPOCH: 2,train_loss: 0.020193981600628384, valid_loss: 0.018894486874341965\nSEED: 7, FOLD: 0, EPOCH: 3,train_loss: 0.018921010277193527, valid_loss: 0.01831011955759355\nSEED: 7, FOLD: 0, EPOCH: 4,train_loss: 0.018102715109083532, valid_loss: 0.018093429638871124\nSEED: 7, FOLD: 0, EPOCH: 5,train_loss: 0.017658051467784073, valid_loss: 0.017807011492550373\nSEED: 7, FOLD: 0, EPOCH: 6,train_loss: 0.01728996561597223, valid_loss: 0.017649641446769237\nSEED: 7, FOLD: 0, EPOCH: 7,train_loss: 0.01705009242573726, valid_loss: 0.017538123258522578\nSEED: 7, FOLD: 0, EPOCH: 8,train_loss: 0.016734428238123655, valid_loss: 0.026059992504971367\nSEED: 7, FOLD: 0, EPOCH: 9,train_loss: 0.01661613446565858, valid_loss: 0.018222552644354958\nSEED: 7, FOLD: 0, EPOCH: 10,train_loss: 0.016377942380157932, valid_loss: 0.018121333846024105\nSEED: 7, FOLD: 0, EPOCH: 11,train_loss: 0.016146259983002707, valid_loss: 0.01789982747286558\nSEED: 7, FOLD: 0, EPOCH: 12,train_loss: 0.015834219247588644, valid_loss: 0.017795483129365103\nSEED: 7, FOLD: 0, EPOCH: 13,train_loss: 0.015445753095158632, valid_loss: 0.018411369009741715\nSEED: 7, FOLD: 0, EPOCH: 14,train_loss: 0.015042471557693638, valid_loss: 0.018916316836008005\nSEED: 7, FOLD: 0, EPOCH: 15,train_loss: 0.014106530534184498, valid_loss: 0.01888707514320101\nSEED: 7, FOLD: 0, EPOCH: 16,train_loss: 0.013106509692211082, valid_loss: 0.01974968822406871\nSEED: 7, FOLD: 0, EPOCH: 17,train_loss: 0.011917857122540043, valid_loss: 0.028746110812893935\nSEED: 7, FOLD: 0, EPOCH: 18,train_loss: 0.010441139434882696, valid_loss: 0.02030593454837799\nSEED: 7, FOLD: 0, EPOCH: 19,train_loss: 0.008964614507377795, valid_loss: 0.020727874365236078\nSEED: 7, FOLD: 0, EPOCH: 20,train_loss: 0.007620255294107441, valid_loss: 0.020856140820043428\nSEED: 7, FOLD: 0, EPOCH: 21,train_loss: 0.006679115142079367, valid_loss: 0.02093849876629455\nSEED: 7, FOLD: 0, EPOCH: 22,train_loss: 0.0060742590306461725, valid_loss: 0.020971546029405933\nSEED: 7, FOLD: 0, EPOCH: 23,train_loss: 0.005791972624137998, valid_loss: 0.020989740613315787\nSEED: 7, FOLD: 0, EPOCH: 24,train_loss: 0.005678386795267031, valid_loss: 0.020999683067202568\nFOLD: 1, EPOCH: 0,train_loss: 0.7341036140054896, valid_loss: 0.7045296651976449\nSEED: 7, FOLD: 1, EPOCH: 0,train_loss: 0.46897763945162296, valid_loss: 0.02301838845014572\nSEED: 7, FOLD: 1, EPOCH: 1,train_loss: 0.0215387890736262, valid_loss: 0.021160980154361044\nSEED: 7, FOLD: 1, EPOCH: 2,train_loss: 0.020197086576102436, valid_loss: 0.01941914074122906\nSEED: 7, FOLD: 1, EPOCH: 3,train_loss: 0.018870418688849262, valid_loss: 0.42858312811170307\nSEED: 7, FOLD: 1, EPOCH: 4,train_loss: 0.018157815628185654, valid_loss: 0.018178373495382923\nSEED: 7, FOLD: 1, EPOCH: 5,train_loss: 0.017658779890262995, valid_loss: 0.06837576340351786\nSEED: 7, FOLD: 1, EPOCH: 6,train_loss: 0.017302590327850288, valid_loss: 0.27908250875771046\nSEED: 7, FOLD: 1, EPOCH: 7,train_loss: 0.016996953892858997, valid_loss: 1.7236911978040423\nSEED: 7, FOLD: 1, EPOCH: 8,train_loss: 0.016757958231196888, valid_loss: 0.01835270438875471\nSEED: 7, FOLD: 1, EPOCH: 9,train_loss: 0.016551946735252506, valid_loss: 0.019230656804783003\nSEED: 7, FOLD: 1, EPOCH: 10,train_loss: 0.016290113019446533, valid_loss: 0.03939223209662097\nSEED: 7, FOLD: 1, EPOCH: 11,train_loss: 0.0160015054937938, valid_loss: 0.01882015402827944\nSEED: 7, FOLD: 1, EPOCH: 12,train_loss: 0.015623265135007492, valid_loss: 0.018875702044793538\nSEED: 7, FOLD: 1, EPOCH: 13,train_loss: 0.01506245637015588, valid_loss: 0.019753743069512504\nSEED: 7, FOLD: 1, EPOCH: 14,train_loss: 0.014303497916114504, valid_loss: 0.0562920339937721\nSEED: 7, FOLD: 1, EPOCH: 15,train_loss: 0.013417296077840138, valid_loss: 0.020189644076994487\nSEED: 7, FOLD: 1, EPOCH: 16,train_loss: 0.01226492600677454, valid_loss: 0.02058866157063416\nSEED: 7, FOLD: 1, EPOCH: 17,train_loss: 0.01088362905210343, valid_loss: 0.021256957788552557\nSEED: 7, FOLD: 1, EPOCH: 18,train_loss: 0.009531995342315538, valid_loss: 0.02153916188648769\nSEED: 7, FOLD: 1, EPOCH: 19,train_loss: 0.008151563481711175, valid_loss: 0.021948126756719182\nSEED: 7, FOLD: 1, EPOCH: 20,train_loss: 0.007019044622185005, valid_loss: 0.022176983367119516\nSEED: 7, FOLD: 1, EPOCH: 21,train_loss: 0.006266643380935209, valid_loss: 0.021890898421406745\nSEED: 7, FOLD: 1, EPOCH: 22,train_loss: 0.0058036502889371, valid_loss: 0.021977321484259196\nSEED: 7, FOLD: 1, EPOCH: 23,train_loss: 0.005559918005019426, valid_loss: 0.021922722086310388\nSEED: 7, FOLD: 1, EPOCH: 24,train_loss: 0.005476102607486689, valid_loss: 0.022000546806624957\nFOLD: 2, EPOCH: 0,train_loss: 0.7337914955788765, valid_loss: 0.7040419902120317\nSEED: 7, FOLD: 2, EPOCH: 0,train_loss: 0.4689985969360324, valid_loss: 0.023560078495315145\nSEED: 7, FOLD: 2, EPOCH: 1,train_loss: 0.021574514364634735, valid_loss: 0.02034048315669809\nSEED: 7, FOLD: 2, EPOCH: 2,train_loss: 0.02039256752671107, valid_loss: 0.032037453353405\nSEED: 7, FOLD: 2, EPOCH: 3,train_loss: 0.019084073886599228, valid_loss: 0.01831898566867624\nSEED: 7, FOLD: 2, EPOCH: 4,train_loss: 0.018512273902424436, valid_loss: 0.0178861139608281\nSEED: 7, FOLD: 2, EPOCH: 5,train_loss: 0.01798800980348302, valid_loss: 0.017897600600762027\nSEED: 7, FOLD: 2, EPOCH: 6,train_loss: 0.017594963278405477, valid_loss: 0.017887648940086365\nSEED: 7, FOLD: 2, EPOCH: 7,train_loss: 0.017348021835736607, valid_loss: 0.01802259978971311\nSEED: 7, FOLD: 2, EPOCH: 8,train_loss: 0.017122537275587303, valid_loss: 0.018087363136666163\nSEED: 7, FOLD: 2, EPOCH: 9,train_loss: 0.016997824281292116, valid_loss: 0.01787088593201978\nSEED: 7, FOLD: 2, EPOCH: 10,train_loss: 0.016758538904073444, valid_loss: 0.018076670143221105\nSEED: 7, FOLD: 2, EPOCH: 11,train_loss: 0.016603372241977766, valid_loss: 0.017972267286053726\nSEED: 7, FOLD: 2, EPOCH: 12,train_loss: 0.016200764820087647, valid_loss: 0.018030867113598754\nSEED: 7, FOLD: 2, EPOCH: 13,train_loss: 0.015879607961877533, valid_loss: 0.01835903802088329\nSEED: 7, FOLD: 2, EPOCH: 14,train_loss: 0.015376913685189642, valid_loss: 0.018956780433654785\nSEED: 7, FOLD: 2, EPOCH: 15,train_loss: 0.01467038776077654, valid_loss: 0.0185434185766748\nSEED: 7, FOLD: 2, EPOCH: 16,train_loss: 0.013682457247236069, valid_loss: 0.01909744947084359\nSEED: 7, FOLD: 2, EPOCH: 17,train_loss: 0.01258834118725381, valid_loss: 0.02027033700474671\nSEED: 7, FOLD: 2, EPOCH: 18,train_loss: 0.011173530542494162, valid_loss: 0.020113856505070415\nSEED: 7, FOLD: 2, EPOCH: 19,train_loss: 0.009515917293079521, valid_loss: 0.020602853915521076\nSEED: 7, FOLD: 2, EPOCH: 20,train_loss: 0.008108499480168457, valid_loss: 0.020680628504071916\nSEED: 7, FOLD: 2, EPOCH: 21,train_loss: 0.007075690360658843, valid_loss: 0.020610698631831577\nSEED: 7, FOLD: 2, EPOCH: 22,train_loss: 0.006409292976520415, valid_loss: 0.020832527907831327\nSEED: 7, FOLD: 2, EPOCH: 23,train_loss: 0.006080081716746739, valid_loss: 0.020827726806913104\nSEED: 7, FOLD: 2, EPOCH: 24,train_loss: 0.005958399813001354, valid_loss: 0.020769333520105907\nFOLD: 3, EPOCH: 0,train_loss: 0.7338573588942089, valid_loss: 0.701973603452955\nSEED: 7, FOLD: 3, EPOCH: 0,train_loss: 0.470193067116894, valid_loss: 0.02442616666001933\nSEED: 7, FOLD: 3, EPOCH: 1,train_loss: 0.021560026060817014, valid_loss: 0.02090989761054516\nSEED: 7, FOLD: 3, EPOCH: 2,train_loss: 0.020074764443357495, valid_loss: 0.019344396410243853\nSEED: 7, FOLD: 3, EPOCH: 3,train_loss: 0.018801651895046234, valid_loss: 0.3037201961768525\nSEED: 7, FOLD: 3, EPOCH: 4,train_loss: 0.018020474793810914, valid_loss: 0.01850510181060859\nSEED: 7, FOLD: 3, EPOCH: 5,train_loss: 0.01757936955286856, valid_loss: 0.018366890693349496\nSEED: 7, FOLD: 3, EPOCH: 6,train_loss: 0.017202143910864408, valid_loss: 0.018382235722882407\nSEED: 7, FOLD: 3, EPOCH: 7,train_loss: 0.016977218333223874, valid_loss: 0.018479365270052638\nSEED: 7, FOLD: 3, EPOCH: 8,train_loss: 0.01680547283133016, valid_loss: 0.01853265416409288\nSEED: 7, FOLD: 3, EPOCH: 9,train_loss: 0.0165634153505964, valid_loss: 0.020900289873991694\nSEED: 7, FOLD: 3, EPOCH: 10,train_loss: 0.016359825237878482, valid_loss: 0.018517398408481052\nSEED: 7, FOLD: 3, EPOCH: 11,train_loss: 0.016082643476879075, valid_loss: 0.018639907480350562\nSEED: 7, FOLD: 3, EPOCH: 12,train_loss: 0.015893207543468388, valid_loss: 0.018883604955460345\nSEED: 7, FOLD: 3, EPOCH: 13,train_loss: 0.015450698119608591, valid_loss: 0.019019157492688725\nSEED: 7, FOLD: 3, EPOCH: 14,train_loss: 0.014898435865277357, valid_loss: 0.019536012783646584\nSEED: 7, FOLD: 3, EPOCH: 15,train_loss: 0.014080465805247753, valid_loss: 0.01974478236266545\nSEED: 7, FOLD: 3, EPOCH: 16,train_loss: 0.012941018943368953, valid_loss: 0.020094170634235655\nSEED: 7, FOLD: 3, EPOCH: 17,train_loss: 0.011653181886042122, valid_loss: 0.020639741527182714\nSEED: 7, FOLD: 3, EPOCH: 18,train_loss: 0.010235549718902929, valid_loss: 0.020725143008998463\nSEED: 7, FOLD: 3, EPOCH: 19,train_loss: 0.008663524778383056, valid_loss: 0.021002527698874472\nSEED: 7, FOLD: 3, EPOCH: 20,train_loss: 0.007321883659184414, valid_loss: 0.021300058013626506\nSEED: 7, FOLD: 3, EPOCH: 21,train_loss: 0.006410903932975374, valid_loss: 0.02145302136029516\nSEED: 7, FOLD: 3, EPOCH: 22,train_loss: 0.005874100518514858, valid_loss: 0.021609586104750632\nSEED: 7, FOLD: 3, EPOCH: 23,train_loss: 0.005630983148504348, valid_loss: 0.021613215282559394\nSEED: 7, FOLD: 3, EPOCH: 24,train_loss: 0.0055417039730742464, valid_loss: 0.021625515339629992\nFOLD: 4, EPOCH: 0,train_loss: 0.734135155228601, valid_loss: 0.7041685376848493\nSEED: 7, FOLD: 4, EPOCH: 0,train_loss: 0.46818428933350503, valid_loss: 0.024481414152043208\nSEED: 7, FOLD: 4, EPOCH: 1,train_loss: 0.021622375612133655, valid_loss: 0.021242205000349453\nSEED: 7, FOLD: 4, EPOCH: 2,train_loss: 0.02009788876318413, valid_loss: 0.02946147107120071\nSEED: 7, FOLD: 4, EPOCH: 3,train_loss: 0.01885761455565259, valid_loss: 0.01825092998998506\nSEED: 7, FOLD: 4, EPOCH: 4,train_loss: 0.018098138232270012, valid_loss: 0.018118777525212085\nSEED: 7, FOLD: 4, EPOCH: 5,train_loss: 0.017604828636715378, valid_loss: 0.018062612786889077\nSEED: 7, FOLD: 4, EPOCH: 6,train_loss: 0.01733550919300836, valid_loss: 0.017990929633378984\nSEED: 7, FOLD: 4, EPOCH: 7,train_loss: 0.017070088468060112, valid_loss: 0.018123280789170946\nSEED: 7, FOLD: 4, EPOCH: 8,train_loss: 0.016906869145569162, valid_loss: 0.01824248725814479\nSEED: 7, FOLD: 4, EPOCH: 9,train_loss: 0.016804652126586956, valid_loss: 0.018103283271193503\nSEED: 7, FOLD: 4, EPOCH: 10,train_loss: 0.016665279743788036, valid_loss: 0.018283094838261606\nSEED: 7, FOLD: 4, EPOCH: 11,train_loss: 0.016451649853716725, valid_loss: 0.018619193801922458\nSEED: 7, FOLD: 4, EPOCH: 12,train_loss: 0.016274472786302584, valid_loss: 0.01843298835945981\nSEED: 7, FOLD: 4, EPOCH: 13,train_loss: 0.015952923402622127, valid_loss: 0.018607563711702822\nSEED: 7, FOLD: 4, EPOCH: 14,train_loss: 0.015535637000710636, valid_loss: 0.018478042632341384\nSEED: 7, FOLD: 4, EPOCH: 15,train_loss: 0.014914771393481373, valid_loss: 0.019250128657690116\nSEED: 7, FOLD: 4, EPOCH: 16,train_loss: 0.014241024480619724, valid_loss: 0.019436087991510118\nSEED: 7, FOLD: 4, EPOCH: 17,train_loss: 0.012995393764551567, valid_loss: 0.024298852602285998\nSEED: 7, FOLD: 4, EPOCH: 18,train_loss: 0.01151817598366651, valid_loss: 0.020221743785909244\nSEED: 7, FOLD: 4, EPOCH: 19,train_loss: 0.009957525008560522, valid_loss: 0.021045853942632677\nSEED: 7, FOLD: 4, EPOCH: 20,train_loss: 0.008388241827217997, valid_loss: 0.020834049795355115\nSEED: 7, FOLD: 4, EPOCH: 21,train_loss: 0.00718035476376721, valid_loss: 0.021206871340317384\nSEED: 7, FOLD: 4, EPOCH: 22,train_loss: 0.006421545942219487, valid_loss: 0.021289380320480892\nSEED: 7, FOLD: 4, EPOCH: 23,train_loss: 0.006056887759030729, valid_loss: 0.02123273318367345\nSEED: 7, FOLD: 4, EPOCH: 24,train_loss: 0.005930098212337581, valid_loss: 0.02123358872319971\nFOLD: 0, EPOCH: 0,train_loss: 0.7306096327565882, valid_loss: 0.6923474754605975\nSEED: 8, FOLD: 0, EPOCH: 0,train_loss: 0.4700183246121572, valid_loss: 0.023801594280770846\nSEED: 8, FOLD: 0, EPOCH: 1,train_loss: 0.021675159135004028, valid_loss: 0.057212640345096585\nSEED: 8, FOLD: 0, EPOCH: 2,train_loss: 0.020182760364382806, valid_loss: 0.01914679887039321\nSEED: 8, FOLD: 0, EPOCH: 3,train_loss: 0.018763569734283607, valid_loss: 0.023656717368534634\nSEED: 8, FOLD: 0, EPOCH: 4,train_loss: 0.018035617276319187, valid_loss: 0.018404515486742767\nSEED: 8, FOLD: 0, EPOCH: 5,train_loss: 0.01759875966847813, valid_loss: 0.01844058952161244\nSEED: 8, FOLD: 0, EPOCH: 6,train_loss: 0.017211641123803863, valid_loss: 0.018240269220301083\nSEED: 8, FOLD: 0, EPOCH: 7,train_loss: 0.016860001312609573, valid_loss: 0.018479362316429614\nSEED: 8, FOLD: 0, EPOCH: 8,train_loss: 0.016611615848475998, valid_loss: 0.018215169730995382\nSEED: 8, FOLD: 0, EPOCH: 9,train_loss: 0.01635376851407498, valid_loss: 0.01815471885991948\nSEED: 8, FOLD: 0, EPOCH: 10,train_loss: 0.016130929010627914, valid_loss: 0.01828052295105798\nSEED: 8, FOLD: 0, EPOCH: 11,train_loss: 0.01574982912109716, valid_loss: 0.018608440033027102\nSEED: 8, FOLD: 0, EPOCH: 12,train_loss: 0.015368470038375714, valid_loss: 0.019575277396610805\nSEED: 8, FOLD: 0, EPOCH: 13,train_loss: 0.014855201927142857, valid_loss: 0.01912369786628655\nSEED: 8, FOLD: 0, EPOCH: 14,train_loss: 0.014194343782906984, valid_loss: 0.019190174607293945\nSEED: 8, FOLD: 0, EPOCH: 15,train_loss: 0.013316173201603611, valid_loss: 0.01996762262923377\nSEED: 8, FOLD: 0, EPOCH: 16,train_loss: 0.012202552601314373, valid_loss: 0.020286432174699647\nSEED: 8, FOLD: 0, EPOCH: 17,train_loss: 0.01094385707601361, valid_loss: 0.02109669399048601\nSEED: 8, FOLD: 0, EPOCH: 18,train_loss: 0.009499923638781927, valid_loss: 0.021766809693404606\nSEED: 8, FOLD: 0, EPOCH: 19,train_loss: 0.00814606329548533, valid_loss: 0.021810507348605564\nSEED: 8, FOLD: 0, EPOCH: 20,train_loss: 0.0070044272749202096, valid_loss: 0.021769955434969495\nSEED: 8, FOLD: 0, EPOCH: 21,train_loss: 0.006251560663464513, valid_loss: 0.021600756315248354\nSEED: 8, FOLD: 0, EPOCH: 22,train_loss: 0.005790368103197891, valid_loss: 0.021770424768328668\nSEED: 8, FOLD: 0, EPOCH: 23,train_loss: 0.005569709226978521, valid_loss: 0.02176419475248882\nSEED: 8, FOLD: 0, EPOCH: 24,train_loss: 0.00548604510071939, valid_loss: 0.021830519288778306\nFOLD: 1, EPOCH: 0,train_loss: 0.7310804547607035, valid_loss: 0.6914794870785305\nSEED: 8, FOLD: 1, EPOCH: 0,train_loss: 0.46882100636814383, valid_loss: 0.023590992125017304\nSEED: 8, FOLD: 1, EPOCH: 1,train_loss: 0.021541630916729355, valid_loss: 181.78523769181754\nSEED: 8, FOLD: 1, EPOCH: 2,train_loss: 0.020250112533677315, valid_loss: 0.018975408508309297\nSEED: 8, FOLD: 1, EPOCH: 3,train_loss: 0.018967023038345833, valid_loss: 0.01848949193954468\nSEED: 8, FOLD: 1, EPOCH: 4,train_loss: 0.018224836597084137, valid_loss: 0.018431089286293303\nSEED: 8, FOLD: 1, EPOCH: 5,train_loss: 0.017688299043347008, valid_loss: 0.10472598341958864\nSEED: 8, FOLD: 1, EPOCH: 6,train_loss: 0.017239016258036314, valid_loss: 0.018868633093578474\nSEED: 8, FOLD: 1, EPOCH: 7,train_loss: 0.01695998170264605, valid_loss: 0.018935303310198444\nSEED: 8, FOLD: 1, EPOCH: 8,train_loss: 0.01658298347847185, valid_loss: 0.01861560602805444\nSEED: 8, FOLD: 1, EPOCH: 9,train_loss: 0.016228030854161236, valid_loss: 0.01855399307927915\nSEED: 8, FOLD: 1, EPOCH: 10,train_loss: 0.015950406233415655, valid_loss: 0.019134574569761753\nSEED: 8, FOLD: 1, EPOCH: 11,train_loss: 0.015599013353441504, valid_loss: 0.019176622693027768\nSEED: 8, FOLD: 1, EPOCH: 12,train_loss: 0.015173679818331764, valid_loss: 0.020046247541904448\nSEED: 8, FOLD: 1, EPOCH: 13,train_loss: 0.014658044645751732, valid_loss: 0.02023345200078828\nSEED: 8, FOLD: 1, EPOCH: 14,train_loss: 0.013948490748694841, valid_loss: 0.020316093973815442\nSEED: 8, FOLD: 1, EPOCH: 15,train_loss: 0.013040991695732742, valid_loss: 0.020861271609153065\nSEED: 8, FOLD: 1, EPOCH: 16,train_loss: 0.012126118646583695, valid_loss: 0.021462043055466243\nSEED: 8, FOLD: 1, EPOCH: 17,train_loss: 0.011025732578844696, valid_loss: 0.022541650065353937\nSEED: 8, FOLD: 1, EPOCH: 18,train_loss: 0.009571469510379045, valid_loss: 0.022583184178386416\nSEED: 8, FOLD: 1, EPOCH: 19,train_loss: 0.008346002829004672, valid_loss: 0.022803336807659693\nSEED: 8, FOLD: 1, EPOCH: 20,train_loss: 0.007185536912520943, valid_loss: 0.02251876736325877\nSEED: 8, FOLD: 1, EPOCH: 21,train_loss: 0.006405529738201395, valid_loss: 0.02261170047734465\nSEED: 8, FOLD: 1, EPOCH: 22,train_loss: 0.0059456132433336716, valid_loss: 0.022574637351291522\nSEED: 8, FOLD: 1, EPOCH: 23,train_loss: 0.005733703634283249, valid_loss: 0.02253499712262835\nSEED: 8, FOLD: 1, EPOCH: 24,train_loss: 0.005636501274463059, valid_loss: 0.02248940835041659\nFOLD: 2, EPOCH: 0,train_loss: 0.730750533549682, valid_loss: 0.6923060468264989\nSEED: 8, FOLD: 2, EPOCH: 0,train_loss: 0.4696597773581743, valid_loss: 0.02366598727447646\nSEED: 8, FOLD: 2, EPOCH: 1,train_loss: 0.021670325794189736, valid_loss: 0.020581410612378802\nSEED: 8, FOLD: 2, EPOCH: 2,train_loss: 0.020378257755352104, valid_loss: 0.019025650673678944\nSEED: 8, FOLD: 2, EPOCH: 3,train_loss: 0.01901459668263577, valid_loss: 0.01833349099116666\nSEED: 8, FOLD: 2, EPOCH: 4,train_loss: 0.018225456412503685, valid_loss: 0.018615846947899885\nSEED: 8, FOLD: 2, EPOCH: 5,train_loss: 0.017729044298007004, valid_loss: 0.018550490029156208\nSEED: 8, FOLD: 2, EPOCH: 6,train_loss: 0.01733103980296764, valid_loss: 0.01831056590058974\nSEED: 8, FOLD: 2, EPOCH: 7,train_loss: 0.016978583541577278, valid_loss: 0.05525681637227535\nSEED: 8, FOLD: 2, EPOCH: 8,train_loss: 0.01682992894337445, valid_loss: 0.01845882170434509\nSEED: 8, FOLD: 2, EPOCH: 9,train_loss: 0.01663006572862682, valid_loss: 0.018301613043461527\nSEED: 8, FOLD: 2, EPOCH: 10,train_loss: 0.01644201995804906, valid_loss: 0.018164383247494698\nSEED: 8, FOLD: 2, EPOCH: 11,train_loss: 0.0161306276930955, valid_loss: 0.018468969315290452\nSEED: 8, FOLD: 2, EPOCH: 12,train_loss: 0.01583642194695447, valid_loss: 0.018394593096205166\nSEED: 8, FOLD: 2, EPOCH: 13,train_loss: 0.01548132423878364, valid_loss: 0.018910140597394536\nSEED: 8, FOLD: 2, EPOCH: 14,train_loss: 0.01484028133901133, valid_loss: 0.018870619551411696\nSEED: 8, FOLD: 2, EPOCH: 15,train_loss: 0.014020340496917133, valid_loss: 0.019179527780839374\nSEED: 8, FOLD: 2, EPOCH: 16,train_loss: 0.012808544372302898, valid_loss: 0.02002249506435224\nSEED: 8, FOLD: 2, EPOCH: 17,train_loss: 0.011475202681469744, valid_loss: 0.020344798054013933\nSEED: 8, FOLD: 2, EPOCH: 18,train_loss: 0.010034638787687256, valid_loss: 0.020395485205309732\nSEED: 8, FOLD: 2, EPOCH: 19,train_loss: 0.008463149249175753, valid_loss: 0.020917828460889205\nSEED: 8, FOLD: 2, EPOCH: 20,train_loss: 0.007196548911135482, valid_loss: 0.021187736626182285\nSEED: 8, FOLD: 2, EPOCH: 21,train_loss: 0.006354483104297432, valid_loss: 0.02255842217377254\nSEED: 8, FOLD: 2, EPOCH: 22,train_loss: 0.005869515790212629, valid_loss: 0.021261151454278402\nSEED: 8, FOLD: 2, EPOCH: 23,train_loss: 0.005634479720712356, valid_loss: 0.021499428365911757\nSEED: 8, FOLD: 2, EPOCH: 24,train_loss: 0.005550531140677091, valid_loss: 0.025609265001756806\nFOLD: 3, EPOCH: 0,train_loss: 0.7309795125671055, valid_loss: 0.6903187547411237\nSEED: 8, FOLD: 3, EPOCH: 0,train_loss: 0.46896703313172294, valid_loss: 0.023352397925087382\nSEED: 8, FOLD: 3, EPOCH: 1,train_loss: 0.021911564795975235, valid_loss: 0.02105848980801446\nSEED: 8, FOLD: 3, EPOCH: 2,train_loss: 0.020346184598578922, valid_loss: 0.01933052558451891\nSEED: 8, FOLD: 3, EPOCH: 3,train_loss: 0.018968497720155596, valid_loss: 0.018470052549881596\nSEED: 8, FOLD: 3, EPOCH: 4,train_loss: 0.01824308154375657, valid_loss: 0.018060775074575628\nSEED: 8, FOLD: 3, EPOCH: 5,train_loss: 0.01778800657990834, valid_loss: 0.01783548974032913\nSEED: 8, FOLD: 3, EPOCH: 6,train_loss: 0.01736042797025563, valid_loss: 0.017786810281021255\nSEED: 8, FOLD: 3, EPOCH: 7,train_loss: 0.01709267169075168, valid_loss: 0.575573092832097\nSEED: 8, FOLD: 3, EPOCH: 8,train_loss: 0.0168788961701743, valid_loss: 0.0419460190726178\nSEED: 8, FOLD: 3, EPOCH: 9,train_loss: 0.016831915535410677, valid_loss: 0.023463400506547518\nSEED: 8, FOLD: 3, EPOCH: 10,train_loss: 0.016569127706621868, valid_loss: 0.017823132340397153\nSEED: 8, FOLD: 3, EPOCH: 11,train_loss: 0.016484730445064495, valid_loss: 0.017788484186998434\nSEED: 8, FOLD: 3, EPOCH: 12,train_loss: 0.016336502750282703, valid_loss: 0.018301723684583392\nSEED: 8, FOLD: 3, EPOCH: 13,train_loss: 0.01592941673985426, valid_loss: 0.017977413535118104\nSEED: 8, FOLD: 3, EPOCH: 14,train_loss: 0.015407589999823898, valid_loss: 0.018247967266610692\nSEED: 8, FOLD: 3, EPOCH: 15,train_loss: 0.014673063936440842, valid_loss: 0.019176464527845383\nSEED: 8, FOLD: 3, EPOCH: 16,train_loss: 0.013750501726146626, valid_loss: 0.01919604486652783\nSEED: 8, FOLD: 3, EPOCH: 17,train_loss: 0.01252649456559532, valid_loss: 0.019808670399444443\nSEED: 8, FOLD: 3, EPOCH: 18,train_loss: 0.010993989960600933, valid_loss: 0.020574358238705567\nSEED: 8, FOLD: 3, EPOCH: 19,train_loss: 0.009327472099845392, valid_loss: 0.02068169760916914\nSEED: 8, FOLD: 3, EPOCH: 20,train_loss: 0.00778153851169391, valid_loss: 0.02089460516082389\nSEED: 8, FOLD: 3, EPOCH: 21,train_loss: 0.0067356283256811075, valid_loss: 0.021031451278499196\nSEED: 8, FOLD: 3, EPOCH: 22,train_loss: 0.006119136438718525, valid_loss: 0.021596804474081313\nSEED: 8, FOLD: 3, EPOCH: 23,train_loss: 0.005823987968050052, valid_loss: 0.02135032516505037\nSEED: 8, FOLD: 3, EPOCH: 24,train_loss: 0.0057143589339988385, valid_loss: 0.021658358084303993\nFOLD: 4, EPOCH: 0,train_loss: 0.7312814087971397, valid_loss: 0.6904568484851292\nSEED: 8, FOLD: 4, EPOCH: 0,train_loss: 0.4698245622271645, valid_loss: 0.024060887630496706\nSEED: 8, FOLD: 4, EPOCH: 1,train_loss: 0.021748772552371887, valid_loss: 0.022520185368401665\nSEED: 8, FOLD: 4, EPOCH: 2,train_loss: 0.020468469182758228, valid_loss: 0.019353211032492774\nSEED: 8, FOLD: 4, EPOCH: 3,train_loss: 0.01915291708934566, valid_loss: 0.01938669764037643\nSEED: 8, FOLD: 4, EPOCH: 4,train_loss: 0.01842749357034547, valid_loss: 0.018424982843654496\nSEED: 8, FOLD: 4, EPOCH: 5,train_loss: 0.017953957147572353, valid_loss: 0.018217420844095094\nSEED: 8, FOLD: 4, EPOCH: 6,train_loss: 0.017602300028438152, valid_loss: 0.017995222391826767\nSEED: 8, FOLD: 4, EPOCH: 7,train_loss: 0.017334590868457504, valid_loss: 0.01831604064043079\nSEED: 8, FOLD: 4, EPOCH: 8,train_loss: 0.01710959274213815, valid_loss: 0.018803761340677738\nSEED: 8, FOLD: 4, EPOCH: 9,train_loss: 0.01690498358853485, valid_loss: 0.018232336321047374\nSEED: 8, FOLD: 4, EPOCH: 10,train_loss: 0.01666554048711407, valid_loss: 0.018008070279444968\nSEED: 8, FOLD: 4, EPOCH: 11,train_loss: 0.0164947343274843, valid_loss: 0.01887465217815978\nSEED: 8, FOLD: 4, EPOCH: 12,train_loss: 0.01611559674737678, valid_loss: 0.01821961442806891\nSEED: 8, FOLD: 4, EPOCH: 13,train_loss: 0.015800259271771578, valid_loss: 0.01902243106492928\nSEED: 8, FOLD: 4, EPOCH: 14,train_loss: 0.015369437317755343, valid_loss: 0.041666000549282343\nSEED: 8, FOLD: 4, EPOCH: 15,train_loss: 0.014926030314055042, valid_loss: 0.018535755654530865\nSEED: 8, FOLD: 4, EPOCH: 16,train_loss: 0.013737947221143522, valid_loss: 0.019337147288024426\nSEED: 8, FOLD: 4, EPOCH: 17,train_loss: 0.012666066406645637, valid_loss: 0.019965597135680063\nSEED: 8, FOLD: 4, EPOCH: 18,train_loss: 0.011280430561822393, valid_loss: 0.020340703135090215\nSEED: 8, FOLD: 4, EPOCH: 19,train_loss: 0.009644797071814537, valid_loss: 0.020729640711631095\nSEED: 8, FOLD: 4, EPOCH: 20,train_loss: 0.008265261157024383, valid_loss: 0.02094964821423803\nSEED: 8, FOLD: 4, EPOCH: 21,train_loss: 0.007215962623772414, valid_loss: 0.021175804255264146\nSEED: 8, FOLD: 4, EPOCH: 22,train_loss: 0.006560904682492433, valid_loss: 0.021371740847826004\nSEED: 8, FOLD: 4, EPOCH: 23,train_loss: 0.006197489389771785, valid_loss: 0.021429688057729174\nSEED: 8, FOLD: 4, EPOCH: 24,train_loss: 0.006066255716849928, valid_loss: 0.021417633284415517\nFOLD: 0, EPOCH: 0,train_loss: 0.7298235323118127, valid_loss: 0.6953397239957537\nSEED: 9, FOLD: 0, EPOCH: 0,train_loss: 0.4696880573325831, valid_loss: 0.02369929983147553\nSEED: 9, FOLD: 0, EPOCH: 1,train_loss: 0.021841571760782295, valid_loss: 0.020574302545615604\nSEED: 9, FOLD: 0, EPOCH: 2,train_loss: 0.0200845529122845, valid_loss: 0.01851718079830919\nSEED: 9, FOLD: 0, EPOCH: 3,train_loss: 0.018672257297388885, valid_loss: 0.0177552105858922\nSEED: 9, FOLD: 0, EPOCH: 4,train_loss: 0.018137712288054005, valid_loss: 0.017973983021719115\nSEED: 9, FOLD: 0, EPOCH: 5,train_loss: 0.017581538472702538, valid_loss: 0.017968495110315934\nSEED: 9, FOLD: 0, EPOCH: 6,train_loss: 0.017181172474300947, valid_loss: 0.01810578192983355\nSEED: 9, FOLD: 0, EPOCH: 7,train_loss: 0.01694331438942016, valid_loss: 0.0180694278595703\nSEED: 9, FOLD: 0, EPOCH: 8,train_loss: 0.016710341084694515, valid_loss: 0.018947577183800083\nSEED: 9, FOLD: 0, EPOCH: 9,train_loss: 0.016585649301608402, valid_loss: 0.018166639309908663\nSEED: 9, FOLD: 0, EPOCH: 10,train_loss: 0.016437975608784218, valid_loss: 0.019103934429585932\nSEED: 9, FOLD: 0, EPOCH: 11,train_loss: 0.01635495302658798, valid_loss: 0.018319115069295677\nSEED: 9, FOLD: 0, EPOCH: 12,train_loss: 0.015999874808704077, valid_loss: 0.018520357699266504\nSEED: 9, FOLD: 0, EPOCH: 13,train_loss: 0.015492983884515537, valid_loss: 0.018553578268204418\nSEED: 9, FOLD: 0, EPOCH: 14,train_loss: 0.0149594332712392, valid_loss: 0.01891991655741419\nSEED: 9, FOLD: 0, EPOCH: 15,train_loss: 0.014255797321759705, valid_loss: 0.019332244726164\nSEED: 9, FOLD: 0, EPOCH: 16,train_loss: 0.013305933450929064, valid_loss: 0.020376187775816235\nSEED: 9, FOLD: 0, EPOCH: 17,train_loss: 0.012091260626102272, valid_loss: 0.020467642109308924\nSEED: 9, FOLD: 0, EPOCH: 18,train_loss: 0.010682025208961273, valid_loss: 0.020527803099581174\nSEED: 9, FOLD: 0, EPOCH: 19,train_loss: 0.009170092365609995, valid_loss: 0.020898957390870367\nSEED: 9, FOLD: 0, EPOCH: 20,train_loss: 0.007791598074381118, valid_loss: 0.021203512219446045\nSEED: 9, FOLD: 0, EPOCH: 21,train_loss: 0.006780024779879529, valid_loss: 0.021204817295074462\nSEED: 9, FOLD: 0, EPOCH: 22,train_loss: 0.006180217773046183, valid_loss: 0.021288467198610307\nSEED: 9, FOLD: 0, EPOCH: 23,train_loss: 0.005890057010549134, valid_loss: 0.021250439382025175\nSEED: 9, FOLD: 0, EPOCH: 24,train_loss: 0.005786542922420346, valid_loss: 0.0212618530860969\nFOLD: 1, EPOCH: 0,train_loss: 0.7298464435730537, valid_loss: 0.692102655342647\nSEED: 9, FOLD: 1, EPOCH: 0,train_loss: 0.470149816455741, valid_loss: 0.023664194984095437\nSEED: 9, FOLD: 1, EPOCH: 1,train_loss: 0.02191091821032719, valid_loss: 0.027138083960328782\nSEED: 9, FOLD: 1, EPOCH: 2,train_loss: 0.02017670007844041, valid_loss: 0.019231354924184936\nSEED: 9, FOLD: 1, EPOCH: 3,train_loss: 0.01891328932812614, valid_loss: 0.01826022049146039\nSEED: 9, FOLD: 1, EPOCH: 4,train_loss: 0.01808756503555244, valid_loss: 0.01790019204573972\nSEED: 9, FOLD: 1, EPOCH: 5,train_loss: 0.01755647431977474, valid_loss: 0.018107156003160135\nSEED: 9, FOLD: 1, EPOCH: 6,train_loss: 0.017274989413410206, valid_loss: 0.01819518195199115\nSEED: 9, FOLD: 1, EPOCH: 7,train_loss: 0.017047284551671822, valid_loss: 0.018058297969400883\nSEED: 9, FOLD: 1, EPOCH: 8,train_loss: 0.016776699430044114, valid_loss: 0.01811961904168129\nSEED: 9, FOLD: 1, EPOCH: 9,train_loss: 0.01667883727753902, valid_loss: 0.018207669497600623\nSEED: 9, FOLD: 1, EPOCH: 10,train_loss: 0.016574846220331906, valid_loss: 0.01814978439360857\nSEED: 9, FOLD: 1, EPOCH: 11,train_loss: 0.016226616531719258, valid_loss: 0.018325314112007617\nSEED: 9, FOLD: 1, EPOCH: 12,train_loss: 0.016030363521001637, valid_loss: 0.018209330524717057\nSEED: 9, FOLD: 1, EPOCH: 13,train_loss: 0.015594517574203711, valid_loss: 0.018622379723404137\nSEED: 9, FOLD: 1, EPOCH: 14,train_loss: 0.015037002592571895, valid_loss: 0.018820515647530556\nSEED: 9, FOLD: 1, EPOCH: 15,train_loss: 0.014219686572514746, valid_loss: 0.018816407423998628\nSEED: 9, FOLD: 1, EPOCH: 16,train_loss: 0.01326888471325166, valid_loss: 0.019428435393742154\nSEED: 9, FOLD: 1, EPOCH: 17,train_loss: 0.011874140002322894, valid_loss: 0.019782784740839686\nSEED: 9, FOLD: 1, EPOCH: 18,train_loss: 0.010359050121403089, valid_loss: 0.020597419887781142\nSEED: 9, FOLD: 1, EPOCH: 19,train_loss: 0.008828112828361727, valid_loss: 0.020894984528422356\nSEED: 9, FOLD: 1, EPOCH: 20,train_loss: 0.007501500709675742, valid_loss: 0.021135284219469343\nSEED: 9, FOLD: 1, EPOCH: 21,train_loss: 0.006524844893872956, valid_loss: 0.021202697711331504\nSEED: 9, FOLD: 1, EPOCH: 22,train_loss: 0.005965128428826149, valid_loss: 0.021214001306465693\nSEED: 9, FOLD: 1, EPOCH: 23,train_loss: 0.005690467346758738, valid_loss: 0.02129104488662311\nSEED: 9, FOLD: 1, EPOCH: 24,train_loss: 0.005593993011046282, valid_loss: 0.021321870014071466\nFOLD: 2, EPOCH: 0,train_loss: 0.7294595353845237, valid_loss: 0.6940509864262172\nSEED: 9, FOLD: 2, EPOCH: 0,train_loss: 0.4704100347758419, valid_loss: 0.024338094517588615\nSEED: 9, FOLD: 2, EPOCH: 1,train_loss: 0.0214267760326249, valid_loss: 0.02353732106941087\nSEED: 9, FOLD: 2, EPOCH: 2,train_loss: 0.02013582458206709, valid_loss: 0.019548556261828966\nSEED: 9, FOLD: 2, EPOCH: 3,train_loss: 0.018814261732757954, valid_loss: 0.019733909783618792\nSEED: 9, FOLD: 2, EPOCH: 4,train_loss: 0.01811124302068914, valid_loss: 0.02065189065677779\nSEED: 9, FOLD: 2, EPOCH: 5,train_loss: 0.017649262218965567, valid_loss: 0.018306814879179\nSEED: 9, FOLD: 2, EPOCH: 6,train_loss: 0.017339418129320595, valid_loss: 0.2986215802175658\nSEED: 9, FOLD: 2, EPOCH: 7,train_loss: 0.017110945299213778, valid_loss: 0.018226529977151327\nSEED: 9, FOLD: 2, EPOCH: 8,train_loss: 0.016985536627201498, valid_loss: 0.018799183304820742\nSEED: 9, FOLD: 2, EPOCH: 9,train_loss: 0.016839448065645454, valid_loss: 0.01813538864787136\nSEED: 9, FOLD: 2, EPOCH: 10,train_loss: 0.01675459411184209, valid_loss: 0.018265145511499475\nSEED: 9, FOLD: 2, EPOCH: 11,train_loss: 0.016526854131370783, valid_loss: 0.01862716509827546\nSEED: 9, FOLD: 2, EPOCH: 12,train_loss: 0.01625643737371201, valid_loss: 0.01830516925879887\nSEED: 9, FOLD: 2, EPOCH: 13,train_loss: 0.015874118253966604, valid_loss: 0.01803331750312022\nSEED: 9, FOLD: 2, EPOCH: 14,train_loss: 0.015333396477111872, valid_loss: 0.019205655104347637\nSEED: 9, FOLD: 2, EPOCH: 15,train_loss: 0.014836447239192068, valid_loss: 0.01876040186200823\nSEED: 9, FOLD: 2, EPOCH: 16,train_loss: 0.014164057398295921, valid_loss: 0.019513979820268496\nSEED: 9, FOLD: 2, EPOCH: 17,train_loss: 0.01285371638537533, valid_loss: 0.02012450248003006\nSEED: 9, FOLD: 2, EPOCH: 18,train_loss: 0.011466106075955473, valid_loss: 0.020068635631884848\nSEED: 9, FOLD: 2, EPOCH: 19,train_loss: 0.009883186037557713, valid_loss: 0.020530158387763158\nSEED: 9, FOLD: 2, EPOCH: 20,train_loss: 0.008399383195311479, valid_loss: 0.02087169225726809\nSEED: 9, FOLD: 2, EPOCH: 21,train_loss: 0.007302068018664916, valid_loss: 0.021186998805829457\nSEED: 9, FOLD: 2, EPOCH: 22,train_loss: 0.0065752634630147095, valid_loss: 0.021128369069525175\nSEED: 9, FOLD: 2, EPOCH: 23,train_loss: 0.0062323324382305145, valid_loss: 0.02124350629746914\nSEED: 9, FOLD: 2, EPOCH: 24,train_loss: 0.006072622443805786, valid_loss: 0.02123940975538322\nFOLD: 3, EPOCH: 0,train_loss: 0.7300600863021353, valid_loss: 0.6951032719191383\nSEED: 9, FOLD: 3, EPOCH: 0,train_loss: 0.46843790794736234, valid_loss: 0.025237484451602486\nSEED: 9, FOLD: 3, EPOCH: 1,train_loss: 0.021783053362067196, valid_loss: 0.021289541824337316\nSEED: 9, FOLD: 3, EPOCH: 2,train_loss: 0.020234229839474396, valid_loss: 0.01924243163974846\nSEED: 9, FOLD: 3, EPOCH: 3,train_loss: 0.018763941186277763, valid_loss: 0.0184979943339439\nSEED: 9, FOLD: 3, EPOCH: 4,train_loss: 0.01792574601004953, valid_loss: 0.01857918846037458\nSEED: 9, FOLD: 3, EPOCH: 5,train_loss: 0.017437777974629316, valid_loss: 0.018286554279791957\nSEED: 9, FOLD: 3, EPOCH: 6,train_loss: 0.01713524150081735, valid_loss: 0.01848206466392559\nSEED: 9, FOLD: 3, EPOCH: 7,train_loss: 0.016896749367478533, valid_loss: 0.01849185795906712\nSEED: 9, FOLD: 3, EPOCH: 8,train_loss: 0.016704081847885813, valid_loss: 0.01862958336577696\nSEED: 9, FOLD: 3, EPOCH: 9,train_loss: 0.016478934622221234, valid_loss: 0.018587883808376157\nSEED: 9, FOLD: 3, EPOCH: 10,train_loss: 0.016341391080261572, valid_loss: 0.01884165871888399\nSEED: 9, FOLD: 3, EPOCH: 11,train_loss: 0.0160987153581843, valid_loss: 0.019138213904464945\nSEED: 9, FOLD: 3, EPOCH: 12,train_loss: 0.01577797093415174, valid_loss: 0.0187430729103439\nSEED: 9, FOLD: 3, EPOCH: 13,train_loss: 0.015341310768617668, valid_loss: 0.01926663956221412\nSEED: 9, FOLD: 3, EPOCH: 14,train_loss: 0.014781319036863852, valid_loss: 0.019415266811847687\nSEED: 9, FOLD: 3, EPOCH: 15,train_loss: 0.014071756400221931, valid_loss: 0.0195075316245065\nSEED: 9, FOLD: 3, EPOCH: 16,train_loss: 0.013023752354733322, valid_loss: 0.020395992104621494\nSEED: 9, FOLD: 3, EPOCH: 17,train_loss: 0.011788333489465109, valid_loss: 0.021334484274334767\nSEED: 9, FOLD: 3, EPOCH: 18,train_loss: 0.010359062684996836, valid_loss: 0.021445900585283253\nSEED: 9, FOLD: 3, EPOCH: 19,train_loss: 0.00884604192527848, valid_loss: 0.021480963291490778\nSEED: 9, FOLD: 3, EPOCH: 20,train_loss: 0.007513181633734401, valid_loss: 0.021785736850955906\nSEED: 9, FOLD: 3, EPOCH: 21,train_loss: 0.006580499861308414, valid_loss: 0.021718123325091952\nSEED: 9, FOLD: 3, EPOCH: 22,train_loss: 0.006021758983505593, valid_loss: 0.02189025129465496\nSEED: 9, FOLD: 3, EPOCH: 23,train_loss: 0.005767686241913749, valid_loss: 0.02189159590531798\nSEED: 9, FOLD: 3, EPOCH: 24,train_loss: 0.005641926306070409, valid_loss: 0.021944271181436145\nFOLD: 4, EPOCH: 0,train_loss: 0.7298861994360485, valid_loss: 0.6939903974533081\nSEED: 9, FOLD: 4, EPOCH: 0,train_loss: 0.4713687039503868, valid_loss: 0.023873851288642202\nSEED: 9, FOLD: 4, EPOCH: 1,train_loss: 0.0220517360190623, valid_loss: 0.02103973834642342\nSEED: 9, FOLD: 4, EPOCH: 2,train_loss: 0.020642928072135813, valid_loss: 0.019261914544871875\nSEED: 9, FOLD: 4, EPOCH: 3,train_loss: 0.019295677610666213, valid_loss: 0.10022721370416028\nSEED: 9, FOLD: 4, EPOCH: 4,train_loss: 0.0186162133612772, valid_loss: 0.018406581399696215\nSEED: 9, FOLD: 4, EPOCH: 5,train_loss: 0.018113917623558184, valid_loss: 0.01821636429854802\nSEED: 9, FOLD: 4, EPOCH: 6,train_loss: 0.017753919640923068, valid_loss: 0.018011532511029923\nSEED: 9, FOLD: 4, EPOCH: 7,train_loss: 0.017463628936858072, valid_loss: 0.017918872833251952\nSEED: 9, FOLD: 4, EPOCH: 8,train_loss: 0.017202486107329818, valid_loss: 0.01826520971953869\nSEED: 9, FOLD: 4, EPOCH: 9,train_loss: 0.017039699649886927, valid_loss: 0.01798259248690946\nSEED: 9, FOLD: 4, EPOCH: 10,train_loss: 0.016757307982031445, valid_loss: 0.018067850704704014\nSEED: 9, FOLD: 4, EPOCH: 11,train_loss: 0.016532218790728682, valid_loss: 0.017872664518654345\nSEED: 9, FOLD: 4, EPOCH: 12,train_loss: 0.016203465252897165, valid_loss: 0.018272313502218043\nSEED: 9, FOLD: 4, EPOCH: 13,train_loss: 0.015849837156379745, valid_loss: 0.01852569675871304\nSEED: 9, FOLD: 4, EPOCH: 14,train_loss: 0.015311444307385135, valid_loss: 0.018608941776411873\nSEED: 9, FOLD: 4, EPOCH: 15,train_loss: 0.014599410504320242, valid_loss: 0.01927064433693886\nSEED: 9, FOLD: 4, EPOCH: 16,train_loss: 0.013656485771393254, valid_loss: 0.019738510996103287\nSEED: 9, FOLD: 4, EPOCH: 17,train_loss: 0.012557593249056462, valid_loss: 0.02040035602237497\nSEED: 9, FOLD: 4, EPOCH: 18,train_loss: 0.011198586308444938, valid_loss: 0.02018579249935491\nSEED: 9, FOLD: 4, EPOCH: 19,train_loss: 0.009644148776130955, valid_loss: 0.02057936420398099\nSEED: 9, FOLD: 4, EPOCH: 20,train_loss: 0.00825854167886024, valid_loss: 0.02072101411010538\nSEED: 9, FOLD: 4, EPOCH: 21,train_loss: 0.007187794054430114, valid_loss: 0.020908154440777642\nSEED: 9, FOLD: 4, EPOCH: 22,train_loss: 0.006524722530323006, valid_loss: 0.021013062660183227\nSEED: 9, FOLD: 4, EPOCH: 23,train_loss: 0.006226242791834103, valid_loss: 0.02106277367898396\nSEED: 9, FOLD: 4, EPOCH: 24,train_loss: 0.0060892417265550934, valid_loss: 0.02105779248688902\nFOLD: 0, EPOCH: 0,train_loss: 0.7320895911990732, valid_loss: 0.6918745689532336\nSEED: 10, FOLD: 0, EPOCH: 0,train_loss: 0.46836719953495526, valid_loss: 0.025031865793554223\nSEED: 10, FOLD: 0, EPOCH: 1,train_loss: 0.02182803334047397, valid_loss: 0.02140820015440969\nSEED: 10, FOLD: 0, EPOCH: 2,train_loss: 0.020174685703671497, valid_loss: 0.019221676513552666\nSEED: 10, FOLD: 0, EPOCH: 3,train_loss: 0.018967507231602634, valid_loss: 0.01858788709539701\nSEED: 10, FOLD: 0, EPOCH: 4,train_loss: 0.018253560380443283, valid_loss: 0.018525591695352513\nSEED: 10, FOLD: 0, EPOCH: 5,train_loss: 0.017670082927182102, valid_loss: 0.01838855946655659\nSEED: 10, FOLD: 0, EPOCH: 6,train_loss: 0.017495770857709904, valid_loss: 0.018234746996313334\nSEED: 10, FOLD: 0, EPOCH: 7,train_loss: 0.017125530546342117, valid_loss: 0.01816728481036775\nSEED: 10, FOLD: 0, EPOCH: 8,train_loss: 0.016906742343976013, valid_loss: 0.01832772561294191\nSEED: 10, FOLD: 0, EPOCH: 9,train_loss: 0.016836698436974617, valid_loss: 0.018344975898371023\nSEED: 10, FOLD: 0, EPOCH: 10,train_loss: 0.0166537513875443, valid_loss: 0.0184744569911238\nSEED: 10, FOLD: 0, EPOCH: 11,train_loss: 0.016541097306416952, valid_loss: 0.01851943926885724\nSEED: 10, FOLD: 0, EPOCH: 12,train_loss: 0.016218152686791575, valid_loss: 0.018539124469765845\nSEED: 10, FOLD: 0, EPOCH: 13,train_loss: 0.01590621002369385, valid_loss: 0.018503077711690876\nSEED: 10, FOLD: 0, EPOCH: 14,train_loss: 0.015527295691055664, valid_loss: 0.018744662650586927\nSEED: 10, FOLD: 0, EPOCH: 15,train_loss: 0.014862481700391441, valid_loss: 0.01908346835304709\nSEED: 10, FOLD: 0, EPOCH: 16,train_loss: 0.014128285628891941, valid_loss: 0.019481681835125473\nSEED: 10, FOLD: 0, EPOCH: 17,train_loss: 0.012997088798632225, valid_loss: 0.31070403053480034\nSEED: 10, FOLD: 0, EPOCH: 18,train_loss: 0.011446465387184551, valid_loss: 0.021048474618617224\nSEED: 10, FOLD: 0, EPOCH: 19,train_loss: 0.009796160381233347, valid_loss: 0.021276668426306808\nSEED: 10, FOLD: 0, EPOCH: 20,train_loss: 0.00824908730978875, valid_loss: 0.02136135506717598\nSEED: 10, FOLD: 0, EPOCH: 21,train_loss: 0.007063966770858869, valid_loss: 0.02168230986332192\nSEED: 10, FOLD: 0, EPOCH: 22,train_loss: 0.0063801540864928475, valid_loss: 0.021764820770305747\nSEED: 10, FOLD: 0, EPOCH: 23,train_loss: 0.006021973150579825, valid_loss: 0.021778551313806984\nSEED: 10, FOLD: 0, EPOCH: 24,train_loss: 0.005885639342654875, valid_loss: 0.02182054497739848\nFOLD: 1, EPOCH: 0,train_loss: 0.7319386866841003, valid_loss: 0.6891461494896147\nSEED: 10, FOLD: 1, EPOCH: 0,train_loss: 0.47022931536074974, valid_loss: 0.02380241531257828\nSEED: 10, FOLD: 1, EPOCH: 1,train_loss: 0.021825238192168465, valid_loss: 0.020765245167745486\nSEED: 10, FOLD: 1, EPOCH: 2,train_loss: 0.02002117943263402, valid_loss: 0.01895011852805813\nSEED: 10, FOLD: 1, EPOCH: 3,train_loss: 0.018787762687208442, valid_loss: 0.01862004642478294\nSEED: 10, FOLD: 1, EPOCH: 4,train_loss: 0.018105782337323593, valid_loss: 0.018162167579349544\nSEED: 10, FOLD: 1, EPOCH: 5,train_loss: 0.017516885292682336, valid_loss: 0.018228673013961978\nSEED: 10, FOLD: 1, EPOCH: 6,train_loss: 0.017263509543870924, valid_loss: 0.02245190294666423\nSEED: 10, FOLD: 1, EPOCH: 7,train_loss: 0.017000730917619094, valid_loss: 0.018016431118465133\nSEED: 10, FOLD: 1, EPOCH: 8,train_loss: 0.016970795334527528, valid_loss: 0.017805197120954592\nSEED: 10, FOLD: 1, EPOCH: 9,train_loss: 0.01687019662999541, valid_loss: 0.017944246866843767\nSEED: 10, FOLD: 1, EPOCH: 10,train_loss: 0.016681484993628776, valid_loss: 0.01824994685335292\nSEED: 10, FOLD: 1, EPOCH: 11,train_loss: 0.016499451150859358, valid_loss: 0.01828765871727632\nSEED: 10, FOLD: 1, EPOCH: 12,train_loss: 0.016242937225658092, valid_loss: 0.01898731731085314\nSEED: 10, FOLD: 1, EPOCH: 13,train_loss: 0.015865965243292986, valid_loss: 0.018375980098628335\nSEED: 10, FOLD: 1, EPOCH: 14,train_loss: 0.015373504292355837, valid_loss: 0.01896870539834102\nSEED: 10, FOLD: 1, EPOCH: 15,train_loss: 0.014693888944376558, valid_loss: 0.01900369172087974\nSEED: 10, FOLD: 1, EPOCH: 16,train_loss: 0.013700687040975929, valid_loss: 0.01964888468177782\nSEED: 10, FOLD: 1, EPOCH: 17,train_loss: 0.01247095912151093, valid_loss: 0.019919351157214906\nSEED: 10, FOLD: 1, EPOCH: 18,train_loss: 0.011046062733461388, valid_loss: 0.020822850935575034\nSEED: 10, FOLD: 1, EPOCH: 19,train_loss: 0.00939489901745624, valid_loss: 0.02068304777559307\nSEED: 10, FOLD: 1, EPOCH: 20,train_loss: 0.007893396779405374, valid_loss: 0.020894275202105444\nSEED: 10, FOLD: 1, EPOCH: 21,train_loss: 0.00679711339465023, valid_loss: 0.021252051047566865\nSEED: 10, FOLD: 1, EPOCH: 22,train_loss: 0.006170239472884114, valid_loss: 0.02117034068538083\nSEED: 10, FOLD: 1, EPOCH: 23,train_loss: 0.005861308734972764, valid_loss: 0.021255151368677616\nSEED: 10, FOLD: 1, EPOCH: 24,train_loss: 0.005762271154807867, valid_loss: 0.021315231246666774\nFOLD: 2, EPOCH: 0,train_loss: 0.7319194367830304, valid_loss: 0.6909674371991839\nSEED: 10, FOLD: 2, EPOCH: 0,train_loss: 0.46892044736423355, valid_loss: 0.024354436727506774\nSEED: 10, FOLD: 2, EPOCH: 1,train_loss: 0.021667102635230705, valid_loss: 0.02300115664090429\nSEED: 10, FOLD: 2, EPOCH: 2,train_loss: 0.020108854141680226, valid_loss: 0.01952984082911696\nSEED: 10, FOLD: 2, EPOCH: 3,train_loss: 0.01882257328733154, valid_loss: 0.018474002342138973\nSEED: 10, FOLD: 2, EPOCH: 4,train_loss: 0.018066321384917566, valid_loss: 0.018139941724283355\nSEED: 10, FOLD: 2, EPOCH: 5,train_loss: 0.017505610699131004, valid_loss: 0.018207119325441973\nSEED: 10, FOLD: 2, EPOCH: 6,train_loss: 0.017060937596134085, valid_loss: 0.0183016228888716\nSEED: 10, FOLD: 2, EPOCH: 7,train_loss: 0.016780830946737442, valid_loss: 0.019257623116884913\nSEED: 10, FOLD: 2, EPOCH: 8,train_loss: 0.016634158047752968, valid_loss: 0.01838420033454895\nSEED: 10, FOLD: 2, EPOCH: 9,train_loss: 0.016465067600264498, valid_loss: 0.01845837788922446\nSEED: 10, FOLD: 2, EPOCH: 10,train_loss: 0.016265408736586138, valid_loss: 0.01851032706243651\nSEED: 10, FOLD: 2, EPOCH: 11,train_loss: 0.016112401258146416, valid_loss: 0.018966570336903844\nSEED: 10, FOLD: 2, EPOCH: 12,train_loss: 0.01577588035117673, valid_loss: 0.019210032267229896\nSEED: 10, FOLD: 2, EPOCH: 13,train_loss: 0.015381675341800936, valid_loss: 0.01926230059138366\nSEED: 10, FOLD: 2, EPOCH: 14,train_loss: 0.014919041669455128, valid_loss: 0.019494873657822608\nSEED: 10, FOLD: 2, EPOCH: 15,train_loss: 0.01403155088748621, valid_loss: 0.02000785619020462\nSEED: 10, FOLD: 2, EPOCH: 16,train_loss: 0.01291755442440078, valid_loss: 0.020286071673035623\nSEED: 10, FOLD: 2, EPOCH: 17,train_loss: 0.011490017506361439, valid_loss: 0.020624160660164696\nSEED: 10, FOLD: 2, EPOCH: 18,train_loss: 0.009999463888074177, valid_loss: 0.021928963969860757\nSEED: 10, FOLD: 2, EPOCH: 19,train_loss: 0.008466470102523115, valid_loss: 0.028800392523407936\nSEED: 10, FOLD: 2, EPOCH: 20,train_loss: 0.007168030194209322, valid_loss: 0.02187942388866629\nSEED: 10, FOLD: 2, EPOCH: 21,train_loss: 0.006313288786812969, valid_loss: 0.02191230186394283\nSEED: 10, FOLD: 2, EPOCH: 22,train_loss: 0.005808443202218716, valid_loss: 0.02201137500149863\nSEED: 10, FOLD: 2, EPOCH: 23,train_loss: 0.005555208488974882, valid_loss: 0.02203961626759597\nSEED: 10, FOLD: 2, EPOCH: 24,train_loss: 0.005482743959873915, valid_loss: 0.022071321202175958\nFOLD: 3, EPOCH: 0,train_loss: 0.7319367700728817, valid_loss: 0.6907807963235038\nSEED: 10, FOLD: 3, EPOCH: 0,train_loss: 0.4688085345281423, valid_loss: 0.02339653064097677\nSEED: 10, FOLD: 3, EPOCH: 1,train_loss: 0.021627184327529823, valid_loss: 0.0204602879605123\nSEED: 10, FOLD: 3, EPOCH: 2,train_loss: 0.0202891419277243, valid_loss: 0.019516530366880554\nSEED: 10, FOLD: 3, EPOCH: 3,train_loss: 0.019079238365309826, valid_loss: 0.01887387436415468\nSEED: 10, FOLD: 3, EPOCH: 4,train_loss: 0.018404226581417563, valid_loss: 0.020629481332642693\nSEED: 10, FOLD: 3, EPOCH: 5,train_loss: 0.017850287002173885, valid_loss: 0.018155221215316226\nSEED: 10, FOLD: 3, EPOCH: 6,train_loss: 0.01745043059244104, valid_loss: 0.43348488568195276\nSEED: 10, FOLD: 3, EPOCH: 7,train_loss: 0.017160389194453972, valid_loss: 0.02646398033414568\nSEED: 10, FOLD: 3, EPOCH: 8,train_loss: 0.0170136078166357, valid_loss: 0.018150771568928446\nSEED: 10, FOLD: 3, EPOCH: 9,train_loss: 0.01687044059128865, valid_loss: 2.4200586458402022\nSEED: 10, FOLD: 3, EPOCH: 10,train_loss: 0.01674862064934079, valid_loss: 0.018243041687778065\nSEED: 10, FOLD: 3, EPOCH: 11,train_loss: 0.016553490611630074, valid_loss: 0.018111364490219526\nSEED: 10, FOLD: 3, EPOCH: 12,train_loss: 0.016352101168392794, valid_loss: 0.018130741960236004\nSEED: 10, FOLD: 3, EPOCH: 13,train_loss: 0.015984729573508535, valid_loss: 0.0256855167980705\nSEED: 10, FOLD: 3, EPOCH: 14,train_loss: 0.015425001831212337, valid_loss: 0.10815905875393322\nSEED: 10, FOLD: 3, EPOCH: 15,train_loss: 0.01480436440476257, valid_loss: 0.019209315414939608\nSEED: 10, FOLD: 3, EPOCH: 16,train_loss: 0.01390505079989848, valid_loss: 0.01950964980891773\nSEED: 10, FOLD: 3, EPOCH: 17,train_loss: 0.012728353645112635, valid_loss: 0.019813886071954456\nSEED: 10, FOLD: 3, EPOCH: 18,train_loss: 0.011325553106818941, valid_loss: 0.020214548973100526\nSEED: 10, FOLD: 3, EPOCH: 19,train_loss: 0.009695688519032969, valid_loss: 0.020900262892246245\nSEED: 10, FOLD: 3, EPOCH: 20,train_loss: 0.008236409040113938, valid_loss: 0.020850968999522074\nSEED: 10, FOLD: 3, EPOCH: 21,train_loss: 0.007094730913261141, valid_loss: 0.020965505923543657\nSEED: 10, FOLD: 3, EPOCH: 22,train_loss: 0.006427572105430822, valid_loss: 0.021064599816288266\nSEED: 10, FOLD: 3, EPOCH: 23,train_loss: 0.00607524064126546, valid_loss: 0.021132444377456393\nSEED: 10, FOLD: 3, EPOCH: 24,train_loss: 0.005962256929310767, valid_loss: 0.02113147665347372\nFOLD: 4, EPOCH: 0,train_loss: 0.7321140157139819, valid_loss: 0.6911177958760942\nSEED: 10, FOLD: 4, EPOCH: 0,train_loss: 0.468324627659783, valid_loss: 0.024455332489950316\nSEED: 10, FOLD: 4, EPOCH: 1,train_loss: 0.02160088173991096, valid_loss: 0.021138492758784974\nSEED: 10, FOLD: 4, EPOCH: 2,train_loss: 0.02023946057897115, valid_loss: 0.019099836024854863\nSEED: 10, FOLD: 4, EPOCH: 3,train_loss: 0.019048569359533165, valid_loss: 0.018473732258592335\nSEED: 10, FOLD: 4, EPOCH: 4,train_loss: 0.018208897551116737, valid_loss: 0.01808894273958036\nSEED: 10, FOLD: 4, EPOCH: 5,train_loss: 0.01773781651981931, valid_loss: 0.018386193364858628\nSEED: 10, FOLD: 4, EPOCH: 6,train_loss: 0.017316709569506886, valid_loss: 0.01798861250281334\nSEED: 10, FOLD: 4, EPOCH: 7,train_loss: 0.017110263172915016, valid_loss: 0.01885288522711822\nSEED: 10, FOLD: 4, EPOCH: 8,train_loss: 0.01698379803016998, valid_loss: 0.017783933877944948\nSEED: 10, FOLD: 4, EPOCH: 9,train_loss: 0.016803430339348488, valid_loss: 0.018097484138395105\nSEED: 10, FOLD: 4, EPOCH: 10,train_loss: 0.0166346221635847, valid_loss: 0.02844576734517302\nSEED: 10, FOLD: 4, EPOCH: 11,train_loss: 0.016492618254615343, valid_loss: 0.01818209146814687\nSEED: 10, FOLD: 4, EPOCH: 12,train_loss: 0.016253629031226687, valid_loss: 0.01813116084252085\nSEED: 10, FOLD: 4, EPOCH: 13,train_loss: 0.015834956895560026, valid_loss: 0.018730901128479412\nSEED: 10, FOLD: 4, EPOCH: 14,train_loss: 0.015341873321196308, valid_loss: 0.01860498703484024\nSEED: 10, FOLD: 4, EPOCH: 15,train_loss: 0.014611538270137447, valid_loss: 0.01920251159795693\nSEED: 10, FOLD: 4, EPOCH: 16,train_loss: 0.013746174119844816, valid_loss: 0.019631472575877395\nSEED: 10, FOLD: 4, EPOCH: 17,train_loss: 0.012473714578410854, valid_loss: 0.01983716796551432\nSEED: 10, FOLD: 4, EPOCH: 18,train_loss: 0.010908028920707495, valid_loss: 0.020260883229119436\nSEED: 10, FOLD: 4, EPOCH: 19,train_loss: 0.009318428019574587, valid_loss: 0.020998640943850788\nSEED: 10, FOLD: 4, EPOCH: 20,train_loss: 0.007903690851441976, valid_loss: 0.021143393431391034\nSEED: 10, FOLD: 4, EPOCH: 21,train_loss: 0.006812328358005354, valid_loss: 0.021049179030316216\nSEED: 10, FOLD: 4, EPOCH: 22,train_loss: 0.006193678120853028, valid_loss: 0.021160324769360677\nSEED: 10, FOLD: 4, EPOCH: 23,train_loss: 0.0058832317050816355, valid_loss: 0.021278483580265725\nSEED: 10, FOLD: 4, EPOCH: 24,train_loss: 0.005773693154417519, valid_loss: 0.021271526121667453\nFOLD: 0, EPOCH: 0,train_loss: 0.7314926541369894, valid_loss: 0.6931930933679853\nSEED: 11, FOLD: 0, EPOCH: 0,train_loss: 0.4706524756561587, valid_loss: 0.02495684927063329\nSEED: 11, FOLD: 0, EPOCH: 1,train_loss: 0.021829554514176605, valid_loss: 0.021823797853929655\nSEED: 11, FOLD: 0, EPOCH: 2,train_loss: 0.020731294676121594, valid_loss: 0.019580542721918653\nSEED: 11, FOLD: 0, EPOCH: 3,train_loss: 0.01940438757394103, valid_loss: 0.018793478395257676\nSEED: 11, FOLD: 0, EPOCH: 4,train_loss: 0.018628876318858154, valid_loss: 0.01817823794803449\nSEED: 11, FOLD: 0, EPOCH: 5,train_loss: 0.018126935232430696, valid_loss: 0.01841363784457956\nSEED: 11, FOLD: 0, EPOCH: 6,train_loss: 0.01776339214268154, valid_loss: 0.018040926887520722\nSEED: 11, FOLD: 0, EPOCH: 7,train_loss: 0.017450955028281263, valid_loss: 0.018209843151271343\nSEED: 11, FOLD: 0, EPOCH: 8,train_loss: 0.017238473492688026, valid_loss: 0.018214094558996814\nSEED: 11, FOLD: 0, EPOCH: 9,train_loss: 0.01699633561614631, valid_loss: 0.018553647824696134\nSEED: 11, FOLD: 0, EPOCH: 10,train_loss: 0.01695125977225278, valid_loss: 0.01798933869493859\nSEED: 11, FOLD: 0, EPOCH: 11,train_loss: 0.016745353340292753, valid_loss: 0.0486322744084256\nSEED: 11, FOLD: 0, EPOCH: 12,train_loss: 0.01651872180915181, valid_loss: 0.018415648117661478\nSEED: 11, FOLD: 0, EPOCH: 13,train_loss: 0.016260346485490816, valid_loss: 0.017830539867281912\nSEED: 11, FOLD: 0, EPOCH: 14,train_loss: 0.01576395048017519, valid_loss: 0.018646358512341976\nSEED: 11, FOLD: 0, EPOCH: 15,train_loss: 0.015108602911071934, valid_loss: 0.018500244644071375\nSEED: 11, FOLD: 0, EPOCH: 16,train_loss: 0.014457773396988277, valid_loss: 0.019361485806959015\nSEED: 11, FOLD: 0, EPOCH: 17,train_loss: 0.014054553448290064, valid_loss: 0.02006099516791957\nSEED: 11, FOLD: 0, EPOCH: 18,train_loss: 0.012410614922966646, valid_loss: 0.019580701552331447\nSEED: 11, FOLD: 0, EPOCH: 19,train_loss: 0.01100989728761108, valid_loss: 0.020138082333973475\nSEED: 11, FOLD: 0, EPOCH: 20,train_loss: 0.010585852217037176, valid_loss: 0.020335248644862858\nSEED: 11, FOLD: 0, EPOCH: 21,train_loss: 0.008915672328431105, valid_loss: 0.020352704663361823\nSEED: 11, FOLD: 0, EPOCH: 22,train_loss: 0.007808416700530527, valid_loss: 0.020473737642168997\nSEED: 11, FOLD: 0, EPOCH: 23,train_loss: 0.0072810541791166515, valid_loss: 0.020498821804566043\nSEED: 11, FOLD: 0, EPOCH: 24,train_loss: 0.007102840414702677, valid_loss: 0.020525741231228623\nFOLD: 1, EPOCH: 0,train_loss: 0.7315744239351024, valid_loss: 0.6948479567255292\nSEED: 11, FOLD: 1, EPOCH: 0,train_loss: 0.46921141065009264, valid_loss: 0.024114177003502844\nSEED: 11, FOLD: 1, EPOCH: 1,train_loss: 0.021762344986200333, valid_loss: 0.0220843819635255\nSEED: 11, FOLD: 1, EPOCH: 2,train_loss: 0.020464558208334274, valid_loss: 0.019885213992425373\nSEED: 11, FOLD: 1, EPOCH: 3,train_loss: 0.01914125630982976, valid_loss: 0.018410621796335493\nSEED: 11, FOLD: 1, EPOCH: 4,train_loss: 0.018362470801271822, valid_loss: 0.01865968523280961\nSEED: 11, FOLD: 1, EPOCH: 5,train_loss: 0.017838130529592003, valid_loss: 0.018541280126997402\nSEED: 11, FOLD: 1, EPOCH: 6,train_loss: 0.017267853912451992, valid_loss: 0.01850192765040057\nSEED: 11, FOLD: 1, EPOCH: 7,train_loss: 0.01697991023083096, valid_loss: 0.018500592479748387\nSEED: 11, FOLD: 1, EPOCH: 8,train_loss: 0.01662499330047032, valid_loss: 0.01863124668598175\nSEED: 11, FOLD: 1, EPOCH: 9,train_loss: 0.0163563236432231, valid_loss: 0.01864748549248491\nSEED: 11, FOLD: 1, EPOCH: 10,train_loss: 0.015999632131686245, valid_loss: 0.01935987195798329\nSEED: 11, FOLD: 1, EPOCH: 11,train_loss: 0.015498281753473524, valid_loss: 0.019095437441553387\nSEED: 11, FOLD: 1, EPOCH: 12,train_loss: 0.015180061618541029, valid_loss: 0.019661694977964675\nSEED: 11, FOLD: 1, EPOCH: 13,train_loss: 0.014907675012406231, valid_loss: 0.020099848881363867\nSEED: 11, FOLD: 1, EPOCH: 14,train_loss: 0.014639678647390741, valid_loss: 0.019535678412233082\nSEED: 11, FOLD: 1, EPOCH: 15,train_loss: 0.013026959647465012, valid_loss: 0.02010063332106386\nSEED: 11, FOLD: 1, EPOCH: 16,train_loss: 0.012223564101842003, valid_loss: 0.020755518759999956\nSEED: 11, FOLD: 1, EPOCH: 17,train_loss: 0.011307729185437378, valid_loss: 0.02082877531647682\nSEED: 11, FOLD: 1, EPOCH: 18,train_loss: 0.00985973211599217, valid_loss: 0.02118706288082259\nSEED: 11, FOLD: 1, EPOCH: 19,train_loss: 0.008393219209856528, valid_loss: 0.021142656675406865\nSEED: 11, FOLD: 1, EPOCH: 20,train_loss: 0.007392679914778126, valid_loss: 0.02184108623436519\nSEED: 11, FOLD: 1, EPOCH: 21,train_loss: 0.006696424720322956, valid_loss: 0.021372356159346444\nSEED: 11, FOLD: 1, EPOCH: 22,train_loss: 0.006209999852665309, valid_loss: 0.021418850336756026\nSEED: 11, FOLD: 1, EPOCH: 23,train_loss: 0.005900933731185353, valid_loss: 0.021499053122741835\nSEED: 11, FOLD: 1, EPOCH: 24,train_loss: 0.005806601849263129, valid_loss: 0.021446808001824788\nFOLD: 2, EPOCH: 0,train_loss: 0.7319128897742949, valid_loss: 0.6963848822257098\nSEED: 11, FOLD: 2, EPOCH: 0,train_loss: 0.46862319979708694, valid_loss: 0.0239302763605819\nSEED: 11, FOLD: 2, EPOCH: 1,train_loss: 0.021621101225415867, valid_loss: 0.021980435642249444\nSEED: 11, FOLD: 2, EPOCH: 2,train_loss: 0.02032525304272987, valid_loss: 0.0194168086437618\nSEED: 11, FOLD: 2, EPOCH: 3,train_loss: 0.019069671347413376, valid_loss: 0.018678602214683506\nSEED: 11, FOLD: 2, EPOCH: 4,train_loss: 0.01830015994230474, valid_loss: 0.02844576455433579\nSEED: 11, FOLD: 2, EPOCH: 5,train_loss: 0.0178546242793833, valid_loss: 0.01856432720909224\nSEED: 11, FOLD: 2, EPOCH: 6,train_loss: 0.01746280313860895, valid_loss: 0.495198299861787\nSEED: 11, FOLD: 2, EPOCH: 7,train_loss: 0.01715712090684236, valid_loss: 0.01862360567183179\nSEED: 11, FOLD: 2, EPOCH: 8,train_loss: 0.016977788016631985, valid_loss: 0.01846012007445097\nSEED: 11, FOLD: 2, EPOCH: 9,train_loss: 0.016819414163035326, valid_loss: 0.01818924695801209\nSEED: 11, FOLD: 2, EPOCH: 10,train_loss: 0.016584455312324175, valid_loss: 0.018698856782387283\nSEED: 11, FOLD: 2, EPOCH: 11,train_loss: 0.016451961367620505, valid_loss: 0.02058550121043535\nSEED: 11, FOLD: 2, EPOCH: 12,train_loss: 0.016110044472135494, valid_loss: 0.020492599236176294\nSEED: 11, FOLD: 2, EPOCH: 13,train_loss: 0.01579205502850422, valid_loss: 0.02288946509361267\nSEED: 11, FOLD: 2, EPOCH: 14,train_loss: 0.015288600448411013, valid_loss: 0.11874848642550848\nSEED: 11, FOLD: 2, EPOCH: 15,train_loss: 0.01467547821474896, valid_loss: 0.02520604689112481\nSEED: 11, FOLD: 2, EPOCH: 16,train_loss: 0.013693538919577131, valid_loss: 0.02247791868798873\nSEED: 11, FOLD: 2, EPOCH: 17,train_loss: 0.012605570560402197, valid_loss: 0.025844907935927895\nSEED: 11, FOLD: 2, EPOCH: 18,train_loss: 0.011128152545163597, valid_loss: 0.027123829075957045\nSEED: 11, FOLD: 2, EPOCH: 19,train_loss: 0.009586051599109087, valid_loss: 0.03335022564758273\nSEED: 11, FOLD: 2, EPOCH: 20,train_loss: 0.008166437017043, valid_loss: 0.037564365193247795\nSEED: 11, FOLD: 2, EPOCH: 21,train_loss: 0.007095896510704272, valid_loss: 0.021832010380047208\nSEED: 11, FOLD: 2, EPOCH: 22,train_loss: 0.006488522244752317, valid_loss: 0.02964391148484805\nSEED: 11, FOLD: 2, EPOCH: 23,train_loss: 0.006147302687168121, valid_loss: 0.02339845857418635\nSEED: 11, FOLD: 2, EPOCH: 24,train_loss: 0.006035933092209524, valid_loss: 0.023204988416503456\nFOLD: 3, EPOCH: 0,train_loss: 0.7309909419737001, valid_loss: 0.6962510909352984\nSEED: 11, FOLD: 3, EPOCH: 0,train_loss: 0.46912481037872855, valid_loss: 0.024347658455371856\nSEED: 11, FOLD: 3, EPOCH: 1,train_loss: 0.02177174478445364, valid_loss: 0.020938649347850256\nSEED: 11, FOLD: 3, EPOCH: 2,train_loss: 0.020517615196497543, valid_loss: 0.01916803632463728\nSEED: 11, FOLD: 3, EPOCH: 3,train_loss: 0.01919399012905964, valid_loss: 0.01856403648853302\nSEED: 11, FOLD: 3, EPOCH: 4,train_loss: 0.018320344435725954, valid_loss: 0.018377877399325372\nSEED: 11, FOLD: 3, EPOCH: 5,train_loss: 0.017788806417282078, valid_loss: 0.017959622932331904\nSEED: 11, FOLD: 3, EPOCH: 6,train_loss: 0.0174306930663685, valid_loss: 0.018989792306508338\nSEED: 11, FOLD: 3, EPOCH: 7,train_loss: 0.01713274781713667, valid_loss: 0.01833950366292681\nSEED: 11, FOLD: 3, EPOCH: 8,train_loss: 0.016907623867787745, valid_loss: 0.017988414157714162\nSEED: 11, FOLD: 3, EPOCH: 9,train_loss: 0.01673373801336772, valid_loss: 0.21550288487757954\nSEED: 11, FOLD: 3, EPOCH: 10,train_loss: 0.016504244177021843, valid_loss: 0.021804438584617208\nSEED: 11, FOLD: 3, EPOCH: 11,train_loss: 0.016362844149757555, valid_loss: 0.01906729021242687\nSEED: 11, FOLD: 3, EPOCH: 12,train_loss: 0.0161721572023479, valid_loss: 0.019118700070040566\nSEED: 11, FOLD: 3, EPOCH: 13,train_loss: 0.015617717023722935, valid_loss: 0.01925498385514532\nSEED: 11, FOLD: 3, EPOCH: 14,train_loss: 0.015193819743243681, valid_loss: 0.019276535883545876\nSEED: 11, FOLD: 3, EPOCH: 15,train_loss: 0.01460828663835275, valid_loss: 0.019416259761367526\nSEED: 11, FOLD: 3, EPOCH: 16,train_loss: 0.013563830931873425, valid_loss: 0.020056778618267605\nSEED: 11, FOLD: 3, EPOCH: 17,train_loss: 0.012329177694746118, valid_loss: 0.020337921806744166\nSEED: 11, FOLD: 3, EPOCH: 18,train_loss: 0.010773581074739712, valid_loss: 0.02360953747161797\nSEED: 11, FOLD: 3, EPOCH: 19,train_loss: 0.009422751750959002, valid_loss: 0.020695132602538382\nSEED: 11, FOLD: 3, EPOCH: 20,train_loss: 0.008052381889327713, valid_loss: 0.021186248000179018\nSEED: 11, FOLD: 3, EPOCH: 21,train_loss: 0.007043152389562, valid_loss: 0.021346576405423028\nSEED: 11, FOLD: 3, EPOCH: 22,train_loss: 0.006385153763509099, valid_loss: 0.021344661552991185\nSEED: 11, FOLD: 3, EPOCH: 23,train_loss: 0.0060407807713077554, valid_loss: 0.021444704277174813\nSEED: 11, FOLD: 3, EPOCH: 24,train_loss: 0.005918278136169133, valid_loss: 0.021442856001002448\nFOLD: 4, EPOCH: 0,train_loss: 0.7313629771671156, valid_loss: 0.6943902577672686\nSEED: 11, FOLD: 4, EPOCH: 0,train_loss: 0.470224875616875, valid_loss: 0.02357586297605719\nSEED: 11, FOLD: 4, EPOCH: 1,train_loss: 0.021893203204130605, valid_loss: 0.020306424157960076\nSEED: 11, FOLD: 4, EPOCH: 2,train_loss: 0.020527410055816608, valid_loss: 0.01882466253425394\nSEED: 11, FOLD: 4, EPOCH: 3,train_loss: 0.019061375837637125, valid_loss: 0.01943404445690768\nSEED: 11, FOLD: 4, EPOCH: 4,train_loss: 0.018330008362549065, valid_loss: 0.018355248123407365\nSEED: 11, FOLD: 4, EPOCH: 5,train_loss: 0.017869631984155542, valid_loss: 0.018062767386436463\nSEED: 11, FOLD: 4, EPOCH: 6,train_loss: 0.01741906318704795, valid_loss: 0.017880324061427796\nSEED: 11, FOLD: 4, EPOCH: 7,train_loss: 0.01712590793188471, valid_loss: 0.017829332128167154\nSEED: 11, FOLD: 4, EPOCH: 8,train_loss: 0.017013921069293995, valid_loss: 0.017709341725068432\nSEED: 11, FOLD: 4, EPOCH: 9,train_loss: 0.01688764277628086, valid_loss: 0.01768295615911484\nSEED: 11, FOLD: 4, EPOCH: 10,train_loss: 0.016727345540140666, valid_loss: 0.018178958179695264\nSEED: 11, FOLD: 4, EPOCH: 11,train_loss: 0.016594281807596232, valid_loss: 0.017816824545817716\nSEED: 11, FOLD: 4, EPOCH: 12,train_loss: 0.01642151557616074, valid_loss: 0.05902054735592433\nSEED: 11, FOLD: 4, EPOCH: 13,train_loss: 0.016181760352023327, valid_loss: 0.018209391246948925\nSEED: 11, FOLD: 4, EPOCH: 14,train_loss: 0.015766888530585017, valid_loss: 0.018410719824688775\nSEED: 11, FOLD: 4, EPOCH: 15,train_loss: 0.015146569777144132, valid_loss: 0.01880773811468056\nSEED: 11, FOLD: 4, EPOCH: 16,train_loss: 0.014383290296107748, valid_loss: 0.018876465888960022\nSEED: 11, FOLD: 4, EPOCH: 17,train_loss: 0.013208195675898642, valid_loss: 0.019725297125322477\nSEED: 11, FOLD: 4, EPOCH: 18,train_loss: 0.012002217760105637, valid_loss: 0.01993146017193794\nSEED: 11, FOLD: 4, EPOCH: 19,train_loss: 0.010506587646847223, valid_loss: 0.30518128047032017\nSEED: 11, FOLD: 4, EPOCH: 20,train_loss: 0.008950877808931753, valid_loss: 0.021174957656434606\nSEED: 11, FOLD: 4, EPOCH: 21,train_loss: 0.007701531905735279, valid_loss: 0.020842224253075463\nSEED: 11, FOLD: 4, EPOCH: 22,train_loss: 0.006889098908507476, valid_loss: 0.020678567300949777\nSEED: 11, FOLD: 4, EPOCH: 23,train_loss: 0.006495697539381303, valid_loss: 0.02079511529632977\nSEED: 11, FOLD: 4, EPOCH: 24,train_loss: 0.006332666742323089, valid_loss: 0.020788754256708283\nFOLD: 0, EPOCH: 0,train_loss: 0.7322802776875703, valid_loss: 0.6935793399810791\nSEED: 12, FOLD: 0, EPOCH: 0,train_loss: 0.4682974064641673, valid_loss: 0.023480604961514474\nSEED: 12, FOLD: 0, EPOCH: 1,train_loss: 0.02163553529459497, valid_loss: 0.020845052919217517\nSEED: 12, FOLD: 0, EPOCH: 2,train_loss: 0.020341526136558125, valid_loss: 0.01947694793343544\nSEED: 12, FOLD: 0, EPOCH: 3,train_loss: 0.018905775135625965, valid_loss: 20.187148710606355\nSEED: 12, FOLD: 0, EPOCH: 4,train_loss: 0.018179659279522257, valid_loss: 0.5180904499654259\nSEED: 12, FOLD: 0, EPOCH: 5,train_loss: 0.01772138833378752, valid_loss: 1.664369795737522\nSEED: 12, FOLD: 0, EPOCH: 6,train_loss: 0.017452083676513554, valid_loss: 0.01847640382392066\nSEED: 12, FOLD: 0, EPOCH: 7,train_loss: 0.0171678916119255, valid_loss: 0.536290184860783\nSEED: 12, FOLD: 0, EPOCH: 8,train_loss: 0.01694669387559744, valid_loss: 1.3577954460467612\nSEED: 12, FOLD: 0, EPOCH: 9,train_loss: 0.01672196222226257, valid_loss: 1.4641285169869662\nSEED: 12, FOLD: 0, EPOCH: 10,train_loss: 0.01655659257042883, valid_loss: 28.329900375806858\nSEED: 12, FOLD: 0, EPOCH: 11,train_loss: 0.01637662265557742, valid_loss: 0.01856716897870813\nSEED: 12, FOLD: 0, EPOCH: 12,train_loss: 0.0159970052216364, valid_loss: 0.29004400574735234\nSEED: 12, FOLD: 0, EPOCH: 13,train_loss: 0.01563894159524985, valid_loss: 0.018726414442062377\nSEED: 12, FOLD: 0, EPOCH: 14,train_loss: 0.015019269693858814, valid_loss: 0.019070765216435706\nSEED: 12, FOLD: 0, EPOCH: 15,train_loss: 0.014344342815541271, valid_loss: 0.019162624861512864\nSEED: 12, FOLD: 0, EPOCH: 16,train_loss: 0.013412405709749546, valid_loss: 0.01982741773660694\nSEED: 12, FOLD: 0, EPOCH: 17,train_loss: 0.012127889846654041, valid_loss: 0.02443580281521593\nSEED: 12, FOLD: 0, EPOCH: 18,train_loss: 0.010715804359727148, valid_loss: 0.021677940604942186\nSEED: 12, FOLD: 0, EPOCH: 19,train_loss: 0.009191560299149241, valid_loss: 0.02197038744177137\nSEED: 12, FOLD: 0, EPOCH: 20,train_loss: 0.007757027174575605, valid_loss: 0.022044757806829043\nSEED: 12, FOLD: 0, EPOCH: 21,train_loss: 0.0067270771208880606, valid_loss: 0.02185909492628915\nSEED: 12, FOLD: 0, EPOCH: 22,train_loss: 0.0061130732216912766, valid_loss: 0.021889898127743175\nSEED: 12, FOLD: 0, EPOCH: 23,train_loss: 0.005792974001741496, valid_loss: 0.021929210584078516\nSEED: 12, FOLD: 0, EPOCH: 24,train_loss: 0.005689191770321433, valid_loss: 0.02194160707294941\nFOLD: 1, EPOCH: 0,train_loss: 0.732361455326495, valid_loss: 0.6946450559531941\nSEED: 12, FOLD: 1, EPOCH: 0,train_loss: 0.4677651445178882, valid_loss: 0.024154821958611992\nSEED: 12, FOLD: 1, EPOCH: 1,train_loss: 0.02179602628060873, valid_loss: 0.020565470187541318\nSEED: 12, FOLD: 1, EPOCH: 2,train_loss: 0.02018133565729511, valid_loss: 0.018934130668640137\nSEED: 12, FOLD: 1, EPOCH: 3,train_loss: 0.018930558047756767, valid_loss: 0.2485311745720751\nSEED: 12, FOLD: 1, EPOCH: 4,train_loss: 0.01809857837666852, valid_loss: 0.01811438355156604\nSEED: 12, FOLD: 1, EPOCH: 5,train_loss: 0.0175492438973616, valid_loss: 0.017938441925627345\nSEED: 12, FOLD: 1, EPOCH: 6,train_loss: 0.017227176401386227, valid_loss: 0.018352012916961136\nSEED: 12, FOLD: 1, EPOCH: 7,train_loss: 0.017024222720900307, valid_loss: 0.018164310604333878\nSEED: 12, FOLD: 1, EPOCH: 8,train_loss: 0.016900107008067593, valid_loss: 0.01796621916925206\nSEED: 12, FOLD: 1, EPOCH: 9,train_loss: 0.01675535105438768, valid_loss: 0.018356933751527\nSEED: 12, FOLD: 1, EPOCH: 10,train_loss: 0.01658344471935129, valid_loss: 0.08381352989989169\nSEED: 12, FOLD: 1, EPOCH: 11,train_loss: 0.01642724724513465, valid_loss: 5.08943894330193\nSEED: 12, FOLD: 1, EPOCH: 12,train_loss: 0.01624340303512155, valid_loss: 0.018114901312133846\nSEED: 12, FOLD: 1, EPOCH: 13,train_loss: 0.015751089077388893, valid_loss: 0.02145543464404695\nSEED: 12, FOLD: 1, EPOCH: 14,train_loss: 0.01532824825414497, valid_loss: 0.01869167042348315\nSEED: 12, FOLD: 1, EPOCH: 15,train_loss: 0.014553074772213247, valid_loss: 0.01908767278141835\nSEED: 12, FOLD: 1, EPOCH: 16,train_loss: 0.01357010080465588, valid_loss: 0.01926856811213143\nSEED: 12, FOLD: 1, EPOCH: 17,train_loss: 0.012278123036620842, valid_loss: 0.019951466470956802\nSEED: 12, FOLD: 1, EPOCH: 18,train_loss: 0.010829970618520958, valid_loss: 0.020655900623430225\nSEED: 12, FOLD: 1, EPOCH: 19,train_loss: 0.009212711873207834, valid_loss: 0.02106564207112088\nSEED: 12, FOLD: 1, EPOCH: 20,train_loss: 0.007809925915511406, valid_loss: 0.02131648706820081\nSEED: 12, FOLD: 1, EPOCH: 21,train_loss: 0.006753960374634767, valid_loss: 0.021255393512547016\nSEED: 12, FOLD: 1, EPOCH: 22,train_loss: 0.006144606734396539, valid_loss: 0.02130179660504355\nSEED: 12, FOLD: 1, EPOCH: 23,train_loss: 0.005834838517172181, valid_loss: 0.021378277417491463\nSEED: 12, FOLD: 1, EPOCH: 24,train_loss: 0.005724574088294437, valid_loss: 0.021384089034708106\nFOLD: 2, EPOCH: 0,train_loss: 0.7324609230034542, valid_loss: 0.6963349308286394\nSEED: 12, FOLD: 2, EPOCH: 0,train_loss: 0.4686042221983636, valid_loss: 0.02447452252464635\nSEED: 12, FOLD: 2, EPOCH: 1,train_loss: 0.02177647086553765, valid_loss: 0.0214470735085862\nSEED: 12, FOLD: 2, EPOCH: 2,train_loss: 0.02038404325118465, valid_loss: 0.019152081065944262\nSEED: 12, FOLD: 2, EPOCH: 3,train_loss: 0.019040395458140513, valid_loss: 0.7243985431534904\nSEED: 12, FOLD: 2, EPOCH: 4,train_loss: 0.01828189157493358, valid_loss: 0.01846380095396723\nSEED: 12, FOLD: 2, EPOCH: 5,train_loss: 0.017769466507772023, valid_loss: 0.01806612562920366\nSEED: 12, FOLD: 2, EPOCH: 6,train_loss: 0.01730285957455635, valid_loss: 0.018168595805764198\nSEED: 12, FOLD: 2, EPOCH: 7,train_loss: 0.017073527155240086, valid_loss: 0.01788580119609833\nSEED: 12, FOLD: 2, EPOCH: 8,train_loss: 0.016812019621151208, valid_loss: 0.018009973370603154\nSEED: 12, FOLD: 2, EPOCH: 9,train_loss: 0.01661318308082375, valid_loss: 0.018168369946735246\nSEED: 12, FOLD: 2, EPOCH: 10,train_loss: 0.01645560811416511, valid_loss: 0.01899857691356114\nSEED: 12, FOLD: 2, EPOCH: 11,train_loss: 0.01625263248400314, valid_loss: 0.018693881322230613\nSEED: 12, FOLD: 2, EPOCH: 12,train_loss: 0.015833383630009464, valid_loss: 0.018394620290824344\nSEED: 12, FOLD: 2, EPOCH: 13,train_loss: 0.015376210885707044, valid_loss: 0.019286624821169034\nSEED: 12, FOLD: 2, EPOCH: 14,train_loss: 0.014687530533240659, valid_loss: 0.01924128931547914\nSEED: 12, FOLD: 2, EPOCH: 15,train_loss: 0.013810347582138803, valid_loss: 0.02970977281885488\nSEED: 12, FOLD: 2, EPOCH: 16,train_loss: 0.0127810501377948, valid_loss: 0.020232919922896792\nSEED: 12, FOLD: 2, EPOCH: 17,train_loss: 0.011425249604848179, valid_loss: 0.020905664616397448\nSEED: 12, FOLD: 2, EPOCH: 18,train_loss: 0.009978649947438797, valid_loss: 0.020956489337342125\nSEED: 12, FOLD: 2, EPOCH: 19,train_loss: 0.008430428000806022, valid_loss: 0.021639488477792058\nSEED: 12, FOLD: 2, EPOCH: 20,train_loss: 0.007184129589012939, valid_loss: 0.02160805282848222\nSEED: 12, FOLD: 2, EPOCH: 21,train_loss: 0.006371409717240256, valid_loss: 0.02161140718630382\nSEED: 12, FOLD: 2, EPOCH: 22,train_loss: 0.005886481738356996, valid_loss: 0.02185679099389485\nSEED: 12, FOLD: 2, EPOCH: 23,train_loss: 0.005645114663362938, valid_loss: 0.02185200218643461\nSEED: 12, FOLD: 2, EPOCH: 24,train_loss: 0.005560232027277459, valid_loss: 0.02183754507984434\nFOLD: 3, EPOCH: 0,train_loss: 0.7320226366105287, valid_loss: 0.6921928191886229\nSEED: 12, FOLD: 3, EPOCH: 0,train_loss: 0.46779889381234196, valid_loss: 0.02396924848503926\nSEED: 12, FOLD: 3, EPOCH: 1,train_loss: 0.021755879987841068, valid_loss: 0.02062623142538702\nSEED: 12, FOLD: 3, EPOCH: 2,train_loss: 0.020378362387418747, valid_loss: 0.01871517607394387\nSEED: 12, FOLD: 3, EPOCH: 3,train_loss: 0.019209109057766804, valid_loss: 0.018129713833332062\nSEED: 12, FOLD: 3, EPOCH: 4,train_loss: 0.01850699673852195, valid_loss: 0.017774180536541867\nSEED: 12, FOLD: 3, EPOCH: 5,train_loss: 0.01807197744188749, valid_loss: 1.5103609160081868\nSEED: 12, FOLD: 3, EPOCH: 6,train_loss: 0.01770140294768456, valid_loss: 0.017780549139441812\nSEED: 12, FOLD: 3, EPOCH: 7,train_loss: 0.017399565384223842, valid_loss: 0.017637729946085635\nSEED: 12, FOLD: 3, EPOCH: 8,train_loss: 0.017143068787898275, valid_loss: 0.017814058563945925\nSEED: 12, FOLD: 3, EPOCH: 9,train_loss: 0.016991642391498106, valid_loss: 0.01786034000927911\nSEED: 12, FOLD: 3, EPOCH: 10,train_loss: 0.016792950409370056, valid_loss: 0.017755042734172416\nSEED: 12, FOLD: 3, EPOCH: 11,train_loss: 0.01658972431703106, valid_loss: 0.018196914098499453\nSEED: 12, FOLD: 3, EPOCH: 12,train_loss: 0.016354084656020437, valid_loss: 0.017951901974704337\nSEED: 12, FOLD: 3, EPOCH: 13,train_loss: 0.01599757249156634, valid_loss: 0.018295290222501055\nSEED: 12, FOLD: 3, EPOCH: 14,train_loss: 0.015562841284048298, valid_loss: 0.036350278430344427\nSEED: 12, FOLD: 3, EPOCH: 15,train_loss: 0.014895707551065994, valid_loss: 0.01863302898538463\nSEED: 12, FOLD: 3, EPOCH: 16,train_loss: 0.013988587121222761, valid_loss: 0.019739267964135197\nSEED: 12, FOLD: 3, EPOCH: 17,train_loss: 0.013055951759705076, valid_loss: 0.01957297927754767\nSEED: 12, FOLD: 3, EPOCH: 18,train_loss: 0.011590054299196472, valid_loss: 0.02018018954378717\nSEED: 12, FOLD: 3, EPOCH: 19,train_loss: 0.010092666669600252, valid_loss: 0.02066900033284636\nSEED: 12, FOLD: 3, EPOCH: 20,train_loss: 0.008592684125608725, valid_loss: 0.020641235086847756\nSEED: 12, FOLD: 3, EPOCH: 21,train_loss: 0.007395321206338163, valid_loss: 0.020641846910995597\nSEED: 12, FOLD: 3, EPOCH: 22,train_loss: 0.006634095579088814, valid_loss: 0.020712270620552933\nSEED: 12, FOLD: 3, EPOCH: 23,train_loss: 0.0062719638648348446, valid_loss: 0.020732754731879514\nSEED: 12, FOLD: 3, EPOCH: 24,train_loss: 0.006122411248962517, valid_loss: 0.020729624721057275\nFOLD: 4, EPOCH: 0,train_loss: 0.7322377349338393, valid_loss: 0.6937794770513263\nSEED: 12, FOLD: 4, EPOCH: 0,train_loss: 0.46930923273474195, valid_loss: 0.02761722542345524\nSEED: 12, FOLD: 4, EPOCH: 1,train_loss: 0.02161625019498985, valid_loss: 0.020949511655739377\nSEED: 12, FOLD: 4, EPOCH: 2,train_loss: 0.019968162515085108, valid_loss: 0.018914590988840376\nSEED: 12, FOLD: 4, EPOCH: 3,train_loss: 0.01887730458737725, valid_loss: 0.018930382068668095\nSEED: 12, FOLD: 4, EPOCH: 4,train_loss: 0.018191211122720347, valid_loss: 0.018375963558043752\nSEED: 12, FOLD: 4, EPOCH: 5,train_loss: 0.017664668877629467, valid_loss: 0.018254486950380462\nSEED: 12, FOLD: 4, EPOCH: 6,train_loss: 0.017318452416110214, valid_loss: 0.018350287792938097\nSEED: 12, FOLD: 4, EPOCH: 7,train_loss: 0.01707741210957731, valid_loss: 0.018421521224081517\nSEED: 12, FOLD: 4, EPOCH: 8,train_loss: 0.016808253094336412, valid_loss: 0.018437035807541437\nSEED: 12, FOLD: 4, EPOCH: 9,train_loss: 0.016611067449035, valid_loss: 0.018432215707642693\nSEED: 12, FOLD: 4, EPOCH: 10,train_loss: 0.01640802351282026, valid_loss: 0.018463919231934207\nSEED: 12, FOLD: 4, EPOCH: 11,train_loss: 0.01622847785806134, valid_loss: 0.01846979231174503\nSEED: 12, FOLD: 4, EPOCH: 12,train_loss: 0.01579591996260803, valid_loss: 0.01855891676885741\nSEED: 12, FOLD: 4, EPOCH: 13,train_loss: 0.015399394203385297, valid_loss: 0.019069398513862065\nSEED: 12, FOLD: 4, EPOCH: 14,train_loss: 0.014833192708120294, valid_loss: 0.01979979851416179\nSEED: 12, FOLD: 4, EPOCH: 15,train_loss: 0.01403734711318338, valid_loss: 0.01946334820240736\nSEED: 12, FOLD: 4, EPOCH: 16,train_loss: 0.013127187613642564, valid_loss: 0.0202020955405065\nSEED: 12, FOLD: 4, EPOCH: 17,train_loss: 0.011868099009033538, valid_loss: 0.020368031625236784\nSEED: 12, FOLD: 4, EPOCH: 18,train_loss: 0.010590603119646111, valid_loss: 0.021070461560572896\nSEED: 12, FOLD: 4, EPOCH: 19,train_loss: 0.009094492321575645, valid_loss: 0.021270112799746648\nSEED: 12, FOLD: 4, EPOCH: 20,train_loss: 0.007848007716646377, valid_loss: 0.02151383337165628\nSEED: 12, FOLD: 4, EPOCH: 21,train_loss: 0.006841976477689769, valid_loss: 0.021390296039836748\nSEED: 12, FOLD: 4, EPOCH: 22,train_loss: 0.006242529281761742, valid_loss: 0.021690383926033972\nSEED: 12, FOLD: 4, EPOCH: 23,train_loss: 0.005966531928547107, valid_loss: 0.021741511193769318\nSEED: 12, FOLD: 4, EPOCH: 24,train_loss: 0.005858231550014584, valid_loss: 0.021779217677456993\nFOLD: 0, EPOCH: 0,train_loss: 0.7341503652109616, valid_loss: 0.6916083523205349\nSEED: 13, FOLD: 0, EPOCH: 0,train_loss: 0.46885210761557455, valid_loss: 0.023505920065300806\nSEED: 13, FOLD: 0, EPOCH: 1,train_loss: 0.021554201530913513, valid_loss: 0.15526099242269992\nSEED: 13, FOLD: 0, EPOCH: 2,train_loss: 0.02012445611636276, valid_loss: 1.4511571609015976\nSEED: 13, FOLD: 0, EPOCH: 3,train_loss: 0.018967209208378757, valid_loss: 0.018626691720315388\nSEED: 13, FOLD: 0, EPOCH: 4,train_loss: 0.018115397880150787, valid_loss: 0.01798219970826592\nSEED: 13, FOLD: 0, EPOCH: 5,train_loss: 0.017633641354631687, valid_loss: 2.9622970189899207\nSEED: 13, FOLD: 0, EPOCH: 6,train_loss: 0.017409014508830034, valid_loss: 0.018551816871123655\nSEED: 13, FOLD: 0, EPOCH: 7,train_loss: 0.017097235832741295, valid_loss: 0.03068288891975369\nSEED: 13, FOLD: 0, EPOCH: 8,train_loss: 0.018110508983279917, valid_loss: 0.01779789961874485\nSEED: 13, FOLD: 0, EPOCH: 9,train_loss: 0.017561957246853388, valid_loss: 0.01795846125377076\nSEED: 13, FOLD: 0, EPOCH: 10,train_loss: 0.01748919625348155, valid_loss: 0.018344249203801156\nSEED: 13, FOLD: 0, EPOCH: 11,train_loss: 0.01730051024587474, valid_loss: 0.5849176131721053\nSEED: 13, FOLD: 0, EPOCH: 12,train_loss: 0.01710341529974687, valid_loss: 0.017897570292864526\nSEED: 13, FOLD: 0, EPOCH: 13,train_loss: 0.016758198080503422, valid_loss: 0.047156026560281006\nSEED: 13, FOLD: 0, EPOCH: 14,train_loss: 0.016614758504041725, valid_loss: 0.01813424200351749\nSEED: 13, FOLD: 0, EPOCH: 15,train_loss: 0.016381159795961088, valid_loss: 0.017992925404437952\nSEED: 13, FOLD: 0, EPOCH: 16,train_loss: 0.01574033269982623, valid_loss: 0.01907834251012121\nSEED: 13, FOLD: 0, EPOCH: 17,train_loss: 0.015146777431979992, valid_loss: 0.018434178616319383\nSEED: 13, FOLD: 0, EPOCH: 18,train_loss: 0.013975373233088118, valid_loss: 0.04917936926441533\nSEED: 13, FOLD: 0, EPOCH: 19,train_loss: 0.012644603376047335, valid_loss: 0.01993991088654314\nSEED: 13, FOLD: 0, EPOCH: 20,train_loss: 0.011048867448192575, valid_loss: 0.020860273843365057\nSEED: 13, FOLD: 0, EPOCH: 21,train_loss: 0.009433431962531979, valid_loss: 0.020371157995292117\nSEED: 13, FOLD: 0, EPOCH: 22,train_loss: 0.00819025376636157, valid_loss: 0.020622178273541587\nSEED: 13, FOLD: 0, EPOCH: 23,train_loss: 0.007504066580609567, valid_loss: 0.020447854857359615\nSEED: 13, FOLD: 0, EPOCH: 24,train_loss: 0.007259132482273423, valid_loss: 0.02070348635315895\nFOLD: 1, EPOCH: 0,train_loss: 0.7342945410721544, valid_loss: 0.6927185339086196\nSEED: 13, FOLD: 1, EPOCH: 0,train_loss: 0.46835445986547763, valid_loss: 0.024526345598347047\nSEED: 13, FOLD: 1, EPOCH: 1,train_loss: 0.02174123290224352, valid_loss: 0.02094060182571411\nSEED: 13, FOLD: 1, EPOCH: 2,train_loss: 0.02038646722887305, valid_loss: 0.01928750704973936\nSEED: 13, FOLD: 1, EPOCH: 3,train_loss: 0.019088329693329506, valid_loss: 0.018674702438361505\nSEED: 13, FOLD: 1, EPOCH: 4,train_loss: 0.018238107518603403, valid_loss: 0.032485337106182295\nSEED: 13, FOLD: 1, EPOCH: 5,train_loss: 0.01783421880606076, valid_loss: 0.018234377562561455\nSEED: 13, FOLD: 1, EPOCH: 6,train_loss: 0.017475016246401312, valid_loss: 0.01804631597855512\nSEED: 13, FOLD: 1, EPOCH: 7,train_loss: 0.017639227987577517, valid_loss: 0.018410709083956832\nSEED: 13, FOLD: 1, EPOCH: 8,train_loss: 0.017336608133400263, valid_loss: 0.018202760695096326\nSEED: 13, FOLD: 1, EPOCH: 9,train_loss: 0.017087800677973723, valid_loss: 0.017954741385491454\nSEED: 13, FOLD: 1, EPOCH: 10,train_loss: 0.01695408693690231, valid_loss: 0.018410378683577564\nSEED: 13, FOLD: 1, EPOCH: 11,train_loss: 0.016749910909034636, valid_loss: 0.01813870991634972\nSEED: 13, FOLD: 1, EPOCH: 12,train_loss: 0.016471877930334944, valid_loss: 0.017998157397789115\nSEED: 13, FOLD: 1, EPOCH: 13,train_loss: 0.016187918554667547, valid_loss: 0.018362691954654807\nSEED: 13, FOLD: 1, EPOCH: 14,train_loss: 0.01580174609451838, valid_loss: 0.018484985784572715\nSEED: 13, FOLD: 1, EPOCH: 15,train_loss: 0.015219228983303343, valid_loss: 0.01872222317273126\nSEED: 13, FOLD: 1, EPOCH: 16,train_loss: 0.014362774694851343, valid_loss: 0.019025766564642683\nSEED: 13, FOLD: 1, EPOCH: 17,train_loss: 0.01323544759762244, valid_loss: 0.01977497136549038\nSEED: 13, FOLD: 1, EPOCH: 18,train_loss: 0.011865536068174719, valid_loss: 0.020268536501509303\nSEED: 13, FOLD: 1, EPOCH: 19,train_loss: 0.010161618202708769, valid_loss: 0.02070902830318493\nSEED: 13, FOLD: 1, EPOCH: 20,train_loss: 0.008541513443587051, valid_loss: 0.021132091553333926\nSEED: 13, FOLD: 1, EPOCH: 21,train_loss: 0.007329892971134488, valid_loss: 0.02125759214601096\nSEED: 13, FOLD: 1, EPOCH: 22,train_loss: 0.006561709969452973, valid_loss: 0.02115940028691993\nSEED: 13, FOLD: 1, EPOCH: 23,train_loss: 0.00616900318140245, valid_loss: 0.0212692123673418\nSEED: 13, FOLD: 1, EPOCH: 24,train_loss: 0.006048765958057365, valid_loss: 0.021318009561475587\nFOLD: 2, EPOCH: 0,train_loss: 0.733751959990764, valid_loss: 0.6894703320094517\nSEED: 13, FOLD: 2, EPOCH: 0,train_loss: 0.46809004499590484, valid_loss: 0.02481204659811088\nSEED: 13, FOLD: 2, EPOCH: 1,train_loss: 0.02164284006247054, valid_loss: 0.02100919442517417\nSEED: 13, FOLD: 2, EPOCH: 2,train_loss: 0.02011655761009973, valid_loss: 0.019134079558508738\nSEED: 13, FOLD: 2, EPOCH: 3,train_loss: 0.018875902376907026, valid_loss: 0.01851346471479961\nSEED: 13, FOLD: 2, EPOCH: 4,train_loss: 0.01809759401594815, valid_loss: 0.017920491046139173\nSEED: 13, FOLD: 2, EPOCH: 5,train_loss: 0.017553042795887028, valid_loss: 0.018082676189286367\nSEED: 13, FOLD: 2, EPOCH: 6,train_loss: 0.01712523495265539, valid_loss: 0.0180830289210592\nSEED: 13, FOLD: 2, EPOCH: 7,train_loss: 0.01687024560743484, valid_loss: 0.01831689854817731\nSEED: 13, FOLD: 2, EPOCH: 8,train_loss: 0.01658146534843937, valid_loss: 0.0182241417201502\nSEED: 13, FOLD: 2, EPOCH: 9,train_loss: 0.016466496748498816, valid_loss: 0.017907651540424143\nSEED: 13, FOLD: 2, EPOCH: 10,train_loss: 0.016117024957539812, valid_loss: 0.017948771880141327\nSEED: 13, FOLD: 2, EPOCH: 11,train_loss: 0.015980293569357498, valid_loss: 0.01824135101799454\nSEED: 13, FOLD: 2, EPOCH: 12,train_loss: 0.015554122734760893, valid_loss: 0.01906460683260645\nSEED: 13, FOLD: 2, EPOCH: 13,train_loss: 0.015015564858913422, valid_loss: 0.018639144061931543\nSEED: 13, FOLD: 2, EPOCH: 14,train_loss: 0.014267351316369099, valid_loss: 0.01943181473761797\nSEED: 13, FOLD: 2, EPOCH: 15,train_loss: 0.013511167231785215, valid_loss: 0.019657184848827974\nSEED: 13, FOLD: 2, EPOCH: 16,train_loss: 0.012392505281267391, valid_loss: 0.020907360714461122\nSEED: 13, FOLD: 2, EPOCH: 17,train_loss: 0.011073326644744131, valid_loss: 0.020829983640994344\nSEED: 13, FOLD: 2, EPOCH: 18,train_loss: 0.009504645549948666, valid_loss: 0.02179013176688126\nSEED: 13, FOLD: 2, EPOCH: 19,train_loss: 0.008118488822483283, valid_loss: 0.021547336424035685\nSEED: 13, FOLD: 2, EPOCH: 20,train_loss: 0.006905387071352722, valid_loss: 0.021850745885499887\nSEED: 13, FOLD: 2, EPOCH: 21,train_loss: 0.006115860194372742, valid_loss: 0.0218600903238569\nSEED: 13, FOLD: 2, EPOCH: 22,train_loss: 0.00566494036330909, valid_loss: 0.022031273879110812\nSEED: 13, FOLD: 2, EPOCH: 23,train_loss: 0.005458411176864436, valid_loss: 0.022011898537831646\nSEED: 13, FOLD: 2, EPOCH: 24,train_loss: 0.00537423442279839, valid_loss: 0.022082758694887163\nFOLD: 3, EPOCH: 0,train_loss: 0.7344892172917833, valid_loss: 0.6947472504207066\nSEED: 13, FOLD: 3, EPOCH: 0,train_loss: 0.4696534418494162, valid_loss: 0.023055298307112285\nSEED: 13, FOLD: 3, EPOCH: 1,train_loss: 0.021829770687614044, valid_loss: 0.020155106538108418\nSEED: 13, FOLD: 3, EPOCH: 2,train_loss: 0.020394011203498735, valid_loss: 0.018506987605776105\nSEED: 13, FOLD: 3, EPOCH: 3,train_loss: 0.019001759724677915, valid_loss: 1598.7223680768693\nSEED: 13, FOLD: 3, EPOCH: 4,train_loss: 0.018278178888080764, valid_loss: 0.017777283010738238\nSEED: 13, FOLD: 3, EPOCH: 5,train_loss: 0.017730885171705353, valid_loss: 0.01755029537848064\nSEED: 13, FOLD: 3, EPOCH: 6,train_loss: 0.017271516130842865, valid_loss: 0.0191005748297487\nSEED: 13, FOLD: 3, EPOCH: 7,train_loss: 0.01701321416146999, valid_loss: 0.018025668311331955\nSEED: 13, FOLD: 3, EPOCH: 8,train_loss: 0.01676488482821597, valid_loss: 0.03259981514087745\nSEED: 13, FOLD: 3, EPOCH: 9,train_loss: 0.01666226538482809, valid_loss: 0.017679542782051222\nSEED: 13, FOLD: 3, EPOCH: 10,train_loss: 0.016466350233467826, valid_loss: 0.01793116708951337\nSEED: 13, FOLD: 3, EPOCH: 11,train_loss: 0.01626424670872027, valid_loss: 0.06459555982479027\nSEED: 13, FOLD: 3, EPOCH: 12,train_loss: 0.01601455612176091, valid_loss: 0.018377577965813022\nSEED: 13, FOLD: 3, EPOCH: 13,train_loss: 0.015611549302337378, valid_loss: 0.018304652108677797\nSEED: 13, FOLD: 3, EPOCH: 14,train_loss: 0.015095890547237256, valid_loss: 0.018441620974668435\nSEED: 13, FOLD: 3, EPOCH: 15,train_loss: 0.014286895044637423, valid_loss: 0.021033574747187752\nSEED: 13, FOLD: 3, EPOCH: 16,train_loss: 0.013299992348808442, valid_loss: 0.019617952991809164\nSEED: 13, FOLD: 3, EPOCH: 17,train_loss: 0.012146971548778297, valid_loss: 0.020118164856519016\nSEED: 13, FOLD: 3, EPOCH: 18,train_loss: 0.010492703625864356, valid_loss: 0.020412255956658295\nSEED: 13, FOLD: 3, EPOCH: 19,train_loss: 0.008945088136778042, valid_loss: 0.020599847579641003\nSEED: 13, FOLD: 3, EPOCH: 20,train_loss: 0.007531597737875515, valid_loss: 0.02080017833837441\nSEED: 13, FOLD: 3, EPOCH: 21,train_loss: 0.006572259222939067, valid_loss: 0.0208320787442582\nSEED: 13, FOLD: 3, EPOCH: 22,train_loss: 0.006025048027182147, valid_loss: 0.02085663540554898\nSEED: 13, FOLD: 3, EPOCH: 23,train_loss: 0.005744664035873474, valid_loss: 0.022263853438198568\nSEED: 13, FOLD: 3, EPOCH: 24,train_loss: 0.005647006288279582, valid_loss: 0.0214276217722467\nFOLD: 4, EPOCH: 0,train_loss: 0.7342814315844627, valid_loss: 0.6911118728773934\nSEED: 13, FOLD: 4, EPOCH: 0,train_loss: 0.4698066651603601, valid_loss: 0.024827335349151065\nSEED: 13, FOLD: 4, EPOCH: 1,train_loss: 0.021494717928614928, valid_loss: 0.9802114013582468\nSEED: 13, FOLD: 4, EPOCH: 2,train_loss: 0.02014751236097221, valid_loss: 0.019538727402687073\nSEED: 13, FOLD: 4, EPOCH: 3,train_loss: 0.018591413097660037, valid_loss: 0.019015181969319072\nSEED: 13, FOLD: 4, EPOCH: 4,train_loss: 0.017864460356696678, valid_loss: 0.018787813984922002\nSEED: 13, FOLD: 4, EPOCH: 5,train_loss: 0.017366006963607603, valid_loss: 0.01876866248037134\nSEED: 13, FOLD: 4, EPOCH: 6,train_loss: 0.017072647286538224, valid_loss: 0.019251027064664025\nSEED: 13, FOLD: 4, EPOCH: 7,train_loss: 0.016825695834836386, valid_loss: 0.018908199721149037\nSEED: 13, FOLD: 4, EPOCH: 8,train_loss: 0.01664711868077734, valid_loss: 0.018646814940231186\nSEED: 13, FOLD: 4, EPOCH: 9,train_loss: 0.016540958396546596, valid_loss: 0.01869129299053124\nSEED: 13, FOLD: 4, EPOCH: 10,train_loss: 0.0163191508257041, valid_loss: 0.018769297056964465\nSEED: 13, FOLD: 4, EPOCH: 11,train_loss: 0.01615393601602664, valid_loss: 0.02743979405079569\nSEED: 13, FOLD: 4, EPOCH: 12,train_loss: 0.01592186657562308, valid_loss: 0.018916206700461253\nSEED: 13, FOLD: 4, EPOCH: 13,train_loss: 0.015578118737542281, valid_loss: 0.029114661791494915\nSEED: 13, FOLD: 4, EPOCH: 14,train_loss: 0.015063289172240417, valid_loss: 0.019757394811936786\nSEED: 13, FOLD: 4, EPOCH: 15,train_loss: 0.014298451017506802, valid_loss: 0.033555325706090246\nSEED: 13, FOLD: 4, EPOCH: 16,train_loss: 0.013345258609548102, valid_loss: 0.020831351993339402\nSEED: 13, FOLD: 4, EPOCH: 17,train_loss: 0.01219596020387907, valid_loss: 0.020983190621648516\nSEED: 13, FOLD: 4, EPOCH: 18,train_loss: 0.010684043563304156, valid_loss: 0.04703924437718732\nSEED: 13, FOLD: 4, EPOCH: 19,train_loss: 0.009093657200544203, valid_loss: 0.02184404746762344\nSEED: 13, FOLD: 4, EPOCH: 20,train_loss: 0.007672240131663797, valid_loss: 0.02231729408460004\nSEED: 13, FOLD: 4, EPOCH: 21,train_loss: 0.00664572334120961, valid_loss: 0.023048236966133118\nSEED: 13, FOLD: 4, EPOCH: 22,train_loss: 0.006032516003797089, valid_loss: 0.023286664805241993\nSEED: 13, FOLD: 4, EPOCH: 23,train_loss: 0.005751190339996867, valid_loss: 0.02359236968415124\nSEED: 13, FOLD: 4, EPOCH: 24,train_loss: 0.005651126085461056, valid_loss: 0.023060465018664086\nFOLD: 0, EPOCH: 0,train_loss: 0.7313291050385737, valid_loss: 0.6970785821185392\nSEED: 14, FOLD: 0, EPOCH: 0,train_loss: 0.4709553116716553, valid_loss: 0.0261268562034649\nSEED: 14, FOLD: 0, EPOCH: 1,train_loss: 0.02154304759333963, valid_loss: 0.021387404488290056\nSEED: 14, FOLD: 0, EPOCH: 2,train_loss: 0.020230451301820038, valid_loss: 0.019402867566575024\nSEED: 14, FOLD: 0, EPOCH: 3,train_loss: 0.018762808023155598, valid_loss: 0.01882704199456117\nSEED: 14, FOLD: 0, EPOCH: 4,train_loss: 0.018152045713656622, valid_loss: 0.018457984305260813\nSEED: 14, FOLD: 0, EPOCH: 5,train_loss: 0.017562206740072674, valid_loss: 0.018335987211150283\nSEED: 14, FOLD: 0, EPOCH: 6,train_loss: 0.0172041746571768, valid_loss: 0.01921738706090871\nSEED: 14, FOLD: 0, EPOCH: 7,train_loss: 0.016994204405911158, valid_loss: 0.01835940367377856\nSEED: 14, FOLD: 0, EPOCH: 8,train_loss: 0.016777882251240637, valid_loss: 0.018149446586475652\nSEED: 14, FOLD: 0, EPOCH: 9,train_loss: 0.0166423961898123, valid_loss: 0.3974163839045693\nSEED: 14, FOLD: 0, EPOCH: 10,train_loss: 0.016556533613660628, valid_loss: 0.0811411609106204\nSEED: 14, FOLD: 0, EPOCH: 11,train_loss: 0.016306181303292946, valid_loss: 0.018277023918926716\nSEED: 14, FOLD: 0, EPOCH: 12,train_loss: 0.016110332770462053, valid_loss: 0.019472993417259526\nSEED: 14, FOLD: 0, EPOCH: 13,train_loss: 0.015627699438482523, valid_loss: 0.01859479655018624\nSEED: 14, FOLD: 0, EPOCH: 14,train_loss: 0.01546139005517614, valid_loss: 0.07584626693278551\nSEED: 14, FOLD: 0, EPOCH: 15,train_loss: 0.014689196920211332, valid_loss: 0.019051613751798868\nSEED: 14, FOLD: 0, EPOCH: 16,train_loss: 0.013780862180705088, valid_loss: 0.019389984992277974\nSEED: 14, FOLD: 0, EPOCH: 17,train_loss: 0.012629652605054604, valid_loss: 0.019728786695529434\nSEED: 14, FOLD: 0, EPOCH: 18,train_loss: 0.011194156624538742, valid_loss: 0.02065971219802604\nSEED: 14, FOLD: 0, EPOCH: 19,train_loss: 0.009608360567548569, valid_loss: 0.020892632248647073\nSEED: 14, FOLD: 0, EPOCH: 20,train_loss: 0.008108447997601352, valid_loss: 0.02132677089642076\nSEED: 14, FOLD: 0, EPOCH: 21,train_loss: 0.006967203337294252, valid_loss: 0.02138323402580093\nSEED: 14, FOLD: 0, EPOCH: 22,train_loss: 0.006234805366240334, valid_loss: 0.022236553623395806\nSEED: 14, FOLD: 0, EPOCH: 23,train_loss: 0.00588745986232939, valid_loss: 0.021789934760069147\nSEED: 14, FOLD: 0, EPOCH: 24,train_loss: 0.005765121261440758, valid_loss: 0.021894743501701775\nFOLD: 1, EPOCH: 0,train_loss: 0.7317494140924329, valid_loss: 0.6941651684897286\nSEED: 14, FOLD: 1, EPOCH: 0,train_loss: 0.4699668441865131, valid_loss: 0.02433066740632057\nSEED: 14, FOLD: 1, EPOCH: 1,train_loss: 0.021819398867605377, valid_loss: 0.021041962876915933\nSEED: 14, FOLD: 1, EPOCH: 2,train_loss: 0.02027396914841485, valid_loss: 0.019300233865422862\nSEED: 14, FOLD: 1, EPOCH: 3,train_loss: 0.019104540959870728, valid_loss: 0.024673176343951907\nSEED: 14, FOLD: 1, EPOCH: 4,train_loss: 0.018330912737950792, valid_loss: 0.018406315759888717\nSEED: 14, FOLD: 1, EPOCH: 5,train_loss: 0.017895956503322524, valid_loss: 0.018407025321253708\nSEED: 14, FOLD: 1, EPOCH: 6,train_loss: 0.01755080146402338, valid_loss: 3.050176441296935\nSEED: 14, FOLD: 1, EPOCH: 7,train_loss: 0.01732130970697116, valid_loss: 0.1012618291590895\nSEED: 14, FOLD: 1, EPOCH: 8,train_loss: 0.01708148835893095, valid_loss: 0.21467867019985404\nSEED: 14, FOLD: 1, EPOCH: 9,train_loss: 0.017055543132778937, valid_loss: 0.018411847949028014\nSEED: 14, FOLD: 1, EPOCH: 10,train_loss: 0.01691860636274745, valid_loss: 0.01842696030757257\nSEED: 14, FOLD: 1, EPOCH: 11,train_loss: 0.01681700378765155, valid_loss: 0.018434226512908936\nSEED: 14, FOLD: 1, EPOCH: 12,train_loss: 0.016547028254037793, valid_loss: 0.018662526298846516\nSEED: 14, FOLD: 1, EPOCH: 13,train_loss: 0.016327099378363494, valid_loss: 0.01838921739586762\nSEED: 14, FOLD: 1, EPOCH: 14,train_loss: 0.01597319777891801, valid_loss: 0.018488743661769797\nSEED: 14, FOLD: 1, EPOCH: 15,train_loss: 0.015483420831661154, valid_loss: 0.019156525044568946\nSEED: 14, FOLD: 1, EPOCH: 16,train_loss: 0.014672752721303136, valid_loss: 0.018775242247751782\nSEED: 14, FOLD: 1, EPOCH: 17,train_loss: 0.013749992332156123, valid_loss: 0.01919696094202144\nSEED: 14, FOLD: 1, EPOCH: 18,train_loss: 0.012482612196655168, valid_loss: 0.019969376921653747\nSEED: 14, FOLD: 1, EPOCH: 19,train_loss: 0.01088267641590677, valid_loss: 0.020247851870954035\nSEED: 14, FOLD: 1, EPOCH: 20,train_loss: 0.009265757566929734, valid_loss: 0.020750543900898526\nSEED: 14, FOLD: 1, EPOCH: 21,train_loss: 0.00787517970876537, valid_loss: 0.02076779564044305\nSEED: 14, FOLD: 1, EPOCH: 22,train_loss: 0.007068606008551199, valid_loss: 0.020844880172184537\nSEED: 14, FOLD: 1, EPOCH: 23,train_loss: 0.006584687756687185, valid_loss: 0.020934238683964525\nSEED: 14, FOLD: 1, EPOCH: 24,train_loss: 0.006418543327327845, valid_loss: 0.020967743332896915\nFOLD: 2, EPOCH: 0,train_loss: 0.731313922698947, valid_loss: 0.6945829619379604\nSEED: 14, FOLD: 2, EPOCH: 0,train_loss: 0.4688597081930957, valid_loss: 0.0239507587188307\nSEED: 14, FOLD: 2, EPOCH: 1,train_loss: 0.02176382434486911, valid_loss: 0.02098674507921233\nSEED: 14, FOLD: 2, EPOCH: 2,train_loss: 0.02046326923089615, valid_loss: 0.01897074786179206\nSEED: 14, FOLD: 2, EPOCH: 3,train_loss: 0.018974272493758926, valid_loss: 0.018253277932458064\nSEED: 14, FOLD: 2, EPOCH: 4,train_loss: 0.018174057671179373, valid_loss: 0.018365469268139673\nSEED: 14, FOLD: 2, EPOCH: 5,train_loss: 0.01762975015394066, valid_loss: 0.018385314985233193\nSEED: 14, FOLD: 2, EPOCH: 6,train_loss: 0.017262686806582453, valid_loss: 0.018087808481034112\nSEED: 14, FOLD: 2, EPOCH: 7,train_loss: 0.017037908715344427, valid_loss: 0.018189066966228625\nSEED: 14, FOLD: 2, EPOCH: 8,train_loss: 0.01686853262177412, valid_loss: 0.017907812068348423\nSEED: 14, FOLD: 2, EPOCH: 9,train_loss: 0.01667305286568792, valid_loss: 0.01828378644388388\nSEED: 14, FOLD: 2, EPOCH: 10,train_loss: 0.0165889414459251, valid_loss: 0.018031504772165242\nSEED: 14, FOLD: 2, EPOCH: 11,train_loss: 0.016430894817239132, valid_loss: 0.01847796722808305\nSEED: 14, FOLD: 2, EPOCH: 12,train_loss: 0.016163689637745636, valid_loss: 0.018542899969307816\nSEED: 14, FOLD: 2, EPOCH: 13,train_loss: 0.015790482308121696, valid_loss: 0.01846770246458404\nSEED: 14, FOLD: 2, EPOCH: 14,train_loss: 0.01522197759291832, valid_loss: 0.019281196846243215\nSEED: 14, FOLD: 2, EPOCH: 15,train_loss: 0.014534975860969744, valid_loss: 0.019199278863037333\nSEED: 14, FOLD: 2, EPOCH: 16,train_loss: 0.013491632119900938, valid_loss: 0.01991509229821317\nSEED: 14, FOLD: 2, EPOCH: 17,train_loss: 0.012278481433842924, valid_loss: 0.02058041950359064\nSEED: 14, FOLD: 2, EPOCH: 18,train_loss: 0.010772193262380535, valid_loss: 0.02079992659170838\nSEED: 14, FOLD: 2, EPOCH: 19,train_loss: 0.009170684329085592, valid_loss: 0.021315546322833088\nSEED: 14, FOLD: 2, EPOCH: 20,train_loss: 0.00777301247285652, valid_loss: 0.021304884048945764\nSEED: 14, FOLD: 2, EPOCH: 21,train_loss: 0.006750732690662793, valid_loss: 0.021416324300362784\nSEED: 14, FOLD: 2, EPOCH: 22,train_loss: 0.006153413982036105, valid_loss: 0.021493532585308832\nSEED: 14, FOLD: 2, EPOCH: 23,train_loss: 0.005862060728469404, valid_loss: 0.021501435953028062\nSEED: 14, FOLD: 2, EPOCH: 24,train_loss: 0.005753985906451725, valid_loss: 0.02147016192183775\nFOLD: 3, EPOCH: 0,train_loss: 0.7312982043210607, valid_loss: 0.6927067262785775\nSEED: 14, FOLD: 3, EPOCH: 0,train_loss: 0.4714075915488231, valid_loss: 0.023695507911699158\nSEED: 14, FOLD: 3, EPOCH: 1,train_loss: 0.021856245265281113, valid_loss: 0.020573904578174863\nSEED: 14, FOLD: 3, EPOCH: 2,train_loss: 0.020342763105448147, valid_loss: 0.018713745474815368\nSEED: 14, FOLD: 3, EPOCH: 3,train_loss: 0.018968100605165437, valid_loss: 0.018396936090929166\nSEED: 14, FOLD: 3, EPOCH: 4,train_loss: 0.018206429470629587, valid_loss: 0.38315227420202325\nSEED: 14, FOLD: 3, EPOCH: 5,train_loss: 0.017725487681527208, valid_loss: 0.018124739878943988\nSEED: 14, FOLD: 3, EPOCH: 6,train_loss: 0.01742253419909164, valid_loss: 0.018294203095138073\nSEED: 14, FOLD: 3, EPOCH: 7,train_loss: 0.017090970405588185, valid_loss: 0.01829650340867894\nSEED: 14, FOLD: 3, EPOCH: 8,train_loss: 0.01701075659153888, valid_loss: 0.01832414636654513\nSEED: 14, FOLD: 3, EPOCH: 9,train_loss: 0.01683233129064532, valid_loss: 0.018439234899623052\nSEED: 14, FOLD: 3, EPOCH: 10,train_loss: 0.016665926651798026, valid_loss: 0.018050335108169488\nSEED: 14, FOLD: 3, EPOCH: 11,train_loss: 0.01659705226112456, valid_loss: 0.018430868163704872\nSEED: 14, FOLD: 3, EPOCH: 12,train_loss: 0.01630558773712085, valid_loss: 0.018339869433215687\nSEED: 14, FOLD: 3, EPOCH: 13,train_loss: 0.016077908347394778, valid_loss: 0.018291040430111545\nSEED: 14, FOLD: 3, EPOCH: 14,train_loss: 0.015614151213671604, valid_loss: 0.018687489043389047\nSEED: 14, FOLD: 3, EPOCH: 15,train_loss: 0.015112441644942673, valid_loss: 0.01895861183958394\nSEED: 14, FOLD: 3, EPOCH: 16,train_loss: 0.014250137503292874, valid_loss: 0.019308367211903845\nSEED: 14, FOLD: 3, EPOCH: 17,train_loss: 0.01313738320974538, valid_loss: 0.02230384834110737\nSEED: 14, FOLD: 3, EPOCH: 18,train_loss: 0.011776570196732552, valid_loss: 0.02059823822762285\nSEED: 14, FOLD: 3, EPOCH: 19,train_loss: 0.010201814455272507, valid_loss: 0.02077763719218118\nSEED: 14, FOLD: 3, EPOCH: 20,train_loss: 0.00864433538658123, valid_loss: 0.021458983634199416\nSEED: 14, FOLD: 3, EPOCH: 21,train_loss: 0.007370569141595251, valid_loss: 0.021250555930393083\nSEED: 14, FOLD: 3, EPOCH: 22,train_loss: 0.006583783500250021, valid_loss: 0.021359015202948025\nSEED: 14, FOLD: 3, EPOCH: 23,train_loss: 0.006181789065853958, valid_loss: 0.02139740266970226\nSEED: 14, FOLD: 3, EPOCH: 24,train_loss: 0.006047224758505603, valid_loss: 0.021400378910558564\nFOLD: 4, EPOCH: 0,train_loss: 0.7312797843104731, valid_loss: 0.6955385293279376\nSEED: 14, FOLD: 4, EPOCH: 0,train_loss: 0.4719326814677376, valid_loss: 0.02372995311660426\nSEED: 14, FOLD: 4, EPOCH: 1,train_loss: 0.02202814090045264, valid_loss: 0.020815184552754676\nSEED: 14, FOLD: 4, EPOCH: 2,train_loss: 0.02061711631062692, valid_loss: 0.018549705243536403\nSEED: 14, FOLD: 4, EPOCH: 3,train_loss: 0.019098036874928614, valid_loss: 0.01841998478131635\nSEED: 14, FOLD: 4, EPOCH: 4,train_loss: 0.018297701638980503, valid_loss: 0.018113120006663458\nSEED: 14, FOLD: 4, EPOCH: 5,train_loss: 0.017812849076831862, valid_loss: 0.017798159005386487\nSEED: 14, FOLD: 4, EPOCH: 6,train_loss: 0.017437810710474958, valid_loss: 0.01774829008749553\nSEED: 14, FOLD: 4, EPOCH: 7,train_loss: 0.01723095358614504, valid_loss: 0.01760220280183213\nSEED: 14, FOLD: 4, EPOCH: 8,train_loss: 0.017047499156944507, valid_loss: 0.017718024578477654\nSEED: 14, FOLD: 4, EPOCH: 9,train_loss: 0.016855999066011748, valid_loss: 0.017762126055146966\nSEED: 14, FOLD: 4, EPOCH: 10,train_loss: 0.01666397419180313, valid_loss: 0.017722907476127148\nSEED: 14, FOLD: 4, EPOCH: 11,train_loss: 0.01641217725908887, valid_loss: 0.018427830189466476\nSEED: 14, FOLD: 4, EPOCH: 12,train_loss: 0.016103874062643433, valid_loss: 0.017981917091778345\nSEED: 14, FOLD: 4, EPOCH: 13,train_loss: 0.01574853093220587, valid_loss: 0.01831941216119698\nSEED: 14, FOLD: 4, EPOCH: 14,train_loss: 0.01524367372430589, valid_loss: 0.01858995370566845\nSEED: 14, FOLD: 4, EPOCH: 15,train_loss: 0.014503229504627903, valid_loss: 0.01865663927580629\nSEED: 14, FOLD: 4, EPOCH: 16,train_loss: 0.013533144060821428, valid_loss: 0.019207461391176496\nSEED: 14, FOLD: 4, EPOCH: 17,train_loss: 0.012232616387416411, valid_loss: 0.020051232618944984\nSEED: 14, FOLD: 4, EPOCH: 18,train_loss: 0.010840061025517265, valid_loss: 0.02036357181412833\nSEED: 14, FOLD: 4, EPOCH: 19,train_loss: 0.009273878040376805, valid_loss: 0.02050760223397187\nSEED: 14, FOLD: 4, EPOCH: 20,train_loss: 0.007896616976762558, valid_loss: 0.020946489753467697\nSEED: 14, FOLD: 4, EPOCH: 21,train_loss: 0.006839986690014166, valid_loss: 0.02105658208685262\nSEED: 14, FOLD: 4, EPOCH: 22,train_loss: 0.006204118197579889, valid_loss: 0.021195130263056073\nSEED: 14, FOLD: 4, EPOCH: 23,train_loss: 0.0058728472914302, valid_loss: 0.021371114892618996\nSEED: 14, FOLD: 4, EPOCH: 24,train_loss: 0.00577606494286961, valid_loss: 0.021365088863032206\nFOLD: 0, EPOCH: 0,train_loss: 0.732853924270964, valid_loss: 0.6908746021134513\nSEED: 15, FOLD: 0, EPOCH: 0,train_loss: 0.4703820264850655, valid_loss: 0.023895102579678806\nSEED: 15, FOLD: 0, EPOCH: 1,train_loss: 0.021796055274070615, valid_loss: 0.02090030796825886\nSEED: 15, FOLD: 0, EPOCH: 2,train_loss: 0.02010626661298919, valid_loss: 0.05813668008361544\nSEED: 15, FOLD: 0, EPOCH: 3,train_loss: 0.01874503320640456, valid_loss: 0.02770452709602458\nSEED: 15, FOLD: 0, EPOCH: 4,train_loss: 0.017983748842656178, valid_loss: 0.018450290762952397\nSEED: 15, FOLD: 0, EPOCH: 5,train_loss: 0.01748367367026797, valid_loss: 0.018607094059033054\nSEED: 15, FOLD: 0, EPOCH: 6,train_loss: 0.017239059212814718, valid_loss: 0.019245426569666182\nSEED: 15, FOLD: 0, EPOCH: 7,train_loss: 0.016932910539372993, valid_loss: 0.018544290134949345\nSEED: 15, FOLD: 0, EPOCH: 8,train_loss: 0.01662508967284956, valid_loss: 0.11058303490281104\nSEED: 15, FOLD: 0, EPOCH: 9,train_loss: 0.01643630283048553, valid_loss: 0.018911058136395046\nSEED: 15, FOLD: 0, EPOCH: 10,train_loss: 0.016186906351116453, valid_loss: 0.01864325555839709\nSEED: 15, FOLD: 0, EPOCH: 11,train_loss: 0.016011726512254156, valid_loss: 0.018880135752260684\nSEED: 15, FOLD: 0, EPOCH: 12,train_loss: 0.01563667541329008, valid_loss: 0.13650714648621423\nSEED: 15, FOLD: 0, EPOCH: 13,train_loss: 0.015143949633640965, valid_loss: 0.019247831430818353\nSEED: 15, FOLD: 0, EPOCH: 14,train_loss: 0.014744918113642366, valid_loss: 0.051748528704047204\nSEED: 15, FOLD: 0, EPOCH: 15,train_loss: 0.0139483505873567, valid_loss: 0.020008235637630736\nSEED: 15, FOLD: 0, EPOCH: 16,train_loss: 0.012914217124781469, valid_loss: 0.020849377076540675\nSEED: 15, FOLD: 0, EPOCH: 17,train_loss: 0.011697260560943697, valid_loss: 0.021040397456714086\nSEED: 15, FOLD: 0, EPOCH: 18,train_loss: 0.010254957808358391, valid_loss: 0.021490539450730595\nSEED: 15, FOLD: 0, EPOCH: 19,train_loss: 0.00878276408099345, valid_loss: 0.0218735968960183\nSEED: 15, FOLD: 0, EPOCH: 20,train_loss: 0.007502691439577263, valid_loss: 0.021991866773792675\nSEED: 15, FOLD: 0, EPOCH: 21,train_loss: 0.006556910843364078, valid_loss: 0.022030578766550336\nSEED: 15, FOLD: 0, EPOCH: 22,train_loss: 0.00598962597181871, valid_loss: 0.02209091553730624\nSEED: 15, FOLD: 0, EPOCH: 23,train_loss: 0.005723808640546172, valid_loss: 0.021960529844675745\nSEED: 15, FOLD: 0, EPOCH: 24,train_loss: 0.005632635454110203, valid_loss: 0.022034477069973947\nFOLD: 1, EPOCH: 0,train_loss: 0.7327966893064803, valid_loss: 0.6962387050901141\nSEED: 15, FOLD: 1, EPOCH: 0,train_loss: 0.4685254128838795, valid_loss: 0.024372472826923643\nSEED: 15, FOLD: 1, EPOCH: 1,train_loss: 0.021724802954797295, valid_loss: 0.02129281328192779\nSEED: 15, FOLD: 1, EPOCH: 2,train_loss: 0.02050999279363432, valid_loss: 0.019284749403595924\nSEED: 15, FOLD: 1, EPOCH: 3,train_loss: 0.01900705858471169, valid_loss: 0.7408877256991607\nSEED: 15, FOLD: 1, EPOCH: 4,train_loss: 0.018164313958444887, valid_loss: 0.031220802425273825\nSEED: 15, FOLD: 1, EPOCH: 5,train_loss: 0.017676165712106486, valid_loss: 0.11598063483834267\nSEED: 15, FOLD: 1, EPOCH: 6,train_loss: 0.01753459393006304, valid_loss: 0.06530323076461042\nSEED: 15, FOLD: 1, EPOCH: 7,train_loss: 0.017098517629547394, valid_loss: 0.020117821358144283\nSEED: 15, FOLD: 1, EPOCH: 8,train_loss: 0.017039363771892975, valid_loss: 0.018347351253032683\nSEED: 15, FOLD: 1, EPOCH: 9,train_loss: 0.017045458259087973, valid_loss: 0.017993365600705146\nSEED: 15, FOLD: 1, EPOCH: 10,train_loss: 0.016632288194976856, valid_loss: 0.0186868249305657\nSEED: 15, FOLD: 1, EPOCH: 11,train_loss: 0.01653791153533519, valid_loss: 0.01901542650801795\nSEED: 15, FOLD: 1, EPOCH: 12,train_loss: 0.01639245950576404, valid_loss: 0.08199120504515511\nSEED: 15, FOLD: 1, EPOCH: 13,train_loss: 0.015810195154146008, valid_loss: 0.02733702707503523\nSEED: 15, FOLD: 1, EPOCH: 14,train_loss: 0.01544779411557576, valid_loss: 0.01924160434199231\nSEED: 15, FOLD: 1, EPOCH: 15,train_loss: 0.015008592964622421, valid_loss: 0.019023021310567857\nSEED: 15, FOLD: 1, EPOCH: 16,train_loss: 0.013866506543928299, valid_loss: 0.019831506269318715\nSEED: 15, FOLD: 1, EPOCH: 17,train_loss: 0.012840112536281778, valid_loss: 0.02076283996658666\nSEED: 15, FOLD: 1, EPOCH: 18,train_loss: 0.011882662881111753, valid_loss: 0.0358139192951577\nSEED: 15, FOLD: 1, EPOCH: 19,train_loss: 0.010067109909394512, valid_loss: 0.02089689459119524\nSEED: 15, FOLD: 1, EPOCH: 20,train_loss: 0.008646965165204112, valid_loss: 0.020851607035313333\nSEED: 15, FOLD: 1, EPOCH: 21,train_loss: 0.007611091366793582, valid_loss: 0.021138144976326396\nSEED: 15, FOLD: 1, EPOCH: 22,train_loss: 0.006866674374222108, valid_loss: 0.021274537539907865\nSEED: 15, FOLD: 1, EPOCH: 23,train_loss: 0.00661566285956381, valid_loss: 0.021365726313420704\nSEED: 15, FOLD: 1, EPOCH: 24,train_loss: 0.006415522174801731, valid_loss: 0.021335491271955626\nFOLD: 2, EPOCH: 0,train_loss: 0.7327543980833413, valid_loss: 0.6931267293060527\nSEED: 15, FOLD: 2, EPOCH: 0,train_loss: 0.46899701509138814, valid_loss: 0.024145566255730742\nSEED: 15, FOLD: 2, EPOCH: 1,train_loss: 0.0217925817773178, valid_loss: 0.020167226793573183\nSEED: 15, FOLD: 2, EPOCH: 2,train_loss: 0.020372329914159534, valid_loss: 0.01862372019711663\nSEED: 15, FOLD: 2, EPOCH: 3,train_loss: 0.01901747849162506, valid_loss: 0.8376745704342338\nSEED: 15, FOLD: 2, EPOCH: 4,train_loss: 0.018245438370259777, valid_loss: 0.01783121341620298\nSEED: 15, FOLD: 2, EPOCH: 5,train_loss: 0.01772556696222096, valid_loss: 0.01777869397226502\nSEED: 15, FOLD: 2, EPOCH: 6,train_loss: 0.01738404217378601, valid_loss: 0.018212053924798965\nSEED: 15, FOLD: 2, EPOCH: 7,train_loss: 0.01706749338494695, valid_loss: 0.017805463278337437\nSEED: 15, FOLD: 2, EPOCH: 8,train_loss: 0.016833935493090445, valid_loss: 0.017856796546017423\nSEED: 15, FOLD: 2, EPOCH: 9,train_loss: 0.016647158744002598, valid_loss: 0.017901079975725973\nSEED: 15, FOLD: 2, EPOCH: 10,train_loss: 0.016444454617474392, valid_loss: 0.017862669904442394\nSEED: 15, FOLD: 2, EPOCH: 11,train_loss: 0.016247192417959803, valid_loss: 0.017974411000442857\nSEED: 15, FOLD: 2, EPOCH: 12,train_loss: 0.015861604713659355, valid_loss: 0.018149027683059957\nSEED: 15, FOLD: 2, EPOCH: 13,train_loss: 0.015519906375287235, valid_loss: 0.0182784414740608\nSEED: 15, FOLD: 2, EPOCH: 14,train_loss: 0.014905180289423552, valid_loss: 0.01908759827561238\nSEED: 15, FOLD: 2, EPOCH: 15,train_loss: 0.014141458869520306, valid_loss: 0.019410622032249674\nSEED: 15, FOLD: 2, EPOCH: 16,train_loss: 0.013034436446817024, valid_loss: 0.01959174674223451\nSEED: 15, FOLD: 2, EPOCH: 17,train_loss: 0.011691236483823994, valid_loss: 0.020453304600189712\nSEED: 15, FOLD: 2, EPOCH: 18,train_loss: 0.01027188486541095, valid_loss: 0.020755814717096442\nSEED: 15, FOLD: 2, EPOCH: 19,train_loss: 0.00883964105995129, valid_loss: 0.021079075577504495\nSEED: 15, FOLD: 2, EPOCH: 20,train_loss: 0.007495696933103212, valid_loss: 0.02098285686224699\nSEED: 15, FOLD: 2, EPOCH: 21,train_loss: 0.006567227644035997, valid_loss: 0.021034129950053552\nSEED: 15, FOLD: 2, EPOCH: 22,train_loss: 0.00603464285613618, valid_loss: 0.02119367492987829\nSEED: 15, FOLD: 2, EPOCH: 23,train_loss: 0.0057464258752061405, valid_loss: 0.021192925105638364\nSEED: 15, FOLD: 2, EPOCH: 24,train_loss: 0.005642125437009162, valid_loss: 0.02120879922500428\nFOLD: 3, EPOCH: 0,train_loss: 0.7333144480767457, valid_loss: 0.6933097669056484\nSEED: 15, FOLD: 3, EPOCH: 0,train_loss: 0.4691204473214305, valid_loss: 0.024628212515796933\nSEED: 15, FOLD: 3, EPOCH: 1,train_loss: 0.021604814998589565, valid_loss: 0.02441108136304787\nSEED: 15, FOLD: 3, EPOCH: 2,train_loss: 0.020470389846604372, valid_loss: 2.6349539279937746\nSEED: 15, FOLD: 3, EPOCH: 3,train_loss: 0.019339689478764067, valid_loss: 0.041217189388615745\nSEED: 15, FOLD: 3, EPOCH: 4,train_loss: 0.018349861402226532, valid_loss: 0.07637429982423782\nSEED: 15, FOLD: 3, EPOCH: 5,train_loss: 0.017728994256290403, valid_loss: 0.1116424777678081\nSEED: 15, FOLD: 3, EPOCH: 6,train_loss: 0.017391013393205576, valid_loss: 4.263326590401785\nSEED: 15, FOLD: 3, EPOCH: 7,train_loss: 0.017239901240564126, valid_loss: 85.78443875994002\nSEED: 15, FOLD: 3, EPOCH: 8,train_loss: 0.017114031560503055, valid_loss: 0.01819236661706652\nSEED: 15, FOLD: 3, EPOCH: 9,train_loss: 0.016962542090618957, valid_loss: 0.022950983739324977\nSEED: 15, FOLD: 3, EPOCH: 10,train_loss: 0.01747804901063226, valid_loss: 0.01924247198871204\nSEED: 15, FOLD: 3, EPOCH: 11,train_loss: 0.01799660012719856, valid_loss: 0.07617199319813933\nSEED: 15, FOLD: 3, EPOCH: 12,train_loss: 0.017287679212302832, valid_loss: 0.018081847578287125\nSEED: 15, FOLD: 3, EPOCH: 13,train_loss: 0.016924407339884318, valid_loss: 0.018854658624955587\nSEED: 15, FOLD: 3, EPOCH: 14,train_loss: 0.016551765795473173, valid_loss: 0.018173928505608013\nSEED: 15, FOLD: 3, EPOCH: 15,train_loss: 0.01607904853164286, valid_loss: 0.01954389911677156\nSEED: 15, FOLD: 3, EPOCH: 16,train_loss: 0.018848140710505886, valid_loss: 0.01826825940183231\nSEED: 15, FOLD: 3, EPOCH: 17,train_loss: 0.017766645866567673, valid_loss: 0.024245940042393548\nSEED: 15, FOLD: 3, EPOCH: 18,train_loss: 0.017308615525995476, valid_loss: 0.3761314548019852\nSEED: 15, FOLD: 3, EPOCH: 19,train_loss: 0.01681835510754499, valid_loss: 0.0719799418002367\nSEED: 15, FOLD: 3, EPOCH: 20,train_loss: 0.016237521267401567, valid_loss: 0.020376807451248168\nSEED: 15, FOLD: 3, EPOCH: 21,train_loss: 0.01575708443271941, valid_loss: 0.01788608841598034\nSEED: 15, FOLD: 3, EPOCH: 22,train_loss: 0.015206373229190924, valid_loss: 0.018264125872935567\nSEED: 15, FOLD: 3, EPOCH: 23,train_loss: 0.015177560531520758, valid_loss: 0.018558399538908685\nSEED: 15, FOLD: 3, EPOCH: 24,train_loss: 0.014734432128244553, valid_loss: 0.018084353634289334\nFOLD: 4, EPOCH: 0,train_loss: 0.7328753618226536, valid_loss: 0.6962228757994515\nSEED: 15, FOLD: 4, EPOCH: 0,train_loss: 0.4690556855791289, valid_loss: 0.024685083276459147\nSEED: 15, FOLD: 4, EPOCH: 1,train_loss: 0.02161588834301717, valid_loss: 0.02084254047700337\nSEED: 15, FOLD: 4, EPOCH: 2,train_loss: 0.020244594082992146, valid_loss: 0.02211659571954182\nSEED: 15, FOLD: 4, EPOCH: 3,train_loss: 0.01898988267487806, valid_loss: 0.019135336365018574\nSEED: 15, FOLD: 4, EPOCH: 4,train_loss: 0.018153465379947338, valid_loss: 2.4711254970569696\nSEED: 15, FOLD: 4, EPOCH: 5,train_loss: 0.017688835120719414, valid_loss: 0.018146043430481637\nSEED: 15, FOLD: 4, EPOCH: 6,train_loss: 0.01728920410454705, valid_loss: 0.18766378611326218\nSEED: 15, FOLD: 4, EPOCH: 7,train_loss: 0.01715451723261588, valid_loss: 0.01791319644876889\nSEED: 15, FOLD: 4, EPOCH: 8,train_loss: 0.017118184786775837, valid_loss: 0.017815337516367435\nSEED: 15, FOLD: 4, EPOCH: 9,train_loss: 0.0169897665526124, valid_loss: 0.05334144913192306\nSEED: 15, FOLD: 4, EPOCH: 10,train_loss: 0.016898227855563164, valid_loss: 0.01855624584215028\nSEED: 15, FOLD: 4, EPOCH: 11,train_loss: 0.01678381902773095, valid_loss: 0.018144600492502962\nSEED: 15, FOLD: 4, EPOCH: 12,train_loss: 0.01657626889916002, valid_loss: 0.018317357370895997\nSEED: 15, FOLD: 4, EPOCH: 13,train_loss: 0.01637305521770664, valid_loss: 0.017923077701457908\nSEED: 15, FOLD: 4, EPOCH: 14,train_loss: 0.015989571776025106, valid_loss: 0.018263407237827777\nSEED: 15, FOLD: 4, EPOCH: 15,train_loss: 0.015406819539603548, valid_loss: 0.02138844278774091\nSEED: 15, FOLD: 4, EPOCH: 16,train_loss: 0.014759757515528927, valid_loss: 0.018821293408317227\nSEED: 15, FOLD: 4, EPOCH: 17,train_loss: 0.013915257703890835, valid_loss: 0.019125611734177386\nSEED: 15, FOLD: 4, EPOCH: 18,train_loss: 0.012942031538788822, valid_loss: 0.01949331010026591\nSEED: 15, FOLD: 4, EPOCH: 19,train_loss: 0.011216632839616226, valid_loss: 0.01973622984119824\nSEED: 15, FOLD: 4, EPOCH: 20,train_loss: 0.009509906125511381, valid_loss: 0.020310549278344428\nSEED: 15, FOLD: 4, EPOCH: 21,train_loss: 0.008162212706562401, valid_loss: 0.020638160301106316\nSEED: 15, FOLD: 4, EPOCH: 22,train_loss: 0.007220611934536609, valid_loss: 0.02070938747908388\nSEED: 15, FOLD: 4, EPOCH: 23,train_loss: 0.006715558168977715, valid_loss: 0.020804265833326747\nSEED: 15, FOLD: 4, EPOCH: 24,train_loss: 0.0065243013128908215, valid_loss: 0.02084932949926172\n0.01723039259304107\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pd.DataFrame(sc_dic,index=['sc']).T","metadata":{"execution":{"iopub.status.busy":"2024-11-20T00:41:32.435047Z","iopub.execute_input":"2024-11-20T00:41:32.435450Z","iopub.status.idle":"2024-11-20T00:41:32.452264Z","shell.execute_reply.started":"2024-11-20T00:41:32.435404Z","shell.execute_reply":"2024-11-20T00:41:32.451350Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"          sc\n0   0.018054\n1   0.017688\n2   0.017531\n3   0.017431\n4   0.017367\n5   0.017357\n6   0.017333\n7   0.017319\n8   0.017313\n9   0.017295\n10  0.017276\n11  0.017262\n12  0.017261\n13  0.017252\n14  0.017238\n15  0.017230","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.018054</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.017688</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.017531</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.017431</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.017367</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.017357</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.017333</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.017319</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.017313</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.017295</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.017276</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.017262</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.017261</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.017252</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.017238</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.017230</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}